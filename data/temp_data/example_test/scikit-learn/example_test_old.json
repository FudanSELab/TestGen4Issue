[
    {
        "repo": "scikit-learn/scikit-learn",
        "instance_id": "scikit-learn__scikit-learn-10508",
        "base_commit": "c753b77ac49e72ebc0fe5e3c2369fe628f975017",
        "issue_title": "LabelEncoder transform fails for empty lists (for certain inputs)",
        "test_file": "/home/zqc/RepoCodeEdit/data/swe_bench_temp_wy/scikit-learn/sklearn/preprocessing/tests/test_label.py",
        "searched_functions": [
            "def test_label_encoder_errors():\n    # Check that invalid arguments yield ValueError\n    le = LabelEncoder()\n    assert_raises(ValueError, le.transform, [])\n    assert_raises(ValueError, le.inverse_transform, [])\n\n    # Fail on unseen labels\n    le = LabelEncoder()\n    le.fit([1, 2, 3, -1, 1])\n    msg = \"contains previously unseen labels\"\n    assert_raise_message(ValueError, msg, le.inverse_transform, [-2])\n    assert_raise_message(ValueError, msg, le.inverse_transform, [-2, -3, -4])",
            "def test_label_encoder():\n    # Test LabelEncoder's transform and inverse_transform methods\n    le = LabelEncoder()\n    le.fit([1, 1, 4, 5, -1, 0])\n    assert_array_equal(le.classes_, [-1, 0, 1, 4, 5])\n    assert_array_equal(le.transform([0, 1, 4, 4, 5, -1, -1]),\n                       [1, 2, 3, 3, 4, 0, 0])\n    assert_array_equal(le.inverse_transform([1, 2, 3, 3, 4, 0, 0]),\n                       [0, 1, 4, 4, 5, -1, -1])\n    assert_raises(ValueError, le.transform, [0, 6])\n\n    le.fit([\"apple\", \"orange\"])\n    msg = \"bad input shape\"\n    assert_raise_message(ValueError, msg, le.transform, \"apple\")",
            "def test_label_encoder_fit_transform():\n    # Test fit_transform\n    le = LabelEncoder()\n    ret = le.fit_transform([1, 1, 4, 5, -1, 0])\n    assert_array_equal(ret, [2, 2, 3, 4, 0, 1])\n\n    le = LabelEncoder()\n    ret = le.fit_transform([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\n    assert_array_equal(ret, [1, 1, 2, 0])",
            "def test_label_binarizer_errors():\n    # Check that invalid arguments yield ValueError\n    one_class = np.array([0, 0, 0, 0])\n    lb = LabelBinarizer().fit(one_class)\n\n    multi_label = [(2, 3), (0,), (0, 2)]\n    assert_raises(ValueError, lb.transform, multi_label)\n\n    lb = LabelBinarizer()\n    assert_raises(ValueError, lb.transform, [])\n    assert_raises(ValueError, lb.inverse_transform, [])\n\n    assert_raises(ValueError, LabelBinarizer, neg_label=2, pos_label=1)\n    assert_raises(ValueError, LabelBinarizer, neg_label=2, pos_label=2)\n\n    assert_raises(ValueError, LabelBinarizer, neg_label=1, pos_label=2,\n                  sparse_output=True)\n\n    # Fail on y_type\n    assert_raises(ValueError, _inverse_binarize_thresholding,\n                  y=csr_matrix([[1, 2], [2, 1]]), output_type=\"foo\",\n                  classes=[1, 2], threshold=0)\n\n    # Sequence of seq type should raise ValueError\n    y_seq_of_seqs = [[], [1, 2], [3], [0, 1, 3], [2]]\n    assert_raises(ValueError, LabelBinarizer().fit_transform, y_seq_of_seqs)\n\n    # Fail on the number of classes\n    assert_raises(ValueError, _inverse_binarize_thresholding,\n                  y=csr_matrix([[1, 2], [2, 1]]), output_type=\"foo\",\n                  classes=[1, 2, 3], threshold=0)\n\n    # Fail on the dimension of 'binary'\n    assert_raises(ValueError, _inverse_binarize_thresholding,\n                  y=np.array([[1, 2, 3], [2, 1, 3]]), output_type=\"binary\",\n                  classes=[1, 2, 3], threshold=0)\n\n    # Fail on multioutput data\n    assert_raises(ValueError, LabelBinarizer().fit, np.array([[1, 3], [2, 1]]))\n    assert_raises(ValueError, label_binarize, np.array([[1, 3], [2, 1]]),\n                  [1, 2, 3])",
            "def test_multilabel_binarizer_empty_sample():\n    mlb = MultiLabelBinarizer()\n    y = [[1, 2], [1], []]\n    Y = np.array([[1, 1],\n                  [1, 0],\n                  [0, 0]])\n    assert_array_equal(mlb.fit_transform(y), Y)",
            "def test_sparse_output_multilabel_binarizer():\n    # test input as iterable of iterables\n    inputs = [\n        lambda: [(2, 3), (1,), (1, 2)],\n        lambda: (set([2, 3]), set([1]), set([1, 2])),\n        lambda: iter([iter((2, 3)), iter((1,)), set([1, 2])]),\n    ]\n    indicator_mat = np.array([[0, 1, 1],\n                              [1, 0, 0],\n                              [1, 1, 0]])\n\n    inverse = inputs[0]()\n    for sparse_output in [True, False]:\n        for inp in inputs:\n            # With fit_transform\n            mlb = MultiLabelBinarizer(sparse_output=sparse_output)\n            got = mlb.fit_transform(inp())\n            assert_equal(issparse(got), sparse_output)\n            if sparse_output:\n                # verify CSR assumption that indices and indptr have same dtype\n                assert_equal(got.indices.dtype, got.indptr.dtype)\n                got = got.toarray()\n            assert_array_equal(indicator_mat, got)\n            assert_array_equal([1, 2, 3], mlb.classes_)\n            assert_equal(mlb.inverse_transform(got), inverse)\n\n            # With fit\n            mlb = MultiLabelBinarizer(sparse_output=sparse_output)\n            got = mlb.fit(inp()).transform(inp())\n            assert_equal(issparse(got), sparse_output)\n            if sparse_output:\n                # verify CSR assumption that indices and indptr have same dtype\n                assert_equal(got.indices.dtype, got.indptr.dtype)\n                got = got.toarray()\n            assert_array_equal(indicator_mat, got)\n            assert_array_equal([1, 2, 3], mlb.classes_)\n            assert_equal(mlb.inverse_transform(got), inverse)\n\n    assert_raises(ValueError, mlb.inverse_transform,\n                  csr_matrix(np.array([[0, 1, 1],\n                                       [2, 0, 0],\n                                       [1, 1, 0]])))",
            "def test_invalid_input_label_binarize():\n    assert_raises(ValueError, label_binarize, [0, 2], classes=[0, 2],\n                  pos_label=0, neg_label=1)",
            "def test_label_binarizer_set_label_encoding():\n    lb = LabelBinarizer(neg_label=-2, pos_label=0)\n\n    # two-class case with pos_label=0\n    inp = np.array([0, 1, 1, 0])\n    expected = np.array([[-2, 0, 0, -2]]).T\n    got = lb.fit_transform(inp)\n    assert_array_equal(expected, got)\n    assert_array_equal(lb.inverse_transform(got), inp)\n\n    lb = LabelBinarizer(neg_label=-2, pos_label=2)\n\n    # multi-class case\n    inp = np.array([3, 2, 1, 2, 0])\n    expected = np.array([[-2, -2, -2, +2],\n                         [-2, -2, +2, -2],\n                         [-2, +2, -2, -2],\n                         [-2, -2, +2, -2],\n                         [+2, -2, -2, -2]])\n    got = lb.fit_transform(inp)\n    assert_array_equal(expected, got)\n    assert_array_equal(lb.inverse_transform(got), inp)",
            "def test_multilabel_binarizer_non_integer_labels():\n    tuple_classes = np.empty(3, dtype=object)\n    tuple_classes[:] = [(1,), (2,), (3,)]\n    inputs = [\n        ([('2', '3'), ('1',), ('1', '2')], ['1', '2', '3']),\n        ([('b', 'c'), ('a',), ('a', 'b')], ['a', 'b', 'c']),\n        ([((2,), (3,)), ((1,),), ((1,), (2,))], tuple_classes),\n    ]\n    indicator_mat = np.array([[0, 1, 1],\n                              [1, 0, 0],\n                              [1, 1, 0]])\n    for inp, classes in inputs:\n        # fit_transform()\n        mlb = MultiLabelBinarizer()\n        assert_array_equal(mlb.fit_transform(inp), indicator_mat)\n        assert_array_equal(mlb.classes_, classes)\n        assert_array_equal(mlb.inverse_transform(indicator_mat), inp)\n\n        # fit().transform()\n        mlb = MultiLabelBinarizer()\n        assert_array_equal(mlb.fit(inp).transform(inp), indicator_mat)\n        assert_array_equal(mlb.classes_, classes)\n        assert_array_equal(mlb.inverse_transform(indicator_mat), inp)\n\n    mlb = MultiLabelBinarizer()\n    assert_raises(TypeError, mlb.fit_transform, [({}), ({}, {'a': 'b'})])",
            "def test_multilabel_binarizer():\n    # test input as iterable of iterables\n    inputs = [\n        lambda: [(2, 3), (1,), (1, 2)],\n        lambda: (set([2, 3]), set([1]), set([1, 2])),\n        lambda: iter([iter((2, 3)), iter((1,)), set([1, 2])]),\n    ]\n    indicator_mat = np.array([[0, 1, 1],\n                              [1, 0, 0],\n                              [1, 1, 0]])\n    inverse = inputs[0]()\n    for inp in inputs:\n        # With fit_transform\n        mlb = MultiLabelBinarizer()\n        got = mlb.fit_transform(inp())\n        assert_array_equal(indicator_mat, got)\n        assert_array_equal([1, 2, 3], mlb.classes_)\n        assert_equal(mlb.inverse_transform(got), inverse)\n\n        # With fit\n        mlb = MultiLabelBinarizer()\n        got = mlb.fit(inp()).transform(inp())\n        assert_array_equal(indicator_mat, got)\n        assert_array_equal([1, 2, 3], mlb.classes_)\n        assert_equal(mlb.inverse_transform(got), inverse)"
        ]
    },
    {
        "repo": "scikit-learn/scikit-learn",
        "instance_id": "scikit-learn__scikit-learn-13142",
        "base_commit": "1c8668b0a021832386470ddf740d834e02c66f69",
        "issue_title": "GaussianMixture predict and fit_predict disagree when n_init>1",
        "test_file": "/home/zqc/RepoCodeEdit/data/swe_bench_temp_wy/scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py",
        "searched_functions": [
            "def test_init():\n    # We check that by increasing the n_init number we have a better solution\n    for random_state in range(25):\n        rand_data = RandomData(np.random.RandomState(random_state), scale=1)\n        n_components = rand_data.n_components\n        X = rand_data.X['full']\n\n        gmm1 = GaussianMixture(n_components=n_components, n_init=1,\n                               max_iter=1, random_state=random_state).fit(X)\n        gmm2 = GaussianMixture(n_components=n_components, n_init=10,\n                               max_iter=1, random_state=random_state).fit(X)\n\n        assert gmm2.lower_bound_ >= gmm1.lower_bound_",
            "def test_gaussian_mixture_fit_predict(seed, max_iter, tol):\n    rng = np.random.RandomState(seed)\n    rand_data = RandomData(rng)\n    for covar_type in COVARIANCE_TYPE:\n        X = rand_data.X[covar_type]\n        Y = rand_data.Y\n        g = GaussianMixture(n_components=rand_data.n_components,\n                            random_state=rng, weights_init=rand_data.weights,\n                            means_init=rand_data.means,\n                            precisions_init=rand_data.precisions[covar_type],\n                            covariance_type=covar_type,\n                            max_iter=max_iter, tol=tol)\n\n        # check if fit_predict(X) is equivalent to fit(X).predict(X)\n        f = copy.deepcopy(g)\n        Y_pred1 = f.fit(X).predict(X)\n        Y_pred2 = g.fit_predict(X)\n        assert_array_equal(Y_pred1, Y_pred2)\n        assert_greater(adjusted_rand_score(Y, Y_pred2), .95)",
            "def test_multiple_init():\n    # Test that multiple inits does not much worse than a single one\n    rng = np.random.RandomState(0)\n    n_samples, n_features, n_components = 50, 5, 2\n    X = rng.randn(n_samples, n_features)\n    for cv_type in COVARIANCE_TYPE:\n        train1 = GaussianMixture(n_components=n_components,\n                                 covariance_type=cv_type,\n                                 random_state=0).fit(X).score(X)\n        train2 = GaussianMixture(n_components=n_components,\n                                 covariance_type=cv_type,\n                                 random_state=0, n_init=5).fit(X).score(X)\n        assert_greater_equal(train2, train1)",
            "def test_gaussian_mixture_fit_convergence_warning():\n    rng = np.random.RandomState(0)\n    rand_data = RandomData(rng, scale=1)\n    n_components = rand_data.n_components\n    max_iter = 1\n    for covar_type in COVARIANCE_TYPE:\n        X = rand_data.X[covar_type]\n        g = GaussianMixture(n_components=n_components, n_init=1,\n                            max_iter=max_iter, reg_covar=0, random_state=rng,\n                            covariance_type=covar_type)\n        assert_warns_message(ConvergenceWarning,\n                             'Initialization %d did not converge. '\n                             'Try different init parameters, '\n                             'or increase max_iter, tol '\n                             'or check for degenerate data.'\n                             % max_iter, g.fit, X)",
            "def test_gaussian_mixture_predict_predict_proba():\n    rng = np.random.RandomState(0)\n    rand_data = RandomData(rng)\n    for covar_type in COVARIANCE_TYPE:\n        X = rand_data.X[covar_type]\n        Y = rand_data.Y\n        g = GaussianMixture(n_components=rand_data.n_components,\n                            random_state=rng, weights_init=rand_data.weights,\n                            means_init=rand_data.means,\n                            precisions_init=rand_data.precisions[covar_type],\n                            covariance_type=covar_type)\n\n        # Check a warning message arrive if we don't do fit\n        assert_raise_message(NotFittedError,\n                             \"This GaussianMixture instance is not fitted \"\n                             \"yet. Call 'fit' with appropriate arguments \"\n                             \"before using this method.\", g.predict, X)\n\n        g.fit(X)\n        Y_pred = g.predict(X)\n        Y_pred_proba = g.predict_proba(X).argmax(axis=1)\n        assert_array_equal(Y_pred, Y_pred_proba)\n        assert_greater(adjusted_rand_score(Y, Y_pred), .95)",
            "def test_gaussian_mixture_fit():\n    # recover the ground truth\n    rng = np.random.RandomState(0)\n    rand_data = RandomData(rng)\n    n_features = rand_data.n_features\n    n_components = rand_data.n_components\n\n    for covar_type in COVARIANCE_TYPE:\n        X = rand_data.X[covar_type]\n        g = GaussianMixture(n_components=n_components, n_init=20,\n                            reg_covar=0, random_state=rng,\n                            covariance_type=covar_type)\n        g.fit(X)\n\n        # needs more data to pass the test with rtol=1e-7\n        assert_allclose(np.sort(g.weights_), np.sort(rand_data.weights),\n                        rtol=0.1, atol=1e-2)\n\n        arg_idx1 = g.means_[:, 0].argsort()\n        arg_idx2 = rand_data.means[:, 0].argsort()\n        assert_allclose(g.means_[arg_idx1], rand_data.means[arg_idx2],\n                        rtol=0.1, atol=1e-2)\n\n        if covar_type == 'full':\n            prec_pred = g.precisions_\n            prec_test = rand_data.precisions['full']\n        elif covar_type == 'tied':\n            prec_pred = np.array([g.precisions_] * n_components)\n            prec_test = np.array([rand_data.precisions['tied']] * n_components)\n        elif covar_type == 'spherical':\n            prec_pred = np.array([np.eye(n_features) * c\n                                 for c in g.precisions_])\n            prec_test = np.array([np.eye(n_features) * c for c in\n                                 rand_data.precisions['spherical']])\n        elif covar_type == 'diag':\n            prec_pred = np.array([np.diag(d) for d in g.precisions_])\n            prec_test = np.array([np.diag(d) for d in\n                                 rand_data.precisions['diag']])\n\n        arg_idx1 = np.trace(prec_pred, axis1=1, axis2=2).argsort()\n        arg_idx2 = np.trace(prec_test, axis1=1, axis2=2).argsort()\n        for k, h in zip(arg_idx1, arg_idx2):\n            ecov = EmpiricalCovariance()\n            ecov.covariance_ = prec_test[h]\n            # the accuracy depends on the number of data and randomness, rng\n            assert_allclose(ecov.error_norm(prec_pred[k]), 0, atol=0.1)",
            "def test_gaussian_mixture_attributes():\n    # test bad parameters\n    rng = np.random.RandomState(0)\n    X = rng.rand(10, 2)\n\n    n_components_bad = 0\n    gmm = GaussianMixture(n_components=n_components_bad)\n    assert_raise_message(ValueError,\n                         \"Invalid value for 'n_components': %d \"\n                         \"Estimation requires at least one component\"\n                         % n_components_bad, gmm.fit, X)\n\n    # covariance_type should be in [spherical, diag, tied, full]\n    covariance_type_bad = 'bad_covariance_type'\n    gmm = GaussianMixture(covariance_type=covariance_type_bad)\n    assert_raise_message(ValueError,\n                         \"Invalid value for 'covariance_type': %s \"\n                         \"'covariance_type' should be in \"\n                         \"['spherical', 'tied', 'diag', 'full']\"\n                         % covariance_type_bad,\n                         gmm.fit, X)\n\n    tol_bad = -1\n    gmm = GaussianMixture(tol=tol_bad)\n    assert_raise_message(ValueError,\n                         \"Invalid value for 'tol': %.5f \"\n                         \"Tolerance used by the EM must be non-negative\"\n                         % tol_bad, gmm.fit, X)\n\n    reg_covar_bad = -1\n    gmm = GaussianMixture(reg_covar=reg_covar_bad)\n    assert_raise_message(ValueError,\n                         \"Invalid value for 'reg_covar': %.5f \"\n                         \"regularization on covariance must be \"\n                         \"non-negative\" % reg_covar_bad, gmm.fit, X)\n\n    max_iter_bad = 0\n    gmm = GaussianMixture(max_iter=max_iter_bad)\n    assert_raise_message(ValueError,\n                         \"Invalid value for 'max_iter': %d \"\n                         \"Estimation requires at least one iteration\"\n                         % max_iter_bad, gmm.fit, X)\n\n    n_init_bad = 0\n    gmm = GaussianMixture(n_init=n_init_bad)\n    assert_raise_message(ValueError,\n                         \"Invalid value for 'n_init': %d \"\n                         \"Estimation requires at least one run\"\n                         % n_init_bad, gmm.fit, X)\n\n    init_params_bad = 'bad_method'\n    gmm = GaussianMixture(init_params=init_params_bad)\n    assert_raise_message(ValueError,\n                         \"Unimplemented initialization method '%s'\"\n                         % init_params_bad,\n                         gmm.fit, X)\n\n    # test good parameters\n    n_components, tol, n_init, max_iter, reg_covar = 2, 1e-4, 3, 30, 1e-1\n    covariance_type, init_params = 'full', 'random'\n    gmm = GaussianMixture(n_components=n_components, tol=tol, n_init=n_init,\n                          max_iter=max_iter, reg_covar=reg_covar,\n                          covariance_type=covariance_type,\n                          init_params=init_params).fit(X)\n\n    assert_equal(gmm.n_components, n_components)\n    assert_equal(gmm.covariance_type, covariance_type)\n    assert_equal(gmm.tol, tol)\n    assert_equal(gmm.reg_covar, reg_covar)\n    assert_equal(gmm.max_iter, max_iter)\n    assert_equal(gmm.n_init, n_init)\n    assert_equal(gmm.init_params, init_params)",
            "def test_gaussian_mixture_n_parameters():\n    # Test that the right number of parameters is estimated\n    rng = np.random.RandomState(0)\n    n_samples, n_features, n_components = 50, 5, 2\n    X = rng.randn(n_samples, n_features)\n    n_params = {'spherical': 13, 'diag': 21, 'tied': 26, 'full': 41}\n    for cv_type in COVARIANCE_TYPE:\n        g = GaussianMixture(\n            n_components=n_components, covariance_type=cv_type,\n            random_state=rng).fit(X)\n        assert_equal(g._n_parameters(), n_params[cv_type])",
            "def test_gaussian_mixture_fit_best_params():\n    rng = np.random.RandomState(0)\n    rand_data = RandomData(rng)\n    n_components = rand_data.n_components\n    n_init = 10\n    for covar_type in COVARIANCE_TYPE:\n        X = rand_data.X[covar_type]\n        g = GaussianMixture(n_components=n_components, n_init=1, reg_covar=0,\n                            random_state=rng, covariance_type=covar_type)\n        ll = []\n        for _ in range(n_init):\n            g.fit(X)\n            ll.append(g.score(X))\n        ll = np.array(ll)\n        g_best = GaussianMixture(n_components=n_components,\n                                 n_init=n_init, reg_covar=0, random_state=rng,\n                                 covariance_type=covar_type)\n        g_best.fit(X)\n        assert_almost_equal(ll.min(), g_best.score(X))",
            "def test_gaussian_mixture_verbose():\n    rng = np.random.RandomState(0)\n    rand_data = RandomData(rng)\n    n_components = rand_data.n_components\n    for covar_type in COVARIANCE_TYPE:\n        X = rand_data.X[covar_type]\n        g = GaussianMixture(n_components=n_components, n_init=1, reg_covar=0,\n                            random_state=rng, covariance_type=covar_type,\n                            verbose=1)\n        h = GaussianMixture(n_components=n_components, n_init=1, reg_covar=0,\n                            random_state=rng, covariance_type=covar_type,\n                            verbose=2)\n        old_stdout = sys.stdout\n        sys.stdout = StringIO()\n        try:\n            g.fit(X)\n            h.fit(X)\n        finally:\n            sys.stdout = old_stdout"
        ]
    },
    {
        "repo": "scikit-learn/scikit-learn",
        "instance_id": "scikit-learn__scikit-learn-13779",
        "base_commit": "b34751b7ed02b2cfcc36037fb729d4360480a299",
        "issue_title": "Voting estimator will fail at fit if weights are passed and an estimator is None",
        "test_file": "/home/zqc/RepoCodeEdit/data/swe_bench_temp_wy/scikit-learn/sklearn/ensemble/tests/test_voting.py",
        "searched_functions": [
            "def test_notfitted():\n    eclf = VotingClassifier(estimators=[('lr1', LogisticRegression()),\n                                        ('lr2', LogisticRegression())],\n                            voting='soft')\n    ereg = VotingRegressor([('dr', DummyRegressor())])\n    msg = (\"This %s instance is not fitted yet. Call \\'fit\\'\"\n           \" with appropriate arguments before using this method.\")\n    assert_raise_message(NotFittedError, msg % 'VotingClassifier',\n                         eclf.predict, X)\n    assert_raise_message(NotFittedError, msg % 'VotingClassifier',\n                         eclf.predict_proba, X)\n    assert_raise_message(NotFittedError, msg % 'VotingClassifier',\n                         eclf.transform, X)\n    assert_raise_message(NotFittedError, msg % 'VotingRegressor',\n                         ereg.predict, X_r)\n    assert_raise_message(NotFittedError, msg % 'VotingRegressor',\n                         ereg.transform, X_r)",
            "def test_set_estimator_none():\n    \"\"\"VotingClassifier set_params should be able to set estimators as None\"\"\"\n    # Test predict\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    clf3 = GaussianNB()\n    eclf1 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2),\n                                         ('nb', clf3)],\n                             voting='hard', weights=[1, 0, 0.5]).fit(X, y)\n\n    eclf2 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2),\n                                         ('nb', clf3)],\n                             voting='hard', weights=[1, 1, 0.5])\n    eclf2.set_params(rf=None).fit(X, y)\n    assert_array_equal(eclf1.predict(X), eclf2.predict(X))\n\n    assert dict(eclf2.estimators)[\"rf\"] is None\n    assert len(eclf2.estimators_) == 2\n    assert all(isinstance(est, (LogisticRegression, GaussianNB))\n               for est in eclf2.estimators_)\n    assert eclf2.get_params()[\"rf\"] is None\n\n    eclf1.set_params(voting='soft').fit(X, y)\n    eclf2.set_params(voting='soft').fit(X, y)\n    assert_array_equal(eclf1.predict(X), eclf2.predict(X))\n    assert_array_almost_equal(eclf1.predict_proba(X), eclf2.predict_proba(X))\n    msg = 'All estimators are None. At least one is required!'\n    assert_raise_message(\n        ValueError, msg, eclf2.set_params(lr=None, rf=None, nb=None).fit, X, y)\n\n    # Test soft voting transform\n    X1 = np.array([[1], [2]])\n    y1 = np.array([1, 2])\n    eclf1 = VotingClassifier(estimators=[('rf', clf2), ('nb', clf3)],\n                             voting='soft', weights=[0, 0.5],\n                             flatten_transform=False).fit(X1, y1)\n\n    eclf2 = VotingClassifier(estimators=[('rf', clf2), ('nb', clf3)],\n                             voting='soft', weights=[1, 0.5],\n                             flatten_transform=False)\n    eclf2.set_params(rf=None).fit(X1, y1)\n    assert_array_almost_equal(eclf1.transform(X1),\n                              np.array([[[0.7, 0.3], [0.3, 0.7]],\n                                        [[1., 0.], [0., 1.]]]))\n    assert_array_almost_equal(eclf2.transform(X1),\n                              np.array([[[1., 0.],\n                                         [0., 1.]]]))\n    eclf1.set_params(voting='hard')\n    eclf2.set_params(voting='hard')\n    assert_array_equal(eclf1.transform(X1), np.array([[0, 0], [1, 1]]))\n    assert_array_equal(eclf2.transform(X1), np.array([[0], [1]]))",
            "def test_estimator_init():\n    eclf = VotingClassifier(estimators=[])\n    msg = ('Invalid `estimators` attribute, `estimators` should be'\n           ' a list of (string, estimator) tuples')\n    assert_raise_message(AttributeError, msg, eclf.fit, X, y)\n\n    clf = LogisticRegression(random_state=1)\n\n    eclf = VotingClassifier(estimators=[('lr', clf)], voting='error')\n    msg = ('Voting must be \\'soft\\' or \\'hard\\'; got (voting=\\'error\\')')\n    assert_raise_message(ValueError, msg, eclf.fit, X, y)\n\n    eclf = VotingClassifier(estimators=[('lr', clf)], weights=[1, 2])\n    msg = ('Number of `estimators` and weights must be equal'\n           '; got 2 weights, 1 estimators')\n    assert_raise_message(ValueError, msg, eclf.fit, X, y)\n\n    eclf = VotingClassifier(estimators=[('lr', clf), ('lr', clf)],\n                            weights=[1, 2])\n    msg = \"Names provided are not unique: ['lr', 'lr']\"\n    assert_raise_message(ValueError, msg, eclf.fit, X, y)\n\n    eclf = VotingClassifier(estimators=[('lr__', clf)])\n    msg = \"Estimator names must not contain __: got ['lr__']\"\n    assert_raise_message(ValueError, msg, eclf.fit, X, y)\n\n    eclf = VotingClassifier(estimators=[('estimators', clf)])\n    msg = \"Estimator names conflict with constructor arguments: ['estimators']\"\n    assert_raise_message(ValueError, msg, eclf.fit, X, y)",
            "def test_estimator_weights_format():\n    # Test estimator weights inputs as list and array\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    eclf1 = VotingClassifier(estimators=[\n                ('lr', clf1), ('rf', clf2)],\n                weights=[1, 2],\n                voting='soft')\n    eclf2 = VotingClassifier(estimators=[\n                ('lr', clf1), ('rf', clf2)],\n                weights=np.array((1, 2)),\n                voting='soft')\n    eclf1.fit(X, y)\n    eclf2.fit(X, y)\n    assert_array_almost_equal(eclf1.predict_proba(X), eclf2.predict_proba(X))",
            "def test_sample_weight():\n    \"\"\"Tests sample_weight parameter of VotingClassifier\"\"\"\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    clf3 = SVC(gamma='scale', probability=True, random_state=123)\n    eclf1 = VotingClassifier(estimators=[\n        ('lr', clf1), ('rf', clf2), ('svc', clf3)],\n        voting='soft').fit(X, y, sample_weight=np.ones((len(y),)))\n    eclf2 = VotingClassifier(estimators=[\n        ('lr', clf1), ('rf', clf2), ('svc', clf3)],\n        voting='soft').fit(X, y)\n    assert_array_equal(eclf1.predict(X), eclf2.predict(X))\n    assert_array_almost_equal(eclf1.predict_proba(X), eclf2.predict_proba(X))\n\n    sample_weight = np.random.RandomState(123).uniform(size=(len(y),))\n    eclf3 = VotingClassifier(estimators=[('lr', clf1)], voting='soft')\n    eclf3.fit(X, y, sample_weight)\n    clf1.fit(X, y, sample_weight)\n    assert_array_equal(eclf3.predict(X), clf1.predict(X))\n    assert_array_almost_equal(eclf3.predict_proba(X), clf1.predict_proba(X))\n\n    clf4 = KNeighborsClassifier()\n    eclf3 = VotingClassifier(estimators=[\n        ('lr', clf1), ('svc', clf3), ('knn', clf4)],\n        voting='soft')\n    msg = ('Underlying estimator \\'knn\\' does not support sample weights.')\n    assert_raise_message(ValueError, msg, eclf3.fit, X, y, sample_weight)",
            "def test_predictproba_hardvoting():\n    eclf = VotingClassifier(estimators=[('lr1', LogisticRegression()),\n                                        ('lr2', LogisticRegression())],\n                            voting='hard')\n    msg = \"predict_proba is not available when voting='hard'\"\n    assert_raise_message(AttributeError, msg, eclf.predict_proba, X)",
            "def test_sample_weight_kwargs():\n    \"\"\"Check that VotingClassifier passes sample_weight as kwargs\"\"\"\n    class MockClassifier(BaseEstimator, ClassifierMixin):\n        \"\"\"Mock Classifier to check that sample_weight is received as kwargs\"\"\"\n        def fit(self, X, y, *args, **sample_weight):\n            assert 'sample_weight' in sample_weight\n\n    clf = MockClassifier()\n    eclf = VotingClassifier(estimators=[('mock', clf)], voting='soft')\n\n    # Should not raise an error.\n    eclf.fit(X, y, sample_weight=np.ones((len(y),)))",
            "def fit(self, X, y, *args, **sample_weight):\n            assert 'sample_weight' in sample_weight",
            "def test_weights_regressor():\n    \"\"\"Check weighted average regression prediction on boston dataset.\"\"\"\n    reg1 = DummyRegressor(strategy='mean')\n    reg2 = DummyRegressor(strategy='median')\n    reg3 = DummyRegressor(strategy='quantile', quantile=.2)\n    ereg = VotingRegressor([('mean', reg1), ('median', reg2),\n                            ('quantile', reg3)], weights=[1, 2, 10])\n\n    X_r_train, X_r_test, y_r_train, y_r_test = \\\n        train_test_split(X_r, y_r, test_size=.25)\n\n    reg1_pred = reg1.fit(X_r_train, y_r_train).predict(X_r_test)\n    reg2_pred = reg2.fit(X_r_train, y_r_train).predict(X_r_test)\n    reg3_pred = reg3.fit(X_r_train, y_r_train).predict(X_r_test)\n    ereg_pred = ereg.fit(X_r_train, y_r_train).predict(X_r_test)\n\n    avg = np.average(np.asarray([reg1_pred, reg2_pred, reg3_pred]), axis=0,\n                     weights=[1, 2, 10])\n    assert_almost_equal(ereg_pred, avg, decimal=2)\n\n    ereg_weights_none = VotingRegressor([('mean', reg1), ('median', reg2),\n                                         ('quantile', reg3)], weights=None)\n    ereg_weights_equal = VotingRegressor([('mean', reg1), ('median', reg2),\n                                          ('quantile', reg3)],\n                                         weights=[1, 1, 1])\n    ereg_weights_none.fit(X_r_train, y_r_train)\n    ereg_weights_equal.fit(X_r_train, y_r_train)\n    ereg_none_pred = ereg_weights_none.predict(X_r_test)\n    ereg_equal_pred = ereg_weights_equal.predict(X_r_test)\n    assert_almost_equal(ereg_none_pred, ereg_equal_pred, decimal=2)",
            "def test_set_params():\n    \"\"\"set_params should be able to set estimators\"\"\"\n    clf1 = LogisticRegression(random_state=123, C=1.0)\n    clf2 = RandomForestClassifier(random_state=123, max_depth=None)\n    clf3 = GaussianNB()\n    eclf1 = VotingClassifier([('lr', clf1), ('rf', clf2)], voting='soft',\n                             weights=[1, 2])\n    assert 'lr' in eclf1.named_estimators\n    assert eclf1.named_estimators.lr is eclf1.estimators[0][1]\n    assert eclf1.named_estimators.lr is eclf1.named_estimators['lr']\n    eclf1.fit(X, y)\n    assert 'lr' in eclf1.named_estimators_\n    assert eclf1.named_estimators_.lr is eclf1.estimators_[0]\n    assert eclf1.named_estimators_.lr is eclf1.named_estimators_['lr']\n\n    eclf2 = VotingClassifier([('lr', clf1), ('nb', clf3)], voting='soft',\n                             weights=[1, 2])\n    eclf2.set_params(nb=clf2).fit(X, y)\n    assert not hasattr(eclf2, 'nb')\n\n    assert_array_equal(eclf1.predict(X), eclf2.predict(X))\n    assert_array_almost_equal(eclf1.predict_proba(X), eclf2.predict_proba(X))\n    assert_equal(eclf2.estimators[0][1].get_params(), clf1.get_params())\n    assert_equal(eclf2.estimators[1][1].get_params(), clf2.get_params())\n\n    eclf1.set_params(lr__C=10.0)\n    eclf2.set_params(nb__max_depth=5)\n\n    assert eclf1.estimators[0][1].get_params()['C'] == 10.0\n    assert eclf2.estimators[1][1].get_params()['max_depth'] == 5\n    assert_equal(eclf1.get_params()[\"lr__C\"],\n                 eclf1.get_params()[\"lr\"].get_params()['C'])"
        ]
    },
    {
        "repo": "scikit-learn/scikit-learn",
        "instance_id": "scikit-learn__scikit-learn-10949",
        "base_commit": "3b5abf76597ce6aff76192869f92647c1b5259e7",
        "issue_title": "warn_on_dtype with DataFrame",
        "test_file": "/home/zqc/RepoCodeEdit/data/swe_bench_temp_wy/scikit-learn/sklearn/tests/test_common.py",
        "searched_functions": [
            "def test_no_attributes_set_in_init(name, Estimator):\n    # input validation etc for non-meta estimators\n    with ignore_warnings(category=(DeprecationWarning, ConvergenceWarning,\n                                   UserWarning, FutureWarning)):\n        estimator = Estimator()\n        # check this on class\n        check_no_attributes_set_in_init(name, estimator)",
            "def _tested_linear_classifiers():\n    classifiers = all_estimators(type_filter='classifier')\n\n    clean_warning_registry()\n    with warnings.catch_warnings(record=True):\n        for name, clazz in classifiers:\n            if ('class_weight' in clazz().get_params().keys() and\n                    issubclass(clazz, LinearClassifierMixin)):\n                yield name, clazz",
            "def test_class_weight_balanced_linear_classifiers(name, Classifier):\n    check_class_weight_balanced_linear_classifier(name, Classifier)",
            "def test_non_meta_estimators(name, Estimator, check):\n    # Common tests for non-meta estimators\n    with ignore_warnings(category=(DeprecationWarning, ConvergenceWarning,\n                                   UserWarning, FutureWarning)):\n        estimator = Estimator()\n        set_checking_parameters(estimator)\n        check(name, estimator)",
            "def test_all_estimator_no_base_class():\n    # test that all_estimators doesn't find abstract classes.\n    for name, Estimator in all_estimators():\n        msg = (\"Base estimators such as {0} should not be included\"\n               \" in all_estimators\").format(name)\n        assert_false(name.lower().startswith('base'), msg=msg)",
            "def test_import_all_consistency():\n    # Smoke test to check that any name in a __all__ list is actually defined\n    # in the namespace of the module or package.\n    pkgs = pkgutil.walk_packages(path=sklearn.__path__, prefix='sklearn.',\n                                 onerror=lambda _: None)\n    submods = [modname for _, modname, _ in pkgs]\n    for modname in submods + ['sklearn']:\n        if \".tests.\" in modname:\n            continue\n        package = __import__(modname, fromlist=\"dummy\")\n        for name in getattr(package, '__all__', ()):\n            if getattr(package, name, None) is None:\n                raise AttributeError(\n                    \"Module '{0}' has no attribute '{1}'\".format(\n                        modname, name))",
            "def test_configure():\n    # Smoke test the 'configure' step of setup, this tests all the\n    # 'configure' functions in the setup.pys in scikit-learn\n    cwd = os.getcwd()\n    setup_path = os.path.abspath(os.path.join(sklearn.__path__[0], '..'))\n    setup_filename = os.path.join(setup_path, 'setup.py')\n    if not os.path.exists(setup_filename):\n        return\n    try:\n        os.chdir(setup_path)\n        old_argv = sys.argv\n        sys.argv = ['setup.py', 'config']\n        clean_warning_registry()\n        with warnings.catch_warnings():\n            # The configuration spits out warnings when not finding\n            # Blas/Atlas development headers\n            warnings.simplefilter('ignore', UserWarning)\n            with open('setup.py') as f:\n                exec(f.read(), dict(__name__='__main__'))\n    finally:\n        sys.argv = old_argv\n        os.chdir(cwd)",
            "def test_parameters_default_constructible(name, Estimator):\n    # Test that estimators are default-constructible\n    check_parameters_default_constructible(name, Estimator)",
            "def test_all_estimators():\n    estimators = all_estimators(include_meta_estimators=True)\n\n    # Meta sanity-check to make sure that the estimator introspection runs\n    # properly\n    assert_greater(len(estimators), 0)",
            "def test_root_import_all_completeness():\n    EXCEPTIONS = ('utils', 'tests', 'base', 'setup')\n    for _, modname, _ in pkgutil.walk_packages(path=sklearn.__path__,\n                                               onerror=lambda _: None):\n        if '.' in modname or modname.startswith('_') or modname in EXCEPTIONS:\n            continue\n        assert_in(modname, sklearn.__all__)"
        ]
    },
    {
        "repo": "scikit-learn/scikit-learn",
        "instance_id": "scikit-learn__scikit-learn-13584",
        "base_commit": "0e3c1879b06d839171b7d0a607d71bbb19a966a9",
        "issue_title": "bug in print_changed_only in new repr: vector values",
        "test_file": "/home/zqc/RepoCodeEdit/data/swe_bench_temp_wy/scikit-learn/sklearn/tests/test_common.py",
        "searched_functions": [
            "def _rename_partial(val):\n    if isinstance(val, functools.partial):\n        kwstring = \"\".join([\"{}={}\".format(k, v)\n                            for k, v in val.keywords.items()])\n        return \"{}({})\".format(val.func.__name__, kwstring)\n    # FIXME once we have short reprs we can use them here!\n    if hasattr(val, \"get_params\") and not isinstance(val, type):\n        return type(val).__name__",
            "def _generate_checks_per_estimator(check_generator, estimators):\n    with ignore_warnings(category=(DeprecationWarning, FutureWarning)):\n        for name, estimator in estimators:\n            for check in check_generator(name, estimator):\n                yield estimator, check",
            "def _tested_linear_classifiers():\n    classifiers = all_estimators(type_filter='classifier')\n\n    clean_warning_registry()\n    with warnings.catch_warnings(record=True):\n        for name, clazz in classifiers:\n            required_parameters = getattr(clazz, \"_required_parameters\", [])\n            if len(required_parameters):\n                # FIXME\n                continue\n\n            if ('class_weight' in clazz().get_params().keys() and\n                    issubclass(clazz, LinearClassifierMixin)):\n                yield name, clazz",
            "def _tested_estimators():\n    for name, Estimator in all_estimators():\n        if issubclass(Estimator, BiclusterMixin):\n            continue\n        if name.startswith(\"_\"):\n            continue\n        # FIXME _skip_test should be used here (if we could)\n\n        required_parameters = getattr(Estimator, \"_required_parameters\", [])\n        if len(required_parameters):\n            if required_parameters in ([\"estimator\"], [\"base_estimator\"]):\n                if issubclass(Estimator, RegressorMixin):\n                    estimator = Estimator(Ridge())\n                else:\n                    estimator = Estimator(LinearDiscriminantAnalysis())\n            else:\n                warnings.warn(\"Can't instantiate estimator {} which requires \"\n                              \"parameters {}\".format(name,\n                                                     required_parameters),\n                              SkipTestWarning)\n                continue\n        else:\n            estimator = Estimator()\n        yield name, estimator",
            "def test_no_attributes_set_in_init(name, estimator):\n    # input validation etc for all estimators\n    with ignore_warnings(category=(DeprecationWarning, ConvergenceWarning,\n                                   UserWarning, FutureWarning)):\n        tags = _safe_tags(estimator)\n        if tags['_skip_test']:\n            warnings.warn(\"Explicit SKIP via _skip_test tag for \"\n                          \"{}.\".format(name),\n                          SkipTestWarning)\n            return\n        # check this on class\n        check_no_attributes_set_in_init(name, estimator)",
            "def test_import_all_consistency():\n    # Smoke test to check that any name in a __all__ list is actually defined\n    # in the namespace of the module or package.\n    pkgs = pkgutil.walk_packages(path=sklearn.__path__, prefix='sklearn.',\n                                 onerror=lambda _: None)\n    submods = [modname for _, modname, _ in pkgs]\n    for modname in submods + ['sklearn']:\n        if \".tests.\" in modname:\n            continue\n        if IS_PYPY and ('_svmlight_format' in modname or\n                        'feature_extraction._hashing' in modname):\n            continue\n        package = __import__(modname, fromlist=\"dummy\")\n        for name in getattr(package, '__all__', ()):\n            if getattr(package, name, None) is None:\n                raise AttributeError(\n                    \"Module '{0}' has no attribute '{1}'\".format(\n                        modname, name))",
            "def test_all_estimator_no_base_class():\n    # test that all_estimators doesn't find abstract classes.\n    for name, Estimator in all_estimators():\n        msg = (\"Base estimators such as {0} should not be included\"\n               \" in all_estimators\").format(name)\n        assert not name.lower().startswith('base'), msg",
            "def test_estimators(estimator, check):\n    # Common tests for estimator instances\n    with ignore_warnings(category=(DeprecationWarning, ConvergenceWarning,\n                                   UserWarning, FutureWarning)):\n        set_checking_parameters(estimator)\n        name = estimator.__class__.__name__\n        check(name, estimator)",
            "def test_configure():\n    # Smoke test the 'configure' step of setup, this tests all the\n    # 'configure' functions in the setup.pys in scikit-learn\n    cwd = os.getcwd()\n    setup_path = os.path.abspath(os.path.join(sklearn.__path__[0], '..'))\n    setup_filename = os.path.join(setup_path, 'setup.py')\n    if not os.path.exists(setup_filename):\n        return\n    try:\n        os.chdir(setup_path)\n        old_argv = sys.argv\n        sys.argv = ['setup.py', 'config']\n        clean_warning_registry()\n        with warnings.catch_warnings():\n            # The configuration spits out warnings when not finding\n            # Blas/Atlas development headers\n            warnings.simplefilter('ignore', UserWarning)\n            with open('setup.py') as f:\n                exec(f.read(), dict(__name__='__main__'))\n    finally:\n        sys.argv = old_argv\n        os.chdir(cwd)",
            "def test_parameters_default_constructible(name, Estimator):\n    # Test that estimators are default-constructible\n    check_parameters_default_constructible(name, Estimator)"
        ]
    },
    {
        "repo": "scikit-learn/scikit-learn",
        "instance_id": "scikit-learn__scikit-learn-14087",
        "base_commit": "a5743ed36fbd3fbc8e351bdab16561fbfca7dfa1",
        "issue_title": "IndexError thrown with LogisticRegressionCV and refit=False",
        "test_file": "/home/zqc/RepoCodeEdit/data/swe_bench_temp_wy/scikit-learn/sklearn/linear_model/tests/test_logistic.py",
        "searched_functions": [
            "def test_LogisticRegressionCV_no_refit(multi_class):\n    # Test LogisticRegressionCV attribute shapes when refit is False\n\n    n_classes = 3\n    n_features = 20\n    X, y = make_classification(n_samples=200, n_classes=n_classes,\n                               n_informative=n_classes, n_features=n_features,\n                               random_state=0)\n\n    Cs = np.logspace(-4, 4, 3)\n    l1_ratios = np.linspace(0, 1, 2)\n\n    lrcv = LogisticRegressionCV(penalty='elasticnet', Cs=Cs, solver='saga',\n                                l1_ratios=l1_ratios, random_state=0,\n                                multi_class=multi_class, refit=False)\n    lrcv.fit(X, y)\n    assert lrcv.C_.shape == (n_classes,)\n    assert lrcv.l1_ratio_.shape == (n_classes,)\n    assert lrcv.coef_.shape == (n_classes, n_features)",
            "def test_logistic_regression_cv_refit(random_seed, penalty):\n    # Test that when refit=True, logistic regression cv with the saga solver\n    # converges to the same solution as logistic regression with a fixed\n    # regularization parameter.\n    # Internally the LogisticRegressionCV model uses a warm start to refit on\n    # the full data model with the optimal C found by CV. As the penalized\n    # logistic regression loss is convex, we should still recover exactly\n    # the same solution as long as the stopping criterion is strict enough (and\n    # that there are no exactly duplicated features when penalty='l1').\n    X, y = make_classification(n_samples=50, n_features=20,\n                               random_state=random_seed)\n    common_params = dict(\n        solver='saga',\n        penalty=penalty,\n        random_state=random_seed,\n        max_iter=10000,\n        tol=1e-12,\n    )\n    lr_cv = LogisticRegressionCV(Cs=[1.0], refit=True, **common_params)\n    lr_cv.fit(X, y)\n    lr = LogisticRegression(C=1.0, **common_params)\n    lr.fit(X, y)\n    assert_array_almost_equal(lr_cv.coef_, lr.coef_)",
            "def test_logistic_cv():\n    # test for LogisticRegressionCV object\n    n_samples, n_features = 50, 5\n    rng = np.random.RandomState(0)\n    X_ref = rng.randn(n_samples, n_features)\n    y = np.sign(X_ref.dot(5 * rng.randn(n_features)))\n    X_ref -= X_ref.mean()\n    X_ref /= X_ref.std()\n    lr_cv = LogisticRegressionCV(Cs=[1.], fit_intercept=False,\n                                 solver='liblinear', multi_class='ovr', cv=3)\n    lr_cv.fit(X_ref, y)\n    lr = LogisticRegression(C=1., fit_intercept=False,\n                            solver='liblinear', multi_class='ovr')\n    lr.fit(X_ref, y)\n    assert_array_almost_equal(lr.coef_, lr_cv.coef_)\n\n    assert_array_equal(lr_cv.coef_.shape, (1, n_features))\n    assert_array_equal(lr_cv.classes_, [-1, 1])\n    assert_equal(len(lr_cv.classes_), 2)\n\n    coefs_paths = np.asarray(list(lr_cv.coefs_paths_.values()))\n    assert_array_equal(coefs_paths.shape, (1, 3, 1, n_features))\n    assert_array_equal(lr_cv.Cs_.shape, (1,))\n    scores = np.asarray(list(lr_cv.scores_.values()))\n    assert_array_equal(scores.shape, (1, 3, 1))",
            "def test_logistic_cv_sparse():\n    X, y = make_classification(n_samples=50, n_features=5,\n                               random_state=0)\n    X[X < 1.0] = 0.0\n    csr = sp.csr_matrix(X)\n\n    clf = LogisticRegressionCV(fit_intercept=True)\n    clf.fit(X, y)\n    clfs = LogisticRegressionCV(fit_intercept=True)\n    clfs.fit(csr, y)\n    assert_array_almost_equal(clfs.coef_, clf.coef_)\n    assert_array_almost_equal(clfs.intercept_, clf.intercept_)\n    assert_equal(clfs.C_, clf.C_)",
            "def test_liblinear_logregcv_sparse():\n    # Test LogRegCV with solver='liblinear' works for sparse matrices\n\n    X, y = make_classification(n_samples=10, n_features=5, random_state=0)\n    clf = LogisticRegressionCV(solver='liblinear', multi_class='ovr')\n    clf.fit(sparse.csr_matrix(X), y)",
            "def test_logistic_cv_score_does_not_warn_by_default():\n    lr = LogisticRegressionCV(cv=2)\n    lr.fit(X, Y1)\n\n    with pytest.warns(None) as record:\n        lr.score(X, lr.predict(X))\n    assert len(record) == 0",
            "def test_LogisticRegressionCV_GridSearchCV_elastic_net_ovr():\n    # make sure LogisticRegressionCV gives same best params (l1 and C) as\n    # GridSearchCV when penalty is elasticnet and multiclass is ovr. We can't\n    # compare best_params like in the previous test because\n    # LogisticRegressionCV with multi_class='ovr' will have one C and one\n    # l1_param for each class, while LogisticRegression will share the\n    # parameters over the *n_classes* classifiers.\n\n    X, y = make_classification(n_samples=200, n_classes=3, n_informative=3,\n                               random_state=0)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n    cv = StratifiedKFold(5, random_state=0)\n\n    l1_ratios = np.linspace(0, 1, 5)\n    Cs = np.logspace(-4, 4, 5)\n\n    lrcv = LogisticRegressionCV(penalty='elasticnet', Cs=Cs, solver='saga',\n                                cv=cv, l1_ratios=l1_ratios, random_state=0,\n                                multi_class='ovr')\n    lrcv.fit(X_train, y_train)\n\n    param_grid = {'C': Cs, 'l1_ratio': l1_ratios}\n    lr = LogisticRegression(penalty='elasticnet', solver='saga',\n                            random_state=0, multi_class='ovr')\n    gs = GridSearchCV(lr, param_grid, cv=cv)\n    gs.fit(X_train, y_train)\n\n    # Check that predictions are 80% the same\n    assert (lrcv.predict(X_train) == gs.predict(X_train)).mean() >= .8\n    assert (lrcv.predict(X_test) == gs.predict(X_test)).mean() >= .8",
            "def test_error():\n    # Test for appropriate exception on errors\n    msg = \"Penalty term must be positive\"\n    assert_raise_message(ValueError, msg,\n                         LogisticRegression(C=-1).fit, X, Y1)\n    assert_raise_message(ValueError, msg,\n                         LogisticRegression(C=\"test\").fit, X, Y1)\n\n    msg = \"is not a valid scoring value\"\n    assert_raise_message(ValueError, msg,\n                         LogisticRegressionCV(scoring='bad-scorer', cv=2).fit,\n                         X, Y1)\n\n    for LR in [LogisticRegression, LogisticRegressionCV]:\n        msg = \"Tolerance for stopping criteria must be positive\"\n        assert_raise_message(ValueError, msg, LR(tol=-1).fit, X, Y1)\n        assert_raise_message(ValueError, msg, LR(tol=\"test\").fit, X, Y1)\n\n        msg = \"Maximum number of iteration must be positive\"\n        assert_raise_message(ValueError, msg, LR(max_iter=-1).fit, X, Y1)\n        assert_raise_message(ValueError, msg, LR(max_iter=\"test\").fit, X, Y1)",
            "def test_LogisticRegressionCV_GridSearchCV_elastic_net(multi_class):\n    # make sure LogisticRegressionCV gives same best params (l1 and C) as\n    # GridSearchCV when penalty is elasticnet\n\n    if multi_class == 'ovr':\n        # This is actually binary classification, ovr multiclass is treated in\n        # test_LogisticRegressionCV_GridSearchCV_elastic_net_ovr\n        X, y = make_classification(random_state=0)\n    else:\n        X, y = make_classification(n_samples=200, n_classes=3, n_informative=3,\n                                   random_state=0)\n\n    cv = StratifiedKFold(5, random_state=0)\n\n    l1_ratios = np.linspace(0, 1, 5)\n    Cs = np.logspace(-4, 4, 5)\n\n    lrcv = LogisticRegressionCV(penalty='elasticnet', Cs=Cs, solver='saga',\n                                cv=cv, l1_ratios=l1_ratios, random_state=0,\n                                multi_class=multi_class)\n    lrcv.fit(X, y)\n\n    param_grid = {'C': Cs, 'l1_ratio': l1_ratios}\n    lr = LogisticRegression(penalty='elasticnet', solver='saga',\n                            random_state=0, multi_class=multi_class)\n    gs = GridSearchCV(lr, param_grid, cv=cv)\n    gs.fit(X, y)\n\n    assert gs.best_params_['l1_ratio'] == lrcv.l1_ratio_[0]\n    assert gs.best_params_['C'] == lrcv.C_[0]",
            "def test_logistic_regression_multinomial():\n    # Tests for the multinomial option in logistic regression\n\n    # Some basic attributes of Logistic Regression\n    n_samples, n_features, n_classes = 50, 20, 3\n    X, y = make_classification(n_samples=n_samples,\n                               n_features=n_features,\n                               n_informative=10,\n                               n_classes=n_classes, random_state=0)\n\n    # 'lbfgs' is used as a referenced\n    solver = 'lbfgs'\n    ref_i = LogisticRegression(solver=solver, multi_class='multinomial')\n    ref_w = LogisticRegression(solver=solver, multi_class='multinomial',\n                               fit_intercept=False)\n    ref_i.fit(X, y)\n    ref_w.fit(X, y)\n    assert_array_equal(ref_i.coef_.shape, (n_classes, n_features))\n    assert_array_equal(ref_w.coef_.shape, (n_classes, n_features))\n    for solver in ['sag', 'saga', 'newton-cg']:\n        clf_i = LogisticRegression(solver=solver, multi_class='multinomial',\n                                   random_state=42, max_iter=2000, tol=1e-7,\n                                   )\n        clf_w = LogisticRegression(solver=solver, multi_class='multinomial',\n                                   random_state=42, max_iter=2000, tol=1e-7,\n                                   fit_intercept=False)\n        clf_i.fit(X, y)\n        clf_w.fit(X, y)\n        assert_array_equal(clf_i.coef_.shape, (n_classes, n_features))\n        assert_array_equal(clf_w.coef_.shape, (n_classes, n_features))\n\n        # Compare solutions between lbfgs and the other solvers\n        assert_almost_equal(ref_i.coef_, clf_i.coef_, decimal=3)\n        assert_almost_equal(ref_w.coef_, clf_w.coef_, decimal=3)\n        assert_almost_equal(ref_i.intercept_, clf_i.intercept_, decimal=3)\n\n    # Test that the path give almost the same results. However since in this\n    # case we take the average of the coefs after fitting across all the\n    # folds, it need not be exactly the same.\n    for solver in ['lbfgs', 'newton-cg', 'sag', 'saga']:\n        clf_path = LogisticRegressionCV(solver=solver, max_iter=2000, tol=1e-6,\n                                        multi_class='multinomial', Cs=[1.])\n        clf_path.fit(X, y)\n        assert_array_almost_equal(clf_path.coef_, ref_i.coef_, decimal=3)\n        assert_almost_equal(clf_path.intercept_, ref_i.intercept_, decimal=3)"
        ]
    },
    {
        "repo": "scikit-learn/scikit-learn",
        "instance_id": "scikit-learn__scikit-learn-14092",
        "base_commit": "df7dd8391148a873d157328a4f0328528a0c4ed9",
        "issue_title": "NCA fails in GridSearch due to too strict parameter checks",
        "test_file": "/home/zqc/RepoCodeEdit/data/swe_bench_temp_wy/scikit-learn/sklearn/neighbors/tests/test_nca.py",
        "searched_functions": [
            "def test_params_validation():\n    # Test that invalid parameters raise value error\n    X = np.arange(12).reshape(4, 3)\n    y = [1, 1, 2, 2]\n    NCA = NeighborhoodComponentsAnalysis\n    rng = np.random.RandomState(42)\n\n    # TypeError\n    assert_raises(TypeError, NCA(max_iter='21').fit, X, y)\n    assert_raises(TypeError, NCA(verbose='true').fit, X, y)\n    assert_raises(TypeError, NCA(tol=1).fit, X, y)\n    assert_raises(TypeError, NCA(n_components='invalid').fit, X, y)\n    assert_raises(TypeError, NCA(warm_start=1).fit, X, y)\n\n    # ValueError\n    assert_raise_message(ValueError,\n                         \"`init` must be 'auto', 'pca', 'lda', 'identity', \"\n                         \"'random' or a numpy array of shape \"\n                         \"(n_components, n_features).\",\n                         NCA(init=1).fit, X, y)\n    assert_raise_message(ValueError,\n                         '`max_iter`= -1, must be >= 1.',\n                         NCA(max_iter=-1).fit, X, y)\n\n    init = rng.rand(5, 3)\n    assert_raise_message(ValueError,\n                         'The output dimensionality ({}) of the given linear '\n                         'transformation `init` cannot be greater than its '\n                         'input dimensionality ({}).'\n                         .format(init.shape[0], init.shape[1]),\n                         NCA(init=init).fit, X, y)\n\n    n_components = 10\n    assert_raise_message(ValueError,\n                         'The preferred dimensionality of the '\n                         'projected space `n_components` ({}) cannot '\n                         'be greater than the given data '\n                         'dimensionality ({})!'\n                         .format(n_components, X.shape[1]),\n                         NCA(n_components=n_components).fit, X, y)",
            "def test_convergence_warning():\n    nca = NeighborhoodComponentsAnalysis(max_iter=2, verbose=1)\n    cls_name = nca.__class__.__name__\n    assert_warns_message(ConvergenceWarning,\n                         '[{}] NCA did not converge'.format(cls_name),\n                         nca.fit, iris_data, iris_target)",
            "def test_auto_init(n_samples, n_features, n_classes, n_components):\n    # Test that auto choose the init as expected with every configuration\n    # of order of n_samples, n_features, n_classes and n_components.\n    rng = np.random.RandomState(42)\n    nca_base = NeighborhoodComponentsAnalysis(init='auto',\n                                              n_components=n_components,\n                                              max_iter=1,\n                                              random_state=rng)\n    if n_classes >= n_samples:\n        pass\n        # n_classes > n_samples is impossible, and n_classes == n_samples\n        # throws an error from lda but is an absurd case\n    else:\n        X = rng.randn(n_samples, n_features)\n        y = np.tile(range(n_classes), n_samples // n_classes + 1)[:n_samples]\n        if n_components > n_features:\n            # this would return a ValueError, which is already tested in\n            # test_params_validation\n            pass\n        else:\n            nca = clone(nca_base)\n            nca.fit(X, y)\n            if n_components <= min(n_classes - 1, n_features):\n                nca_other = clone(nca_base).set_params(init='lda')\n            elif n_components < min(n_features, n_samples):\n                nca_other = clone(nca_base).set_params(init='pca')\n            else:\n                nca_other = clone(nca_base).set_params(init='identity')\n            nca_other.fit(X, y)\n            assert_array_almost_equal(nca.components_, nca_other.components_)",
            "def test_warm_start_validation():\n    X, y = make_classification(n_samples=30, n_features=5, n_classes=4,\n                               n_redundant=0, n_informative=5, random_state=0)\n\n    nca = NeighborhoodComponentsAnalysis(warm_start=True, max_iter=5)\n    nca.fit(X, y)\n\n    X_less_features, y = make_classification(n_samples=30, n_features=4,\n                                             n_classes=4, n_redundant=0,\n                                             n_informative=4, random_state=0)\n    assert_raise_message(ValueError,\n                         'The new inputs dimensionality ({}) does not '\n                         'match the input dimensionality of the '\n                         'previously learned transformation ({}).'\n                         .format(X_less_features.shape[1],\n                                 nca.components_.shape[1]),\n                         nca.fit, X_less_features, y)",
            "def __init__(self, X, y):\n            self.loss = np.inf  # initialize the loss to very high\n            # Initialize a fake NCA and variables needed to compute the loss:\n            self.fake_nca = NeighborhoodComponentsAnalysis()\n            self.fake_nca.n_iter_ = np.inf\n            self.X, y, _ = self.fake_nca._validate_params(X, y)\n            self.same_class_mask = y[:, np.newaxis] == y[np.newaxis, :]",
            "def __init__(self, X, y):\n            # Initialize a fake NCA and variables needed to call the loss\n            # function:\n            self.fake_nca = NeighborhoodComponentsAnalysis()\n            self.fake_nca.n_iter_ = np.inf\n            self.X, y, _ = self.fake_nca._validate_params(X, y)\n            self.same_class_mask = y[:, np.newaxis] == y[np.newaxis, :]",
            "def test_verbose(init_name, capsys):\n    # assert there is proper output when verbose = 1, for every initialization\n    # except auto because auto will call one of the others\n    rng = np.random.RandomState(42)\n    X, y = make_blobs(n_samples=30, centers=6, n_features=5, random_state=0)\n    regexp_init = r'... done in \\ *\\d+\\.\\d{2}s'\n    msgs = {'pca': \"Finding principal components\" + regexp_init,\n            'lda': \"Finding most discriminative components\" + regexp_init}\n    if init_name == 'precomputed':\n        init = rng.randn(X.shape[1], X.shape[1])\n    else:\n        init = init_name\n    nca = NeighborhoodComponentsAnalysis(verbose=1, init=init)\n    nca.fit(X, y)\n    out, _ = capsys.readouterr()\n\n    # check output\n    lines = re.split('\\n+', out)\n    # if pca or lda init, an additional line is printed, so we test\n    # it and remove it to test the rest equally among initializations\n    if init_name in ['pca', 'lda']:\n        assert re.match(msgs[init_name], lines[0])\n        lines = lines[1:]\n    assert lines[0] == '[NeighborhoodComponentsAnalysis]'\n    header = '{:>10} {:>20} {:>10}'.format('Iteration', 'Objective Value',\n                                           'Time(s)')\n    assert lines[1] == '[NeighborhoodComponentsAnalysis] {}'.format(header)\n    assert lines[2] == ('[NeighborhoodComponentsAnalysis] {}'\n                        .format('-' * len(header)))\n    for line in lines[3:-2]:\n        # The following regex will match for instance:\n        # '[NeighborhoodComponentsAnalysis]  0    6.988936e+01   0.01'\n        assert re.match(r'\\[NeighborhoodComponentsAnalysis\\] *\\d+ *\\d\\.\\d{6}e'\n                        r'[+|-]\\d+\\ *\\d+\\.\\d{2}', line)\n    assert re.match(r'\\[NeighborhoodComponentsAnalysis\\] Training took\\ *'\n                    r'\\d+\\.\\d{2}s\\.', lines[-2])\n    assert lines[-1] == ''",
            "def test_n_components():\n    rng = np.random.RandomState(42)\n    X = np.arange(12).reshape(4, 3)\n    y = [1, 1, 2, 2]\n\n    init = rng.rand(X.shape[1] - 1, 3)\n\n    # n_components = X.shape[1] != transformation.shape[0]\n    n_components = X.shape[1]\n    nca = NeighborhoodComponentsAnalysis(init=init, n_components=n_components)\n    assert_raise_message(ValueError,\n                         'The preferred dimensionality of the '\n                         'projected space `n_components` ({}) does not match '\n                         'the output dimensionality of the given '\n                         'linear transformation `init` ({})!'\n                         .format(n_components, init.shape[0]),\n                         nca.fit, X, y)\n\n    # n_components > X.shape[1]\n    n_components = X.shape[1] + 2\n    nca = NeighborhoodComponentsAnalysis(init=init, n_components=n_components)\n    assert_raise_message(ValueError,\n                         'The preferred dimensionality of the '\n                         'projected space `n_components` ({}) cannot '\n                         'be greater than the given data '\n                         'dimensionality ({})!'\n                         .format(n_components, X.shape[1]),\n                         nca.fit, X, y)\n\n    # n_components < X.shape[1]\n    nca = NeighborhoodComponentsAnalysis(n_components=2, init='identity')\n    nca.fit(X, y)",
            "def test_init_transformation():\n    rng = np.random.RandomState(42)\n    X, y = make_blobs(n_samples=30, centers=6, n_features=5, random_state=0)\n\n    # Start learning from scratch\n    nca = NeighborhoodComponentsAnalysis(init='identity')\n    nca.fit(X, y)\n\n    # Initialize with random\n    nca_random = NeighborhoodComponentsAnalysis(init='random')\n    nca_random.fit(X, y)\n\n    # Initialize with auto\n    nca_auto = NeighborhoodComponentsAnalysis(init='auto')\n    nca_auto.fit(X, y)\n\n    # Initialize with PCA\n    nca_pca = NeighborhoodComponentsAnalysis(init='pca')\n    nca_pca.fit(X, y)\n\n    # Initialize with LDA\n    nca_lda = NeighborhoodComponentsAnalysis(init='lda')\n    nca_lda.fit(X, y)\n\n    init = rng.rand(X.shape[1], X.shape[1])\n    nca = NeighborhoodComponentsAnalysis(init=init)\n    nca.fit(X, y)\n\n    # init.shape[1] must match X.shape[1]\n    init = rng.rand(X.shape[1], X.shape[1] + 1)\n    nca = NeighborhoodComponentsAnalysis(init=init)\n    assert_raise_message(ValueError,\n                         'The input dimensionality ({}) of the given '\n                         'linear transformation `init` must match the '\n                         'dimensionality of the given inputs `X` ({}).'\n                         .format(init.shape[1], X.shape[1]),\n                         nca.fit, X, y)\n\n    # init.shape[0] must be <= init.shape[1]\n    init = rng.rand(X.shape[1] + 1, X.shape[1])\n    nca = NeighborhoodComponentsAnalysis(init=init)\n    assert_raise_message(ValueError,\n                         'The output dimensionality ({}) of the given '\n                         'linear transformation `init` cannot be '\n                         'greater than its input dimensionality ({}).'\n                         .format(init.shape[0], init.shape[1]),\n                         nca.fit, X, y)\n\n    # init.shape[0] must match n_components\n    init = rng.rand(X.shape[1], X.shape[1])\n    n_components = X.shape[1] - 2\n    nca = NeighborhoodComponentsAnalysis(init=init, n_components=n_components)\n    assert_raise_message(ValueError,\n                         'The preferred dimensionality of the '\n                         'projected space `n_components` ({}) does not match '\n                         'the output dimensionality of the given '\n                         'linear transformation `init` ({})!'\n                         .format(n_components, init.shape[0]),\n                         nca.fit, X, y)",
            "def test_one_class():\n    X = iris_data[iris_target == 0]\n    y = iris_target[iris_target == 0]\n\n    nca = NeighborhoodComponentsAnalysis(max_iter=30,\n                                         n_components=X.shape[1],\n                                         init='identity')\n    nca.fit(X, y)\n    assert_array_equal(X, nca.transform(X))"
        ]
    },
    {
        "repo": "scikit-learn/scikit-learn",
        "instance_id": "scikit-learn__scikit-learn-11281",
        "base_commit": "4143356c3c51831300789e4fdf795d83716dbab6",
        "issue_title": "Should mixture models have a clusterer-compatible interface",
        "test_file": "/home/zqc/RepoCodeEdit/data/swe_bench_temp_wy/scikit-learn/sklearn/mixture/tests/test_gmm.py",
        "searched_functions": [
            "def test_GMM_attributes():\n    n_components, n_features = 10, 4\n    covariance_type = 'diag'\n    g = mixture.GMM(n_components, covariance_type, random_state=rng)\n    weights = rng.rand(n_components)\n    weights = weights / weights.sum()\n    means = rng.randint(-20, 20, (n_components, n_features))\n\n    assert_true(g.n_components == n_components)\n    assert_true(g.covariance_type == covariance_type)\n\n    g.weights_ = weights\n    assert_array_almost_equal(g.weights_, weights)\n    g.means_ = means\n    assert_array_almost_equal(g.means_, means)\n\n    covars = (0.1 + 2 * rng.rand(n_components, n_features)) ** 2\n    g.covars_ = covars\n    assert_array_almost_equal(g.covars_, covars)\n    assert_raises(ValueError, g._set_covars, [])\n    assert_raises(ValueError, g._set_covars,\n                  np.zeros((n_components - 2, n_features)))\n    assert_raises(ValueError, mixture.GMM, n_components=20,\n                  covariance_type='badcovariance_type')",
            "def test_train(self, params='wmc'):\n        g = mixture.GMM(n_components=self.n_components,\n                        covariance_type=self.covariance_type)\n        with ignore_warnings(category=DeprecationWarning):\n            g.weights_ = self.weights\n            g.means_ = self.means\n            g.covars_ = 20 * self.covars[self.covariance_type]\n\n        # Create a training set by sampling from the predefined distribution.\n        with ignore_warnings(category=DeprecationWarning):\n            X = g.sample(n_samples=100)\n            g = self.model(n_components=self.n_components,\n                           covariance_type=self.covariance_type,\n                           random_state=rng, min_covar=1e-1,\n                           n_iter=1, init_params=params)\n            g.fit(X)\n\n        # Do one training iteration at a time so we can keep track of\n        # the log likelihood to make sure that it increases after each\n        # iteration.\n        trainll = []\n        with ignore_warnings(category=DeprecationWarning):\n            for _ in range(5):\n                g.params = params\n                g.init_params = ''\n                g.fit(X)\n                trainll.append(self.score(g, X))\n            g.n_iter = 10\n            g.init_params = ''\n            g.params = params\n            g.fit(X)  # finish fitting\n\n        # Note that the log likelihood will sometimes decrease by a\n        # very small amount after it has more or less converged due to\n        # the addition of min_covar to the covariance (to prevent\n        # underflow).  This is why the threshold is set to -0.5\n        # instead of 0.\n        with ignore_warnings(category=DeprecationWarning):\n            delta_min = np.diff(trainll).min()\n        self.assertTrue(\n            delta_min > self.threshold,\n            \"The min nll increase is %f which is lower than the admissible\"\n            \" threshold of %f, for model %s. The likelihoods are %s.\"\n            % (delta_min, self.threshold, self.covariance_type, trainll))",
            "def test_sample_gaussian():\n    # Test sample generation from mixture.sample_gaussian where covariance\n    # is diagonal, spherical and full\n\n    n_features, n_samples = 2, 300\n    axis = 1\n    mu = rng.randint(10) * rng.rand(n_features)\n    cv = (rng.rand(n_features) + 1.0) ** 2\n\n    samples = mixture.gmm._sample_gaussian(\n        mu, cv, covariance_type='diag', n_samples=n_samples)\n\n    assert_true(np.allclose(samples.mean(axis), mu, atol=1.3))\n    assert_true(np.allclose(samples.var(axis), cv, atol=1.5))\n\n    # the same for spherical covariances\n    cv = (rng.rand() + 1.0) ** 2\n    samples = mixture.gmm._sample_gaussian(\n        mu, cv, covariance_type='spherical', n_samples=n_samples)\n\n    assert_true(np.allclose(samples.mean(axis), mu, atol=1.5))\n    assert_true(np.allclose(\n        samples.var(axis), np.repeat(cv, n_features), atol=1.5))\n\n    # and for full covariances\n    A = rng.randn(n_features, n_features)\n    cv = np.dot(A.T, A) + np.eye(n_features)\n    samples = mixture.gmm._sample_gaussian(\n        mu, cv, covariance_type='full', n_samples=n_samples)\n    assert_true(np.allclose(samples.mean(axis), mu, atol=1.3))\n    assert_true(np.allclose(np.cov(samples), cv, atol=2.5))\n\n    # Numerical stability check: in SciPy 0.12.0 at least, eigh may return\n    # tiny negative values in its second return value.\n    x = mixture.gmm._sample_gaussian(\n        [0, 0], [[4, 3], [1, .1]], covariance_type='full', random_state=42)\n    assert_true(np.isfinite(x).all())",
            "def test_1d_1component():\n    # Test all of the covariance_types return the same BIC score for\n    # 1-dimensional, 1 component fits.\n    n_samples, n_dim, n_components = 100, 1, 1\n    X = rng.randn(n_samples, n_dim)\n    g_full = mixture.GMM(n_components=n_components, covariance_type='full',\n                         random_state=rng, min_covar=1e-7, n_iter=1)\n    with ignore_warnings(category=DeprecationWarning):\n        g_full.fit(X)\n        g_full_bic = g_full.bic(X)\n        for cv_type in ['tied', 'diag', 'spherical']:\n            g = mixture.GMM(n_components=n_components, covariance_type=cv_type,\n                            random_state=rng, min_covar=1e-7, n_iter=1)\n            g.fit(X)\n            assert_array_almost_equal(g.bic(X), g_full_bic)",
            "def test_aic():\n    # Test the aic and bic criteria\n    n_samples, n_dim, n_components = 50, 3, 2\n    X = rng.randn(n_samples, n_dim)\n    SGH = 0.5 * (X.var() + np.log(2 * np.pi))  # standard gaussian entropy\n\n    for cv_type in ['full', 'tied', 'diag', 'spherical']:\n        g = mixture.GMM(n_components=n_components, covariance_type=cv_type,\n                        random_state=rng, min_covar=1e-7)\n        g.fit(X)\n        aic = 2 * n_samples * SGH * n_dim + 2 * g._n_parameters()\n        bic = (2 * n_samples * SGH * n_dim +\n               np.log(n_samples) * g._n_parameters())\n        bound = n_dim * 3. / np.sqrt(n_samples)\n        assert_true(np.abs(g.aic(X) - aic) / n_samples < bound)\n        assert_true(np.abs(g.bic(X) - bic) / n_samples < bound)",
            "def test_verbose_first_level():\n    # Create sample data\n    X = rng.randn(30, 5)\n    X[:10] += 2\n    g = mixture.GMM(n_components=2, n_init=2, verbose=1)\n\n    old_stdout = sys.stdout\n    sys.stdout = StringIO()\n    try:\n        g.fit(X)\n    finally:\n        sys.stdout = old_stdout",
            "def test_verbose_second_level():\n    # Create sample data\n    X = rng.randn(30, 5)\n    X[:10] += 2\n    g = mixture.GMM(n_components=2, n_init=2, verbose=2)\n\n    old_stdout = sys.stdout\n    sys.stdout = StringIO()\n    try:\n        g.fit(X)\n    finally:\n        sys.stdout = old_stdout",
            "def test_multiple_init():\n    # Test that multiple inits does not much worse than a single one\n    X = rng.randn(30, 5)\n    X[:10] += 2\n    g = mixture.GMM(n_components=2, covariance_type='spherical',\n                    random_state=rng, min_covar=1e-7, n_iter=5)\n    with ignore_warnings(category=DeprecationWarning):\n        train1 = g.fit(X).score(X).sum()\n        g.n_init = 5\n        train2 = g.fit(X).score(X).sum()\n    assert_true(train2 >= train1 - 1.e-2)",
            "def test_eval(self):\n        if not self.do_test_eval:\n            return  # DPGMM does not support setting the means and\n        # covariances before fitting There is no way of fixing this\n        # due to the variational parameters being more expressive than\n        # covariance matrices\n        g = self.model(n_components=self.n_components,\n                       covariance_type=self.covariance_type, random_state=rng)\n        # Make sure the means are far apart so responsibilities.argmax()\n        # picks the actual component used to generate the observations.\n        g.means_ = 20 * self.means\n        g.covars_ = self.covars[self.covariance_type]\n        g.weights_ = self.weights\n\n        gaussidx = np.repeat(np.arange(self.n_components), 5)\n        n_samples = len(gaussidx)\n        X = rng.randn(n_samples, self.n_features) + g.means_[gaussidx]\n\n        with ignore_warnings(category=DeprecationWarning):\n            ll, responsibilities = g.score_samples(X)\n\n        self.assertEqual(len(ll), n_samples)\n        self.assertEqual(responsibilities.shape,\n                         (n_samples, self.n_components))\n        assert_array_almost_equal(responsibilities.sum(axis=1),\n                                  np.ones(n_samples))\n        assert_array_equal(responsibilities.argmax(axis=1), gaussidx)",
            "def test_lvmpdf_full_cv_non_positive_definite():\n    n_features, n_samples = 2, 10\n    rng = np.random.RandomState(0)\n    X = rng.randint(10) * rng.rand(n_samples, n_features)\n    mu = np.mean(X, 0)\n    cv = np.array([[[-1, 0], [0, 1]]])\n    expected_message = \"'covars' must be symmetric, positive-definite\"\n    assert_raise_message(ValueError, expected_message,\n                         mixture.log_multivariate_normal_density,\n                         X, mu, cv, 'full')"
        ]
    },
    {
        "repo": "scikit-learn/scikit-learn",
        "instance_id": "scikit-learn__scikit-learn-25570",
        "base_commit": "cd25abee0ad0ac95225d4a9be8948eff69f49690",
        "issue_title": "ColumnTransformer with pandas output can't handle transformers with no features",
        "test_file": "/home/zqc/RepoCodeEdit/data/swe_bench_temp_wy/scikit-learn/sklearn/compose/tests/test_column_transformer.py",
        "searched_functions": [
            "def test_2D_transformer_output_pandas():\n    pd = pytest.importorskip(\"pandas\")\n\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    X_df = pd.DataFrame(X_array, columns=[\"col1\", \"col2\"])\n\n    # if one transformer is dropped, test that name is still correct\n    ct = ColumnTransformer([(\"trans1\", TransNo2D(), \"col1\")])\n    msg = \"the 'trans1' transformer should be 2D\"\n    with pytest.raises(ValueError, match=msg):\n        ct.fit_transform(X_df)\n    # because fit is also doing transform, this raises already on fit\n    with pytest.raises(ValueError, match=msg):\n        ct.fit(X_df)",
            "def test_column_transformer_empty_columns(pandas, column_selection, callable_column):\n    # test case that ensures that the column transformer does also work when\n    # a given transformer doesn't have any columns to work on\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    X_res_both = X_array\n\n    if pandas:\n        pd = pytest.importorskip(\"pandas\")\n        X = pd.DataFrame(X_array, columns=[\"first\", \"second\"])\n    else:\n        X = X_array\n\n    if callable_column:\n        column = lambda X: column_selection  # noqa\n    else:\n        column = column_selection\n\n    ct = ColumnTransformer(\n        [(\"trans1\", Trans(), [0, 1]), (\"trans2\", TransRaise(), column)]\n    )\n    assert_array_equal(ct.fit_transform(X), X_res_both)\n    assert_array_equal(ct.fit(X).transform(X), X_res_both)\n    assert len(ct.transformers_) == 2\n    assert isinstance(ct.transformers_[1][1], TransRaise)\n\n    ct = ColumnTransformer(\n        [(\"trans1\", TransRaise(), column), (\"trans2\", Trans(), [0, 1])]\n    )\n    assert_array_equal(ct.fit_transform(X), X_res_both)\n    assert_array_equal(ct.fit(X).transform(X), X_res_both)\n    assert len(ct.transformers_) == 2\n    assert isinstance(ct.transformers_[0][1], TransRaise)\n\n    ct = ColumnTransformer([(\"trans\", TransRaise(), column)], remainder=\"passthrough\")\n    assert_array_equal(ct.fit_transform(X), X_res_both)\n    assert_array_equal(ct.fit(X).transform(X), X_res_both)\n    assert len(ct.transformers_) == 2  # including remainder\n    assert isinstance(ct.transformers_[0][1], TransRaise)\n\n    fixture = np.array([[], [], []])\n    ct = ColumnTransformer([(\"trans\", TransRaise(), column)], remainder=\"drop\")\n    assert_array_equal(ct.fit_transform(X), fixture)\n    assert_array_equal(ct.fit(X).transform(X), fixture)\n    assert len(ct.transformers_) == 2  # including remainder\n    assert isinstance(ct.transformers_[0][1], TransRaise)",
            "def test_feature_names_empty_columns(empty_col):\n    pd = pytest.importorskip(\"pandas\")\n\n    df = pd.DataFrame({\"col1\": [\"a\", \"a\", \"b\"], \"col2\": [\"z\", \"z\", \"z\"]})\n\n    ct = ColumnTransformer(\n        transformers=[\n            (\"ohe\", OneHotEncoder(), [\"col1\", \"col2\"]),\n            (\"empty_features\", OneHotEncoder(), empty_col),\n        ],\n    )\n\n    ct.fit(df)\n    assert_array_equal(\n        ct.get_feature_names_out(), [\"ohe__col1_a\", \"ohe__col1_b\", \"ohe__col2_z\"]\n    )",
            "def test_column_transformer_dataframe():\n    pd = pytest.importorskip(\"pandas\")\n\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    X_df = pd.DataFrame(X_array, columns=[\"first\", \"second\"])\n\n    X_res_first = np.array([0, 1, 2]).reshape(-1, 1)\n    X_res_both = X_array\n\n    cases = [\n        # String keys: label based\n        # scalar\n        (\"first\", X_res_first),\n        # list\n        ([\"first\"], X_res_first),\n        ([\"first\", \"second\"], X_res_both),\n        # slice\n        (slice(\"first\", \"second\"), X_res_both),\n        # int keys: positional\n        # scalar\n        (0, X_res_first),\n        # list\n        ([0], X_res_first),\n        ([0, 1], X_res_both),\n        (np.array([0, 1]), X_res_both),\n        # slice\n        (slice(0, 1), X_res_first),\n        (slice(0, 2), X_res_both),\n        # boolean mask\n        (np.array([True, False]), X_res_first),\n        (pd.Series([True, False], index=[\"first\", \"second\"]), X_res_first),\n        ([True, False], X_res_first),\n    ]\n\n    for selection, res in cases:\n        ct = ColumnTransformer([(\"trans\", Trans(), selection)], remainder=\"drop\")\n        assert_array_equal(ct.fit_transform(X_df), res)\n        assert_array_equal(ct.fit(X_df).transform(X_df), res)\n\n        # callable that returns any of the allowed specifiers\n        ct = ColumnTransformer(\n            [(\"trans\", Trans(), lambda X: selection)], remainder=\"drop\"\n        )\n        assert_array_equal(ct.fit_transform(X_df), res)\n        assert_array_equal(ct.fit(X_df).transform(X_df), res)\n\n    ct = ColumnTransformer(\n        [(\"trans1\", Trans(), [\"first\"]), (\"trans2\", Trans(), [\"second\"])]\n    )\n    assert_array_equal(ct.fit_transform(X_df), X_res_both)\n    assert_array_equal(ct.fit(X_df).transform(X_df), X_res_both)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] != \"remainder\"\n\n    ct = ColumnTransformer([(\"trans1\", Trans(), [0]), (\"trans2\", Trans(), [1])])\n    assert_array_equal(ct.fit_transform(X_df), X_res_both)\n    assert_array_equal(ct.fit(X_df).transform(X_df), X_res_both)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] != \"remainder\"\n\n    # test with transformer_weights\n    transformer_weights = {\"trans1\": 0.1, \"trans2\": 10}\n    both = ColumnTransformer(\n        [(\"trans1\", Trans(), [\"first\"]), (\"trans2\", Trans(), [\"second\"])],\n        transformer_weights=transformer_weights,\n    )\n    res = np.vstack(\n        [\n            transformer_weights[\"trans1\"] * X_df[\"first\"],\n            transformer_weights[\"trans2\"] * X_df[\"second\"],\n        ]\n    ).T\n    assert_array_equal(both.fit_transform(X_df), res)\n    assert_array_equal(both.fit(X_df).transform(X_df), res)\n    assert len(both.transformers_) == 2\n    assert both.transformers_[-1][0] != \"remainder\"\n\n    # test multiple columns\n    both = ColumnTransformer(\n        [(\"trans\", Trans(), [\"first\", \"second\"])], transformer_weights={\"trans\": 0.1}\n    )\n    assert_array_equal(both.fit_transform(X_df), 0.1 * X_res_both)\n    assert_array_equal(both.fit(X_df).transform(X_df), 0.1 * X_res_both)\n    assert len(both.transformers_) == 1\n    assert both.transformers_[-1][0] != \"remainder\"\n\n    both = ColumnTransformer(\n        [(\"trans\", Trans(), [0, 1])], transformer_weights={\"trans\": 0.1}\n    )\n    assert_array_equal(both.fit_transform(X_df), 0.1 * X_res_both)\n    assert_array_equal(both.fit(X_df).transform(X_df), 0.1 * X_res_both)\n    assert len(both.transformers_) == 1\n    assert both.transformers_[-1][0] != \"remainder\"\n\n    # ensure pandas object is passed through\n\n    class TransAssert(BaseEstimator):\n        def fit(self, X, y=None):\n            return self\n\n        def transform(self, X, y=None):\n            assert isinstance(X, (pd.DataFrame, pd.Series))\n            if isinstance(X, pd.Series):\n                X = X.to_frame()\n            return X\n\n    ct = ColumnTransformer([(\"trans\", TransAssert(), \"first\")], remainder=\"drop\")\n    ct.fit_transform(X_df)\n    ct = ColumnTransformer([(\"trans\", TransAssert(), [\"first\", \"second\"])])\n    ct.fit_transform(X_df)\n\n    # integer column spec + integer column names -> still use positional\n    X_df2 = X_df.copy()\n    X_df2.columns = [1, 0]\n    ct = ColumnTransformer([(\"trans\", Trans(), 0)], remainder=\"drop\")\n    assert_array_equal(ct.fit_transform(X_df2), X_res_first)\n    assert_array_equal(ct.fit(X_df2).transform(X_df2), X_res_first)\n\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] == \"remainder\"\n    assert ct.transformers_[-1][1] == \"drop\"\n    assert_array_equal(ct.transformers_[-1][2], [1])",
            "def test_feature_names_in_():\n    \"\"\"Feature names are stored in column transformer.\n\n    Column transformer deliberately does not check for column name consistency.\n    It only checks that the non-dropped names seen in `fit` are seen\n    in `transform`. This behavior is already tested in\n    `test_feature_name_validation_missing_columns_drop_passthough`\"\"\"\n\n    pd = pytest.importorskip(\"pandas\")\n\n    feature_names = [\"a\", \"c\", \"d\"]\n    df = pd.DataFrame([[1, 2, 3]], columns=feature_names)\n    ct = ColumnTransformer([(\"bycol\", Trans(), [\"a\", \"d\"])], remainder=\"passthrough\")\n\n    ct.fit(df)\n    assert_array_equal(ct.feature_names_in_, feature_names)\n    assert isinstance(ct.feature_names_in_, np.ndarray)\n    assert ct.feature_names_in_.dtype == object",
            "def test_column_transformer_output_indices_df():\n    # Checks for the output_indices_ attribute with data frames\n    pd = pytest.importorskip(\"pandas\")\n\n    X_df = pd.DataFrame(np.arange(6).reshape(3, 2), columns=[\"first\", \"second\"])\n\n    ct = ColumnTransformer(\n        [(\"trans1\", Trans(), [\"first\"]), (\"trans2\", Trans(), [\"second\"])]\n    )\n    X_trans = ct.fit_transform(X_df)\n    assert ct.output_indices_ == {\n        \"trans1\": slice(0, 1),\n        \"trans2\": slice(1, 2),\n        \"remainder\": slice(0, 0),\n    }\n    assert_array_equal(X_trans[:, [0]], X_trans[:, ct.output_indices_[\"trans1\"]])\n    assert_array_equal(X_trans[:, [1]], X_trans[:, ct.output_indices_[\"trans2\"]])\n    assert_array_equal(X_trans[:, []], X_trans[:, ct.output_indices_[\"remainder\"]])\n\n    ct = ColumnTransformer([(\"trans1\", Trans(), [0]), (\"trans2\", Trans(), [1])])\n    X_trans = ct.fit_transform(X_df)\n    assert ct.output_indices_ == {\n        \"trans1\": slice(0, 1),\n        \"trans2\": slice(1, 2),\n        \"remainder\": slice(0, 0),\n    }\n    assert_array_equal(X_trans[:, [0]], X_trans[:, ct.output_indices_[\"trans1\"]])\n    assert_array_equal(X_trans[:, [1]], X_trans[:, ct.output_indices_[\"trans2\"]])\n    assert_array_equal(X_trans[:, []], X_trans[:, ct.output_indices_[\"remainder\"]])",
            "def test_transformers_with_pandas_out_but_not_feature_names_out(\n    trans_1, expected_verbose_names, expected_non_verbose_names\n):\n    \"\"\"Check that set_config(transform=\"pandas\") is compatible with more transformers.\n\n    Specifically, if transformers returns a DataFrame, but does not define\n    `get_feature_names_out`.\n    \"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    X_df = pd.DataFrame({\"feat0\": [1.0, 2.0, 3.0], \"feat1\": [2.0, 3.0, 4.0]})\n    ct = ColumnTransformer(\n        [\n            (\"trans_0\", PandasOutTransformer(offset=3.0), [\"feat1\"]),\n            (\"trans_1\", trans_1, [\"feat0\"]),\n        ]\n    )\n    X_trans_np = ct.fit_transform(X_df)\n    assert isinstance(X_trans_np, np.ndarray)\n\n    # `ct` does not have `get_feature_names_out` because `PandasOutTransformer` does\n    # not define the method.\n    with pytest.raises(AttributeError, match=\"not provide get_feature_names_out\"):\n        ct.get_feature_names_out()\n\n    # The feature names are prefixed because verbose_feature_names_out=True is default\n    ct.set_output(transform=\"pandas\")\n    X_trans_df0 = ct.fit_transform(X_df)\n    assert_array_equal(X_trans_df0.columns, expected_verbose_names)\n\n    ct.set_params(verbose_feature_names_out=False)\n    X_trans_df1 = ct.fit_transform(X_df)\n    assert_array_equal(X_trans_df1.columns, expected_non_verbose_names)",
            "def test_column_transformer_invalid_columns(remainder):\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    # general invalid\n    for col in [1.5, [\"string\", 1], slice(1, \"s\"), np.array([1.0])]:\n        ct = ColumnTransformer([(\"trans\", Trans(), col)], remainder=remainder)\n        with pytest.raises(ValueError, match=\"No valid specification\"):\n            ct.fit(X_array)\n\n    # invalid for arrays\n    for col in [\"string\", [\"string\", \"other\"], slice(\"a\", \"b\")]:\n        ct = ColumnTransformer([(\"trans\", Trans(), col)], remainder=remainder)\n        with pytest.raises(ValueError, match=\"Specifying the columns\"):\n            ct.fit(X_array)\n\n    # transformed n_features does not match fitted n_features\n    col = [0, 1]\n    ct = ColumnTransformer([(\"trans\", Trans(), col)], remainder=remainder)\n    ct.fit(X_array)\n    X_array_more = np.array([[0, 1, 2], [2, 4, 6], [3, 6, 9]]).T\n    msg = \"X has 3 features, but ColumnTransformer is expecting 2 features as input.\"\n    with pytest.raises(ValueError, match=msg):\n        ct.transform(X_array_more)\n    X_array_fewer = np.array(\n        [\n            [0, 1, 2],\n        ]\n    ).T\n    err_msg = (\n        \"X has 1 features, but ColumnTransformer is expecting 2 features as input.\"\n    )\n    with pytest.raises(ValueError, match=err_msg):\n        ct.transform(X_array_fewer)",
            "def test_make_column_transformer_kwargs():\n    scaler = StandardScaler()\n    norm = Normalizer()\n    ct = make_column_transformer(\n        (scaler, \"first\"),\n        (norm, [\"second\"]),\n        n_jobs=3,\n        remainder=\"drop\",\n        sparse_threshold=0.5,\n    )\n    assert (\n        ct.transformers\n        == make_column_transformer((scaler, \"first\"), (norm, [\"second\"])).transformers\n    )\n    assert ct.n_jobs == 3\n    assert ct.remainder == \"drop\"\n    assert ct.sparse_threshold == 0.5\n    # invalid keyword parameters should raise an error message\n    msg = re.escape(\n        \"make_column_transformer() got an unexpected \"\n        \"keyword argument 'transformer_weights'\"\n    )\n    with pytest.raises(TypeError, match=msg):\n        make_column_transformer(\n            (scaler, \"first\"),\n            (norm, [\"second\"]),\n            transformer_weights={\"pca\": 10, \"Transf\": 1},\n        )",
            "def test_column_transformer_set_output(verbose_feature_names_out, remainder):\n    \"\"\"Check column transformer behavior with set_output.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    df = pd.DataFrame([[1, 2, 3, 4]], columns=[\"a\", \"b\", \"c\", \"d\"], index=[10])\n    ct = ColumnTransformer(\n        [(\"first\", TransWithNames(), [\"a\", \"c\"]), (\"second\", TransWithNames(), [\"d\"])],\n        remainder=remainder,\n        verbose_feature_names_out=verbose_feature_names_out,\n    )\n    X_trans = ct.fit_transform(df)\n    assert isinstance(X_trans, np.ndarray)\n\n    ct.set_output(transform=\"pandas\")\n\n    df_test = pd.DataFrame([[1, 2, 3, 4]], columns=df.columns, index=[20])\n    X_trans = ct.transform(df_test)\n    assert isinstance(X_trans, pd.DataFrame)\n\n    feature_names_out = ct.get_feature_names_out()\n    assert_array_equal(X_trans.columns, feature_names_out)\n    assert_array_equal(X_trans.index, df_test.index)"
        ]
    },
    {
        "repo": "scikit-learn/scikit-learn",
        "instance_id": "scikit-learn__scikit-learn-14983",
        "base_commit": "06632c0d185128a53c57ccc73b25b6408e90bb89",
        "issue_title": "RepeatedKFold and RepeatedStratifiedKFold do not show correct __repr__ string",
        "test_file": "/home/zqc/RepoCodeEdit/data/swe_bench_temp_wy/scikit-learn/sklearn/model_selection/tests/test_validation.py",
        "searched_functions": [
            "def test_learning_curve():\n    n_samples = 30\n    n_splits = 3\n    X, y = make_classification(n_samples=n_samples, n_features=1,\n                               n_informative=1, n_redundant=0, n_classes=2,\n                               n_clusters_per_class=1, random_state=0)\n    estimator = MockImprovingEstimator(n_samples * ((n_splits - 1) / n_splits))\n    for shuffle_train in [False, True]:\n        with warnings.catch_warnings(record=True) as w:\n            train_sizes, train_scores, test_scores, fit_times, score_times = \\\n                learning_curve(estimator, X, y, cv=KFold(n_splits=n_splits),\n                               train_sizes=np.linspace(0.1, 1.0, 10),\n                               shuffle=shuffle_train, return_times=True)\n        if len(w) > 0:\n            raise RuntimeError(\"Unexpected warning: %r\" % w[0].message)\n        assert train_scores.shape == (10, 3)\n        assert test_scores.shape == (10, 3)\n        assert fit_times.shape == (10, 3)\n        assert score_times.shape == (10, 3)\n        assert_array_equal(train_sizes, np.linspace(2, 20, 10))\n        assert_array_almost_equal(train_scores.mean(axis=1),\n                                  np.linspace(1.9, 1.0, 10))\n        assert_array_almost_equal(test_scores.mean(axis=1),\n                                  np.linspace(0.1, 1.0, 10))\n\n        # Cannot use assert_array_almost_equal for fit and score times because\n        # the values are hardware-dependant\n        assert fit_times.dtype == \"float64\"\n        assert score_times.dtype == \"float64\"\n\n        # Test a custom cv splitter that can iterate only once\n        with warnings.catch_warnings(record=True) as w:\n            train_sizes2, train_scores2, test_scores2 = learning_curve(\n                estimator, X, y,\n                cv=OneTimeSplitter(n_splits=n_splits, n_samples=n_samples),\n                train_sizes=np.linspace(0.1, 1.0, 10),\n                shuffle=shuffle_train)\n        if len(w) > 0:\n            raise RuntimeError(\"Unexpected warning: %r\" % w[0].message)\n        assert_array_almost_equal(train_scores2, train_scores)\n        assert_array_almost_equal(test_scores2, test_scores)",
            "def fit(self, X_subset, y_subset):\n        assert not hasattr(self, 'fit_called_'), \\\n                   'fit is called the second time'\n        self.fit_called_ = True\n        return super().fit(X_subset, y_subset)",
            "def test_cross_val_predict():\n    X, y = load_boston(return_X_y=True)\n    cv = KFold()\n\n    est = Ridge()\n\n    # Naive loop (should be same as cross_val_predict):\n    preds2 = np.zeros_like(y)\n    for train, test in cv.split(X, y):\n        est.fit(X[train], y[train])\n        preds2[test] = est.predict(X[test])\n\n    preds = cross_val_predict(est, X, y, cv=cv)\n    assert_array_almost_equal(preds, preds2)\n\n    preds = cross_val_predict(est, X, y)\n    assert len(preds) == len(y)\n\n    cv = LeaveOneOut()\n    preds = cross_val_predict(est, X, y, cv=cv)\n    assert len(preds) == len(y)\n\n    Xsp = X.copy()\n    Xsp *= (Xsp > np.median(Xsp))\n    Xsp = coo_matrix(Xsp)\n    preds = cross_val_predict(est, Xsp, y)\n    assert_array_almost_equal(len(preds), len(y))\n\n    preds = cross_val_predict(KMeans(), X)\n    assert len(preds) == len(y)\n\n    class BadCV():\n        def split(self, X, y=None, groups=None):\n            for i in range(4):\n                yield np.array([0, 1, 2, 3]), np.array([4, 5, 6, 7, 8])\n\n    assert_raises(ValueError, cross_val_predict, est, X, y, cv=BadCV())\n\n    X, y = load_iris(return_X_y=True)\n\n    warning_message = ('Number of classes in training fold (2) does '\n                       'not match total number of classes (3). '\n                       'Results may not be appropriate for your use case.')\n    assert_warns_message(RuntimeWarning, warning_message,\n                         cross_val_predict,\n                         LogisticRegression(solver=\"liblinear\"),\n                         X, y, method='predict_proba', cv=KFold(2))",
            "def test_learning_curve_with_shuffle():\n    # Following test case was designed this way to verify the code\n    # changes made in pull request: #7506.\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [11, 12], [13, 14], [15, 16],\n                 [17, 18], [19, 20], [7, 8], [9, 10], [11, 12], [13, 14],\n                 [15, 16], [17, 18]])\n    y = np.array([1, 1, 1, 2, 3, 4, 1, 1, 2, 3, 4, 1, 2, 3, 4])\n    groups = np.array([1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 4, 4, 4, 4])\n    # Splits on these groups fail without shuffle as the first iteration\n    # of the learning curve doesn't contain label 4 in the training set.\n    estimator = PassiveAggressiveClassifier(max_iter=5, tol=None,\n                                            shuffle=False)\n\n    cv = GroupKFold(n_splits=2)\n    train_sizes_batch, train_scores_batch, test_scores_batch = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=1, train_sizes=np.linspace(0.3, 1.0, 3),\n        groups=groups, shuffle=True, random_state=2)\n    assert_array_almost_equal(train_scores_batch.mean(axis=1),\n                              np.array([0.75, 0.3, 0.36111111]))\n    assert_array_almost_equal(test_scores_batch.mean(axis=1),\n                              np.array([0.36111111, 0.25, 0.25]))\n    assert_raises(ValueError, learning_curve, estimator, X, y, cv=cv, n_jobs=1,\n                  train_sizes=np.linspace(0.3, 1.0, 3), groups=groups,\n                  error_score='raise')\n\n    train_sizes_inc, train_scores_inc, test_scores_inc = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=1, train_sizes=np.linspace(0.3, 1.0, 3),\n        groups=groups, shuffle=True, random_state=2,\n        exploit_incremental_learning=True)\n    assert_array_almost_equal(train_scores_inc.mean(axis=1),\n                              train_scores_batch.mean(axis=1))\n    assert_array_almost_equal(test_scores_inc.mean(axis=1),\n                              test_scores_batch.mean(axis=1))",
            "def test_cross_val_predict_class_subset():\n\n    X = np.arange(200).reshape(100, 2)\n    y = np.array([x // 10 for x in range(100)])\n    classes = 10\n\n    kfold3 = KFold(n_splits=3)\n    kfold4 = KFold(n_splits=4)\n\n    le = LabelEncoder()\n\n    methods = ['decision_function', 'predict_proba', 'predict_log_proba']\n    for method in methods:\n        est = LogisticRegression(solver=\"liblinear\")\n\n        # Test with n_splits=3\n        predictions = cross_val_predict(est, X, y, method=method,\n                                        cv=kfold3)\n\n        # Runs a naive loop (should be same as cross_val_predict):\n        expected_predictions = get_expected_predictions(X, y, kfold3, classes,\n                                                        est, method)\n        assert_array_almost_equal(expected_predictions, predictions)\n\n        # Test with n_splits=4\n        predictions = cross_val_predict(est, X, y, method=method,\n                                        cv=kfold4)\n        expected_predictions = get_expected_predictions(X, y, kfold4, classes,\n                                                        est, method)\n        assert_array_almost_equal(expected_predictions, predictions)\n\n        # Testing unordered labels\n        y = shuffle(np.repeat(range(10), 10), random_state=0)\n        predictions = cross_val_predict(est, X, y, method=method,\n                                        cv=kfold3)\n        y = le.fit_transform(y)\n        expected_predictions = get_expected_predictions(X, y, kfold3, classes,\n                                                        est, method)\n        assert_array_almost_equal(expected_predictions, predictions)",
            "def test_cross_val_predict_decision_function_shape():\n    X, y = make_classification(n_classes=2, n_samples=50, random_state=0)\n\n    preds = cross_val_predict(LogisticRegression(solver=\"liblinear\"), X, y,\n                              method='decision_function')\n    assert preds.shape == (50,)\n\n    X, y = load_iris(return_X_y=True)\n\n    preds = cross_val_predict(LogisticRegression(solver=\"liblinear\"), X, y,\n                              method='decision_function')\n    assert preds.shape == (150, 3)\n\n    # This specifically tests imbalanced splits for binary\n    # classification with decision_function. This is only\n    # applicable to classifiers that can be fit on a single\n    # class.\n    X = X[:100]\n    y = y[:100]\n    assert_raise_message(ValueError,\n                         'Only 1 class/es in training fold,'\n                         ' but 2 in overall dataset. This'\n                         ' is not supported for decision_function'\n                         ' with imbalanced folds. To fix '\n                         'this, use a cross-validation technique '\n                         'resulting in properly stratified folds',\n                         cross_val_predict, RidgeClassifier(), X, y,\n                         method='decision_function', cv=KFold(2))\n\n    X, y = load_digits(return_X_y=True)\n    est = SVC(kernel='linear', decision_function_shape='ovo')\n\n    preds = cross_val_predict(est,\n                              X, y,\n                              method='decision_function')\n    assert preds.shape == (1797, 45)\n\n    ind = np.argsort(y)\n    X, y = X[ind], y[ind]\n    assert_raises_regex(ValueError,\n                        r'Output shape \\(599L?, 21L?\\) of decision_function '\n                        r'does not match number of classes \\(7\\) in fold. '\n                        'Irregular decision_function .*',\n                        cross_val_predict, est, X, y,\n                        cv=KFold(n_splits=3), method='decision_function')",
            "def test_fit_and_score_failing():\n    # Create a failing classifier to deliberately fail\n    failing_clf = FailingClassifier(FailingClassifier.FAILING_PARAMETER)\n    # dummy X data\n    X = np.arange(1, 10)\n    y = np.ones(9)\n    fit_and_score_args = [failing_clf, X, None, dict(), None, None, 0,\n                          None, None]\n    # passing error score to trigger the warning message\n    fit_and_score_kwargs = {'error_score': 0}\n    # check if the warning message type is as expected\n    assert_warns(FitFailedWarning, _fit_and_score, *fit_and_score_args,\n                 **fit_and_score_kwargs)\n    # since we're using FailingClassfier, our error will be the following\n    error_message = \"ValueError: Failing classifier failed as required\"\n    # the warning message we're expecting to see\n    warning_message = (\"Estimator fit failed. The score on this train-test \"\n                       \"partition for these parameters will be set to %f. \"\n                       \"Details: \\n%s\" % (fit_and_score_kwargs['error_score'],\n                                          error_message))\n    # check if the same warning is triggered\n    assert_warns_message(FitFailedWarning, warning_message, _fit_and_score,\n                         *fit_and_score_args, **fit_and_score_kwargs)\n\n    fit_and_score_kwargs = {'error_score': 'raise'}\n    # check if exception was raised, with default error_score='raise'\n    assert_raise_message(ValueError, \"Failing classifier failed as required\",\n                         _fit_and_score, *fit_and_score_args,\n                         **fit_and_score_kwargs)\n\n    # check that functions upstream pass error_score param to _fit_and_score\n    error_message = (\"error_score must be the string 'raise' or a\"\n                     \" numeric value. (Hint: if using 'raise', please\"\n                     \" make sure that it has been spelled correctly.)\")\n\n    assert_raise_message(ValueError, error_message, cross_validate,\n                         failing_clf, X, cv=3, error_score='unvalid-string')\n\n    assert_raise_message(ValueError, error_message, cross_val_score,\n                         failing_clf, X, cv=3, error_score='unvalid-string')\n\n    assert_raise_message(ValueError, error_message, learning_curve,\n                         failing_clf, X, y, cv=3, error_score='unvalid-string')\n\n    assert_raise_message(ValueError, error_message, validation_curve,\n                         failing_clf, X, y, 'parameter',\n                         [FailingClassifier.FAILING_PARAMETER], cv=3,\n                         error_score='unvalid-string')\n\n    assert failing_clf.score() == 0.",
            "def test_cross_validate():\n    # Compute train and test mse/r2 scores\n    cv = KFold()\n\n    # Regression\n    X_reg, y_reg = make_regression(n_samples=30, random_state=0)\n    reg = Ridge(random_state=0)\n\n    # Classification\n    X_clf, y_clf = make_classification(n_samples=30, random_state=0)\n    clf = SVC(kernel=\"linear\", random_state=0)\n\n    for X, y, est in ((X_reg, y_reg, reg), (X_clf, y_clf, clf)):\n        # It's okay to evaluate regression metrics on classification too\n        mse_scorer = check_scoring(est, 'neg_mean_squared_error')\n        r2_scorer = check_scoring(est, 'r2')\n        train_mse_scores = []\n        test_mse_scores = []\n        train_r2_scores = []\n        test_r2_scores = []\n        fitted_estimators = []\n        for train, test in cv.split(X, y):\n            est = clone(reg).fit(X[train], y[train])\n            train_mse_scores.append(mse_scorer(est, X[train], y[train]))\n            train_r2_scores.append(r2_scorer(est, X[train], y[train]))\n            test_mse_scores.append(mse_scorer(est, X[test], y[test]))\n            test_r2_scores.append(r2_scorer(est, X[test], y[test]))\n            fitted_estimators.append(est)\n\n        train_mse_scores = np.array(train_mse_scores)\n        test_mse_scores = np.array(test_mse_scores)\n        train_r2_scores = np.array(train_r2_scores)\n        test_r2_scores = np.array(test_r2_scores)\n        fitted_estimators = np.array(fitted_estimators)\n\n        scores = (train_mse_scores, test_mse_scores, train_r2_scores,\n                  test_r2_scores, fitted_estimators)\n\n        check_cross_validate_single_metric(est, X, y, scores)\n        check_cross_validate_multi_metric(est, X, y, scores)",
            "def test_cross_val_predict_unbalanced():\n    X, y = make_classification(n_samples=100, n_features=2, n_redundant=0,\n                               n_informative=2, n_clusters_per_class=1,\n                               random_state=1)\n    # Change the first sample to a new class\n    y[0] = 2\n    clf = LogisticRegression(random_state=1, solver=\"liblinear\")\n    cv = StratifiedKFold(n_splits=2)\n    train, test = list(cv.split(X, y))\n    yhat_proba = cross_val_predict(clf, X, y, cv=cv, method=\"predict_proba\")\n    assert y[test[0]][0] == 2  # sanity check for further assertions\n    assert np.all(yhat_proba[test[0]][:, 2] == 0)\n    assert np.all(yhat_proba[test[0]][:, 0:1] > 0)\n    assert np.all(yhat_proba[test[1]] > 0)\n    assert_array_almost_equal(yhat_proba.sum(axis=1), np.ones(y.shape),\n                              decimal=12)",
            "def test_learning_curve_batch_and_incremental_learning_are_equal():\n    X, y = make_classification(n_samples=30, n_features=1, n_informative=1,\n                               n_redundant=0, n_classes=2,\n                               n_clusters_per_class=1, random_state=0)\n    train_sizes = np.linspace(0.2, 1.0, 5)\n    estimator = PassiveAggressiveClassifier(max_iter=1, tol=None,\n                                            shuffle=False)\n\n    train_sizes_inc, train_scores_inc, test_scores_inc = \\\n        learning_curve(\n            estimator, X, y, train_sizes=train_sizes,\n            cv=3, exploit_incremental_learning=True)\n    train_sizes_batch, train_scores_batch, test_scores_batch = \\\n        learning_curve(\n            estimator, X, y, cv=3, train_sizes=train_sizes,\n            exploit_incremental_learning=False)\n\n    assert_array_equal(train_sizes_inc, train_sizes_batch)\n    assert_array_almost_equal(train_scores_inc.mean(axis=1),\n                              train_scores_batch.mean(axis=1))\n    assert_array_almost_equal(test_scores_inc.mean(axis=1),\n                              test_scores_batch.mean(axis=1))"
        ]
    },
    {
        "repo": "scikit-learn/scikit-learn",
        "instance_id": "scikit-learn__scikit-learn-15535",
        "base_commit": "70b0ddea992c01df1a41588fa9e2d130fb6b13f8",
        "issue_title": "regression in input validation of clustering metrics",
        "test_file": "/home/zqc/RepoCodeEdit/data/swe_bench_temp_wy/scikit-learn/sklearn/metrics/cluster/tests/test_common.py",
        "searched_functions": [
            "def test_permute_labels(metric_name):\n    # All clustering metrics do not change score due to permutations of labels\n    # that is when 0 and 1 exchanged.\n    y_label = np.array([0, 0, 0, 1, 1, 0, 1])\n    y_pred = np.array([1, 0, 1, 0, 1, 1, 0])\n    if metric_name in SUPERVISED_METRICS:\n        metric = SUPERVISED_METRICS[metric_name]\n        score_1 = metric(y_pred, y_label)\n        assert_allclose(score_1, metric(1 - y_pred, y_label))\n        assert_allclose(score_1, metric(1 - y_pred, 1 - y_label))\n        assert_allclose(score_1, metric(y_pred, 1 - y_label))\n    else:\n        metric = UNSUPERVISED_METRICS[metric_name]\n        X = np.random.randint(10, size=(7, 10))\n        score_1 = metric(X, y_pred)\n        assert_allclose(score_1, metric(X, 1 - y_pred))",
            "def test_normalized_output(metric_name):\n    upper_bound_1 = [0, 0, 0, 1, 1, 1]\n    upper_bound_2 = [0, 0, 0, 1, 1, 1]\n    metric = SUPERVISED_METRICS[metric_name]\n    assert metric([0, 0, 0, 1, 1], [0, 0, 0, 1, 2]) > 0.0\n    assert metric([0, 0, 1, 1, 2], [0, 0, 1, 1, 1]) > 0.0\n    assert metric([0, 0, 0, 1, 2], [0, 1, 1, 1, 1]) < 1.0\n    assert metric([0, 0, 0, 1, 2], [0, 1, 1, 1, 1]) < 1.0\n    assert metric(upper_bound_1, upper_bound_2) == pytest.approx(1.0)\n\n    lower_bound_1 = [0, 0, 0, 0, 0, 0]\n    lower_bound_2 = [0, 1, 2, 3, 4, 5]\n    score = np.array([metric(lower_bound_1, lower_bound_2),\n                      metric(lower_bound_2, lower_bound_1)])\n    assert not (score < 0).any()",
            "def test_inf_nan_input(metric_name, metric_func):\n    if metric_name in SUPERVISED_METRICS:\n        invalids = [([0, 1], [np.inf, np.inf]),\n                    ([0, 1], [np.nan, np.nan]),\n                    ([0, 1], [np.nan, np.inf])]\n    else:\n        X = np.random.randint(10, size=(2, 10))\n        invalids = [(X, [np.inf, np.inf]),\n                    (X, [np.nan, np.nan]),\n                    (X, [np.nan, np.inf])]\n    with pytest.raises(ValueError, match='contains NaN, infinity'):\n        for args in invalids:\n            metric_func(*args)",
            "def test_format_invariance(metric_name):\n    y_true = [0, 0, 0, 0, 1, 1, 1, 1]\n    y_pred = [0, 1, 2, 3, 4, 5, 6, 7]\n\n    def generate_formats(y):\n        y = np.array(y)\n        yield y, 'array of ints'\n        yield y.tolist(), 'list of ints'\n        yield [str(x) for x in y.tolist()], 'list of strs'\n        yield y - 1, 'including negative ints'\n        yield y + 1, 'strictly positive ints'\n\n    if metric_name in SUPERVISED_METRICS:\n        metric = SUPERVISED_METRICS[metric_name]\n        score_1 = metric(y_true, y_pred)\n        y_true_gen = generate_formats(y_true)\n        y_pred_gen = generate_formats(y_pred)\n        for (y_true_fmt, fmt_name), (y_pred_fmt, _) in zip(y_true_gen,\n                                                           y_pred_gen):\n            assert score_1 == metric(y_true_fmt, y_pred_fmt)\n    else:\n        metric = UNSUPERVISED_METRICS[metric_name]\n        X = np.random.randint(10, size=(8, 10))\n        score_1 = metric(X, y_true)\n        assert score_1 == metric(X.astype(float), y_true)\n        y_true_gen = generate_formats(y_true)\n        for (y_true_fmt, fmt_name) in y_true_gen:\n            assert score_1 == metric(X, y_true_fmt)",
            "def test_non_symmetry(metric_name, y1, y2):\n    metric = SUPERVISED_METRICS[metric_name]\n    assert metric(y1, y2) != pytest.approx(metric(y2, y1))",
            "def test_single_sample(metric):\n    # only the supervised metrics support single sample\n    for i, j in [(0, 0), (0, 1), (1, 0), (1, 1)]:\n        metric([i], [j])",
            "def test_symmetric_non_symmetric_union():\n    assert (sorted(SYMMETRIC_METRICS + NON_SYMMETRIC_METRICS) ==\n            sorted(SUPERVISED_METRICS))",
            "def test_symmetry(metric_name, y1, y2):\n    metric = SUPERVISED_METRICS[metric_name]\n    assert metric(y1, y2) == pytest.approx(metric(y2, y1))",
            "def generate_formats(y):\n        y = np.array(y)\n        yield y, 'array of ints'\n        yield y.tolist(), 'list of ints'\n        yield [str(x) for x in y.tolist()], 'list of strs'\n        yield y - 1, 'including negative ints'\n        yield y + 1, 'strictly positive ints'"
        ]
    },
    {
        "repo": "scikit-learn/scikit-learn",
        "instance_id": "scikit-learn__scikit-learn-25747",
        "base_commit": "2c867b8f822eb7a684f0d5c4359e4426e1c9cfe0",
        "issue_title": "FeatureUnion not working when aggregating data and pandas transform output selected",
        "test_file": "/home/zqc/RepoCodeEdit/data/swe_bench_temp_wy/scikit-learn/sklearn/cluster/tests/test_feature_agglomeration.py",
        "searched_functions": [
            "def test_feature_agglomeration():\n    n_clusters = 1\n    X = np.array([0, 0, 1]).reshape(1, 3)  # (n_samples, n_features)\n\n    agglo_mean = FeatureAgglomeration(n_clusters=n_clusters, pooling_func=np.mean)\n    agglo_median = FeatureAgglomeration(n_clusters=n_clusters, pooling_func=np.median)\n    agglo_mean.fit(X)\n    agglo_median.fit(X)\n\n    assert np.size(np.unique(agglo_mean.labels_)) == n_clusters\n    assert np.size(np.unique(agglo_median.labels_)) == n_clusters\n    assert np.size(agglo_mean.labels_) == X.shape[1]\n    assert np.size(agglo_median.labels_) == X.shape[1]\n\n    # Test transform\n    Xt_mean = agglo_mean.transform(X)\n    Xt_median = agglo_median.transform(X)\n    assert Xt_mean.shape[1] == n_clusters\n    assert Xt_median.shape[1] == n_clusters\n    assert Xt_mean == np.array([1 / 3.0])\n    assert Xt_median == np.array([0.0])\n\n    # Test inverse transform\n    X_full_mean = agglo_mean.inverse_transform(Xt_mean)\n    X_full_median = agglo_median.inverse_transform(Xt_median)\n    assert np.unique(X_full_mean[0]).size == n_clusters\n    assert np.unique(X_full_median[0]).size == n_clusters\n\n    assert_array_almost_equal(agglo_mean.transform(X_full_mean), Xt_mean)\n    assert_array_almost_equal(agglo_median.transform(X_full_median), Xt_median)",
            "def test_feature_agglomeration_feature_names_out():\n    \"\"\"Check `get_feature_names_out` for `FeatureAgglomeration`.\"\"\"\n    X, _ = make_blobs(n_features=6, random_state=0)\n    agglo = FeatureAgglomeration(n_clusters=3)\n    agglo.fit(X)\n    n_clusters = agglo.n_clusters_\n\n    names_out = agglo.get_feature_names_out()\n    assert_array_equal(\n        [f\"featureagglomeration{i}\" for i in range(n_clusters)], names_out\n    )"
        ]
    },
    {
        "repo": "scikit-learn/scikit-learn",
        "instance_id": "scikit-learn__scikit-learn-12471",
        "base_commit": "02dc9ed680e7f53f1b0d410dcdd37341c7958eb1",
        "issue_title": "OneHotEncoder ignore unknown error when categories are strings ",
        "test_file": "/home/zqc/RepoCodeEdit/data/swe_bench_temp_wy/scikit-learn/sklearn/preprocessing/tests/test_encoders.py",
        "searched_functions": [
            "def test_one_hot_encoder_raise_missing(X, handle_unknown):\n    ohe = OneHotEncoder(categories='auto', handle_unknown=handle_unknown)\n\n    with pytest.raises(ValueError, match=\"Input contains NaN\"):\n        ohe.fit(X)\n\n    with pytest.raises(ValueError, match=\"Input contains NaN\"):\n        ohe.fit_transform(X)\n\n    ohe.fit(X[:1, :])\n\n    with pytest.raises(ValueError, match=\"Input contains NaN\"):\n        ohe.transform(X)",
            "def test_one_hot_encoder_inverse():\n    for sparse_ in [True, False]:\n        X = [['abc', 2, 55], ['def', 1, 55], ['abc', 3, 55]]\n        enc = OneHotEncoder(sparse=sparse_)\n        X_tr = enc.fit_transform(X)\n        exp = np.array(X, dtype=object)\n        assert_array_equal(enc.inverse_transform(X_tr), exp)\n\n        X = [[2, 55], [1, 55], [3, 55]]\n        enc = OneHotEncoder(sparse=sparse_, categories='auto')\n        X_tr = enc.fit_transform(X)\n        exp = np.array(X)\n        assert_array_equal(enc.inverse_transform(X_tr), exp)\n\n        # with unknown categories\n        X = [['abc', 2, 55], ['def', 1, 55], ['abc', 3, 55]]\n        enc = OneHotEncoder(sparse=sparse_, handle_unknown='ignore',\n                            categories=[['abc', 'def'], [1, 2],\n                                        [54, 55, 56]])\n        X_tr = enc.fit_transform(X)\n        exp = np.array(X, dtype=object)\n        exp[2, 1] = None\n        assert_array_equal(enc.inverse_transform(X_tr), exp)\n\n        # with an otherwise numerical output, still object if unknown\n        X = [[2, 55], [1, 55], [3, 55]]\n        enc = OneHotEncoder(sparse=sparse_, categories=[[1, 2], [54, 56]],\n                            handle_unknown='ignore')\n        X_tr = enc.fit_transform(X)\n        exp = np.array(X, dtype=object)\n        exp[2, 0] = None\n        exp[:, 1] = None\n        assert_array_equal(enc.inverse_transform(X_tr), exp)\n\n        # incorrect shape raises\n        X_tr = np.array([[0, 1, 1], [1, 0, 1]])\n        msg = re.escape('Shape of the passed X data is not correct')\n        assert_raises_regex(ValueError, msg, enc.inverse_transform, X_tr)",
            "def test_one_hot_encoder_specified_categories(X, X2, cats, cat_dtype):\n    enc = OneHotEncoder(categories=cats)\n    exp = np.array([[1., 0., 0.],\n                    [0., 1., 0.]])\n    assert_array_equal(enc.fit_transform(X).toarray(), exp)\n    assert list(enc.categories[0]) == list(cats[0])\n    assert enc.categories_[0].tolist() == list(cats[0])\n    # manually specified categories should have same dtype as\n    # the data when coerced from lists\n    assert enc.categories_[0].dtype == cat_dtype\n\n    # when specifying categories manually, unknown categories should already\n    # raise when fitting\n    enc = OneHotEncoder(categories=cats)\n    with pytest.raises(ValueError, match=\"Found unknown categories\"):\n        enc.fit(X2)\n    enc = OneHotEncoder(categories=cats, handle_unknown='ignore')\n    exp = np.array([[1., 0., 0.], [0., 0., 0.]])\n    assert_array_equal(enc.fit(X2).transform(X2).toarray(), exp)",
            "def test_one_hot_encoder_force_new_behaviour():\n    # ambiguous integer case (non secutive range of categories)\n    X = np.array([[1, 2]]).T\n    X2 = np.array([[0, 1]]).T\n\n    # without argument -> by default using legacy behaviour with warnings\n    enc = OneHotEncoder()\n\n    with ignore_warnings(category=FutureWarning):\n        enc.fit(X)\n\n    res = enc.transform(X2)\n    exp = np.array([[0, 0], [1, 0]])\n    assert_array_equal(res.toarray(), exp)\n\n    # with explicit auto argument -> don't use legacy behaviour\n    # (so will raise an error on unseen value within range)\n    enc = OneHotEncoder(categories='auto')\n    enc.fit(X)\n    assert_raises(ValueError, enc.transform, X2)",
            "def test_one_hot_encoder_specified_categories_mixed_columns():\n    # multiple columns\n    X = np.array([['a', 'b'], [0, 2]], dtype=object).T\n    enc = OneHotEncoder(categories=[['a', 'b', 'c'], [0, 1, 2]])\n    exp = np.array([[1., 0., 0., 1., 0., 0.],\n                    [0., 1., 0., 0., 0., 1.]])\n    assert_array_equal(enc.fit_transform(X).toarray(), exp)\n    assert enc.categories_[0].tolist() == ['a', 'b', 'c']\n    assert np.issubdtype(enc.categories_[0].dtype, np.object_)\n    assert enc.categories_[1].tolist() == [0, 1, 2]\n    # integer categories but from object dtype data\n    assert np.issubdtype(enc.categories_[1].dtype, np.object_)",
            "def test_one_hot_encoder_unsorted_categories():\n    X = np.array([['a', 'b']], dtype=object).T\n\n    enc = OneHotEncoder(categories=[['b', 'a', 'c']])\n    exp = np.array([[0., 1., 0.],\n                    [1., 0., 0.]])\n    assert_array_equal(enc.fit(X).transform(X).toarray(), exp)\n    assert_array_equal(enc.fit_transform(X).toarray(), exp)\n    assert enc.categories_[0].tolist() == ['b', 'a', 'c']\n    assert np.issubdtype(enc.categories_[0].dtype, np.object_)\n\n    # unsorted passed categories still raise for numerical values\n    X = np.array([[1, 2]]).T\n    enc = OneHotEncoder(categories=[[2, 1, 3]])\n    msg = 'Unsorted categories are not supported'\n    with pytest.raises(ValueError, match=msg):\n        enc.fit_transform(X)",
            "def test_one_hot_encoder_categories(X, cat_exp, cat_dtype):\n    # order of categories should not depend on order of samples\n    for Xi in [X, X[::-1]]:\n        enc = OneHotEncoder(categories='auto')\n        enc.fit(Xi)\n        # assert enc.categories == 'auto'\n        assert isinstance(enc.categories_, list)\n        for res, exp in zip(enc.categories_, cat_exp):\n            assert res.tolist() == exp\n            assert np.issubdtype(res.dtype, cat_dtype)",
            "def test_one_hot_encoder_handle_unknown():\n    X = np.array([[0, 2, 1], [1, 0, 3], [1, 0, 2]])\n    X2 = np.array([[4, 1, 1]])\n\n    # Test that one hot encoder raises error for unknown features\n    # present during transform.\n    oh = OneHotEncoder(handle_unknown='error')\n    assert_warns(FutureWarning, oh.fit, X)\n    assert_raises(ValueError, oh.transform, X2)\n\n    # Test the ignore option, ignores unknown features (giving all 0's)\n    oh = OneHotEncoder(handle_unknown='ignore')\n    oh.fit(X)\n    X2_passed = X2.copy()\n    assert_array_equal(\n        oh.transform(X2_passed).toarray(),\n        np.array([[0.,  0.,  0.,  0.,  1.,  0.,  0.]]))\n    # ensure transformed data was not modified in place\n    assert_allclose(X2, X2_passed)\n\n    # Raise error if handle_unknown is neither ignore or error.\n    oh = OneHotEncoder(handle_unknown='42')\n    assert_raises(ValueError, oh.fit, X)",
            "def test_one_hot_encoder_not_fitted():\n    X = np.array([['a'], ['b']])\n    enc = OneHotEncoder(categories=['a', 'b'])\n    msg = (\"This OneHotEncoder instance is not fitted yet. \"\n           \"Call 'fit' with appropriate arguments before using this method.\")\n    with pytest.raises(NotFittedError, match=msg):\n        enc.transform(X)",
            "def test_encoder_dtypes():\n    # check that dtypes are preserved when determining categories\n    enc = OneHotEncoder(categories='auto')\n    exp = np.array([[1., 0., 1., 0.], [0., 1., 0., 1.]], dtype='float64')\n\n    for X in [np.array([[1, 2], [3, 4]], dtype='int64'),\n              np.array([[1, 2], [3, 4]], dtype='float64'),\n              np.array([['a', 'b'], ['c', 'd']]),  # string dtype\n              np.array([[1, 'a'], [3, 'b']], dtype='object')]:\n        enc.fit(X)\n        assert all([enc.categories_[i].dtype == X.dtype for i in range(2)])\n        assert_array_equal(enc.transform(X).toarray(), exp)\n\n    X = [[1, 2], [3, 4]]\n    enc.fit(X)\n    assert all([np.issubdtype(enc.categories_[i].dtype, np.integer)\n                for i in range(2)])\n    assert_array_equal(enc.transform(X).toarray(), exp)\n\n    X = [[1, 'a'], [3, 'b']]\n    enc.fit(X)\n    assert all([enc.categories_[i].dtype == 'object' for i in range(2)])\n    assert_array_equal(enc.transform(X).toarray(), exp)"
        ]
    },
    {
        "repo": "scikit-learn/scikit-learn",
        "instance_id": "scikit-learn__scikit-learn-13496",
        "base_commit": "3aefc834dce72e850bff48689bea3c7dff5f3fad",
        "issue_title": "Expose warm_start in Isolation forest",
        "test_file": "/home/zqc/RepoCodeEdit/data/swe_bench_temp_wy/scikit-learn/sklearn/ensemble/tests/test_iforest.py",
        "searched_functions": [
            "def test_iforest_performance():\n    \"\"\"Test Isolation Forest performs well\"\"\"\n\n    # Generate train/test data\n    rng = check_random_state(2)\n    X = 0.3 * rng.randn(120, 2)\n    X_train = np.r_[X + 2, X - 2]\n    X_train = X[:100]\n\n    # Generate some abnormal novel observations\n    X_outliers = rng.uniform(low=-4, high=4, size=(20, 2))\n    X_test = np.r_[X[100:], X_outliers]\n    y_test = np.array([0] * 20 + [1] * 20)\n\n    # fit the model\n    clf = IsolationForest(max_samples=100, random_state=rng).fit(X_train)\n\n    # predict scores (the lower, the more normal)\n    y_pred = - clf.decision_function(X_test)\n\n    # check that there is at most 6 errors (false positive or false negative)\n    assert_greater(roc_auc_score(y_test, y_pred), 0.98)",
            "def test_iforest():\n    \"\"\"Check Isolation Forest for various parameter settings.\"\"\"\n    X_train = np.array([[0, 1], [1, 2]])\n    X_test = np.array([[2, 1], [1, 1]])\n\n    grid = ParameterGrid({\"n_estimators\": [3],\n                          \"max_samples\": [0.5, 1.0, 3],\n                          \"bootstrap\": [True, False]})\n\n    with ignore_warnings():\n        for params in grid:\n            IsolationForest(random_state=rng,\n                            **params).fit(X_train).predict(X_test)",
            "def test_deprecation():\n    X = [[0.0], [1.0]]\n    clf = IsolationForest()\n\n    assert_warns_message(FutureWarning,\n                         'default contamination parameter 0.1 will change '\n                         'in version 0.22 to \"auto\"',\n                         clf.fit, X)\n\n    assert_warns_message(FutureWarning,\n                         'behaviour=\"old\" is deprecated and will be removed '\n                         'in version 0.22',\n                         clf.fit, X)\n\n    clf = IsolationForest().fit(X)\n    assert_warns_message(DeprecationWarning,\n                         \"threshold_ attribute is deprecated in 0.20 and will\"\n                         \" be removed in 0.22.\",\n                         getattr, clf, \"threshold_\")",
            "def test_iforest_works(contamination):\n    # toy sample (the last two samples are outliers)\n    X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1], [6, 3], [-4, 7]]\n\n    # Test IsolationForest\n    clf = IsolationForest(\n        behaviour=\"new\", random_state=rng, contamination=contamination\n    )\n    clf.fit(X)\n    decision_func = -clf.decision_function(X)\n    pred = clf.predict(X)\n    # assert detect outliers:\n    assert_greater(np.min(decision_func[-2:]), np.max(decision_func[:-2]))\n    assert_array_equal(pred, 6 * [1] + 2 * [-1])",
            "def test_behaviour_param():\n    X_train = [[1, 1], [1, 2], [2, 1]]\n    clf1 = IsolationForest(behaviour='old').fit(X_train)\n    clf2 = IsolationForest(behaviour='new', contamination='auto').fit(X_train)\n    assert_array_equal(clf1.decision_function([[2., 2.]]),\n                       clf2.decision_function([[2., 2.]]))",
            "def test_iforest_sparse():\n    \"\"\"Check IForest for various parameter settings on sparse input.\"\"\"\n    rng = check_random_state(0)\n    X_train, X_test, y_train, y_test = train_test_split(boston.data[:50],\n                                                        boston.target[:50],\n                                                        random_state=rng)\n    grid = ParameterGrid({\"max_samples\": [0.5, 1.0],\n                          \"bootstrap\": [True, False]})\n\n    for sparse_format in [csc_matrix, csr_matrix]:\n        X_train_sparse = sparse_format(X_train)\n        X_test_sparse = sparse_format(X_test)\n\n        for params in grid:\n            # Trained on sparse format\n            sparse_classifier = IsolationForest(\n                n_estimators=10, random_state=1, **params).fit(X_train_sparse)\n            sparse_results = sparse_classifier.predict(X_test_sparse)\n\n            # Trained on dense format\n            dense_classifier = IsolationForest(\n                n_estimators=10, random_state=1, **params).fit(X_train)\n            dense_results = dense_classifier.predict(X_test)\n\n            assert_array_equal(sparse_results, dense_results)",
            "def test_iforest_subsampled_features():\n    # It tests non-regression for #5732 which failed at predict.\n    rng = check_random_state(0)\n    X_train, X_test, y_train, y_test = train_test_split(boston.data[:50],\n                                                        boston.target[:50],\n                                                        random_state=rng)\n    clf = IsolationForest(max_features=0.8)\n    clf.fit(X_train, y_train)\n    clf.predict(X_test)",
            "def test_max_samples_consistency():\n    # Make sure validated max_samples in iforest and BaseBagging are identical\n    X = iris.data\n    clf = IsolationForest().fit(X)\n    assert_equal(clf.max_samples_, clf._max_samples)",
            "def test_iforest_chunks_works2(\n    mocked_get_chunk, contamination, n_predict_calls\n):\n    test_iforest_works(contamination)\n    assert mocked_get_chunk.call_count == n_predict_calls",
            "def test_iforest_chunks_works1(\n    mocked_get_chunk, contamination, n_predict_calls\n):\n    test_iforest_works(contamination)\n    assert mocked_get_chunk.call_count == n_predict_calls"
        ]
    },
    {
        "repo": "scikit-learn/scikit-learn",
        "instance_id": "scikit-learn__scikit-learn-13241",
        "base_commit": "f8b108d0c6f2f82b2dc4e32a6793f9d9ac9cf2f4",
        "issue_title": "Differences among the results of KernelPCA with rbf kernel",
        "test_file": "/home/zqc/RepoCodeEdit/data/swe_bench_temp_wy/scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py",
        "searched_functions": [
            "def test_kernel_pca():\n    rng = np.random.RandomState(0)\n    X_fit = rng.random_sample((5, 4))\n    X_pred = rng.random_sample((2, 4))\n\n    def histogram(x, y, **kwargs):\n        # Histogram kernel implemented as a callable.\n        assert_equal(kwargs, {})    # no kernel_params that we didn't ask for\n        return np.minimum(x, y).sum()\n\n    for eigen_solver in (\"auto\", \"dense\", \"arpack\"):\n        for kernel in (\"linear\", \"rbf\", \"poly\", histogram):\n            # histogram kernel produces singular matrix inside linalg.solve\n            # XXX use a least-squares approximation?\n            inv = not callable(kernel)\n\n            # transform fit data\n            kpca = KernelPCA(4, kernel=kernel, eigen_solver=eigen_solver,\n                             fit_inverse_transform=inv)\n            X_fit_transformed = kpca.fit_transform(X_fit)\n            X_fit_transformed2 = kpca.fit(X_fit).transform(X_fit)\n            assert_array_almost_equal(np.abs(X_fit_transformed),\n                                      np.abs(X_fit_transformed2))\n\n            # non-regression test: previously, gamma would be 0 by default,\n            # forcing all eigenvalues to 0 under the poly kernel\n            assert_not_equal(X_fit_transformed.size, 0)\n\n            # transform new data\n            X_pred_transformed = kpca.transform(X_pred)\n            assert_equal(X_pred_transformed.shape[1],\n                         X_fit_transformed.shape[1])\n\n            # inverse transform\n            if inv:\n                X_pred2 = kpca.inverse_transform(X_pred_transformed)\n                assert_equal(X_pred2.shape, X_pred.shape)",
            "def test_kernel_pca_sparse():\n    rng = np.random.RandomState(0)\n    X_fit = sp.csr_matrix(rng.random_sample((5, 4)))\n    X_pred = sp.csr_matrix(rng.random_sample((2, 4)))\n\n    for eigen_solver in (\"auto\", \"arpack\"):\n        for kernel in (\"linear\", \"rbf\", \"poly\"):\n            # transform fit data\n            kpca = KernelPCA(4, kernel=kernel, eigen_solver=eigen_solver,\n                             fit_inverse_transform=False)\n            X_fit_transformed = kpca.fit_transform(X_fit)\n            X_fit_transformed2 = kpca.fit(X_fit).transform(X_fit)\n            assert_array_almost_equal(np.abs(X_fit_transformed),\n                                      np.abs(X_fit_transformed2))\n\n            # transform new data\n            X_pred_transformed = kpca.transform(X_pred)\n            assert_equal(X_pred_transformed.shape[1],\n                         X_fit_transformed.shape[1])",
            "def test_kernel_pca_precomputed():\n    rng = np.random.RandomState(0)\n    X_fit = rng.random_sample((5, 4))\n    X_pred = rng.random_sample((2, 4))\n\n    for eigen_solver in (\"dense\", \"arpack\"):\n        X_kpca = KernelPCA(4, eigen_solver=eigen_solver).\\\n            fit(X_fit).transform(X_pred)\n        X_kpca2 = KernelPCA(\n            4, eigen_solver=eigen_solver, kernel='precomputed').fit(\n                np.dot(X_fit, X_fit.T)).transform(np.dot(X_pred, X_fit.T))\n\n        X_kpca_train = KernelPCA(\n            4, eigen_solver=eigen_solver,\n            kernel='precomputed').fit_transform(np.dot(X_fit, X_fit.T))\n        X_kpca_train2 = KernelPCA(\n            4, eigen_solver=eigen_solver, kernel='precomputed').fit(\n                np.dot(X_fit, X_fit.T)).transform(np.dot(X_fit, X_fit.T))\n\n        assert_array_almost_equal(np.abs(X_kpca),\n                                  np.abs(X_kpca2))\n\n        assert_array_almost_equal(np.abs(X_kpca_train),\n                                  np.abs(X_kpca_train2))",
            "def test_kernel_pca_linear_kernel():\n    rng = np.random.RandomState(0)\n    X_fit = rng.random_sample((5, 4))\n    X_pred = rng.random_sample((2, 4))\n\n    # for a linear kernel, kernel PCA should find the same projection as PCA\n    # modulo the sign (direction)\n    # fit only the first four components: fifth is near zero eigenvalue, so\n    # can be trimmed due to roundoff error\n    assert_array_almost_equal(\n        np.abs(KernelPCA(4).fit(X_fit).transform(X_pred)),\n        np.abs(PCA(4).fit(X_fit).transform(X_pred)))",
            "def test_kernel_pca_invalid_parameters():\n    assert_raises(ValueError, KernelPCA, 10, fit_inverse_transform=True,\n                  kernel='precomputed')",
            "def test_kernel_pca_invalid_kernel():\n    rng = np.random.RandomState(0)\n    X_fit = rng.random_sample((2, 4))\n    kpca = KernelPCA(kernel=\"tototiti\")\n    assert_raises(ValueError, kpca.fit, X_fit)",
            "def test_nested_circles():\n    # Test the linear separability of the first 2D KPCA transform\n    X, y = make_circles(n_samples=400, factor=.3, noise=.05,\n                        random_state=0)\n\n    # 2D nested circles are not linearly separable\n    train_score = Perceptron(max_iter=5).fit(X, y).score(X, y)\n    assert_less(train_score, 0.8)\n\n    # Project the circles data into the first 2 components of a RBF Kernel\n    # PCA model.\n    # Note that the gamma value is data dependent. If this test breaks\n    # and the gamma value has to be updated, the Kernel PCA example will\n    # have to be updated too.\n    kpca = KernelPCA(kernel=\"rbf\", n_components=2,\n                     fit_inverse_transform=True, gamma=2.)\n    X_kpca = kpca.fit_transform(X)\n\n    # The data is perfectly linearly separable in that space\n    train_score = Perceptron(max_iter=5).fit(X_kpca, y).score(X_kpca, y)\n    assert_equal(train_score, 1.0)",
            "def test_kernel_pca_n_components():\n    rng = np.random.RandomState(0)\n    X_fit = rng.random_sample((5, 4))\n    X_pred = rng.random_sample((2, 4))\n\n    for eigen_solver in (\"dense\", \"arpack\"):\n        for c in [1, 2, 4]:\n            kpca = KernelPCA(n_components=c, eigen_solver=eigen_solver)\n            shape = kpca.fit(X_fit).transform(X_pred).shape\n\n            assert_equal(shape, (2, c))",
            "def test_kernel_pca_consistent_transform():\n    # X_fit_ needs to retain the old, unmodified copy of X\n    state = np.random.RandomState(0)\n    X = state.rand(10, 10)\n    kpca = KernelPCA(random_state=state).fit(X)\n    transformed1 = kpca.transform(X)\n\n    X_copy = X.copy()\n    X[:, 0] = 666\n    transformed2 = kpca.transform(X_copy)\n    assert_array_almost_equal(transformed1, transformed2)",
            "def test_gridsearch_pipeline_precomputed():\n    # Test if we can do a grid-search to find parameters to separate\n    # circles with a perceptron model using a precomputed kernel.\n    X, y = make_circles(n_samples=400, factor=.3, noise=.05,\n                        random_state=0)\n    kpca = KernelPCA(kernel=\"precomputed\", n_components=2)\n    pipeline = Pipeline([(\"kernel_pca\", kpca),\n                         (\"Perceptron\", Perceptron(max_iter=5))])\n    param_grid = dict(Perceptron__max_iter=np.arange(1, 5))\n    grid_search = GridSearchCV(pipeline, cv=3, param_grid=param_grid)\n    X_kernel = rbf_kernel(X, gamma=2.)\n    grid_search.fit(X_kernel, y)\n    assert_equal(grid_search.best_score_, 1)"
        ]
    },
    {
        "repo": "scikit-learn/scikit-learn",
        "instance_id": "scikit-learn__scikit-learn-15512",
        "base_commit": "b8a4da8baa1137f173e7035f104067c7d2ffde22",
        "issue_title": "Return values of non converged affinity propagation clustering",
        "test_file": "/home/zqc/RepoCodeEdit/data/swe_bench_temp_wy/scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py",
        "searched_functions": [
            "def test_affinity_propagation_fit_non_convergence():\n    # In case of non-convergence of affinity_propagation(), the cluster\n    # centers should be an empty array and training samples should be labelled\n    # as noise (-1)\n    X = np.array([[0, 0], [1, 1], [-2, -2]])\n\n    # Force non-convergence by allowing only a single iteration\n    af = AffinityPropagation(preference=-10, max_iter=1)\n\n    assert_warns(ConvergenceWarning, af.fit, X)\n    assert_array_equal(np.empty((0, 2)), af.cluster_centers_)\n    assert_array_equal(np.array([-1, -1, -1]), af.labels_)",
            "def test_affinity_propagation_predict_non_convergence():\n    # In case of non-convergence of affinity_propagation(), the cluster\n    # centers should be an empty array\n    X = np.array([[0, 0], [1, 1], [-2, -2]])\n\n    # Force non-convergence by allowing only a single iteration\n    af = assert_warns(ConvergenceWarning,\n                      AffinityPropagation(preference=-10, max_iter=1).fit, X)\n\n    # At prediction time, consider new samples as noise since there are no\n    # clusters\n    to_predict = np.array([[2, 2], [3, 3], [4, 4]])\n    y = assert_warns(ConvergenceWarning, af.predict, to_predict)\n    assert_array_equal(np.array([-1, -1, -1]), y)",
            "def test_affinity_propagation():\n    # Affinity Propagation algorithm\n    # Compute similarities\n    S = -euclidean_distances(X, squared=True)\n    preference = np.median(S) * 10\n    # Compute Affinity Propagation\n    cluster_centers_indices, labels = affinity_propagation(\n        S, preference=preference)\n\n    n_clusters_ = len(cluster_centers_indices)\n\n    assert n_clusters == n_clusters_\n\n    af = AffinityPropagation(preference=preference, affinity=\"precomputed\")\n    labels_precomputed = af.fit(S).labels_\n\n    af = AffinityPropagation(preference=preference, verbose=True)\n    labels = af.fit(X).labels_\n\n    assert_array_equal(labels, labels_precomputed)\n\n    cluster_centers_indices = af.cluster_centers_indices_\n\n    n_clusters_ = len(cluster_centers_indices)\n    assert np.unique(labels).size == n_clusters_\n    assert n_clusters == n_clusters_\n\n    # Test also with no copy\n    _, labels_no_copy = affinity_propagation(S, preference=preference,\n                                             copy=False)\n    assert_array_equal(labels, labels_no_copy)\n\n    # Test input validation\n    with pytest.raises(ValueError):\n        affinity_propagation(S[:, :-1])\n    with pytest.raises(ValueError):\n        affinity_propagation(S, damping=0)\n    af = AffinityPropagation(affinity=\"unknown\")\n    with pytest.raises(ValueError):\n        af.fit(X)\n    af_2 = AffinityPropagation(affinity='precomputed')\n    with pytest.raises(TypeError):\n        af_2.fit(csr_matrix((3, 3)))",
            "def test_affinity_propagation_convergence_warning_dense_sparse(centers):\n    \"\"\"Non-regression, see #13334\"\"\"\n    rng = np.random.RandomState(42)\n    X = rng.rand(40, 10)\n    y = (4 * rng.rand(40)).astype(np.int)\n    ap = AffinityPropagation()\n    ap.fit(X, y)\n    ap.cluster_centers_ = centers\n    with pytest.warns(None) as record:\n        assert_array_equal(ap.predict(X),\n                           np.zeros(X.shape[0], dtype=int))\n    assert len(record) == 0",
            "def test_affinity_propagation_predict():\n    # Test AffinityPropagation.predict\n    af = AffinityPropagation(affinity=\"euclidean\")\n    labels = af.fit_predict(X)\n    labels2 = af.predict(X)\n    assert_array_equal(labels, labels2)",
            "def test_affinity_propagation_equal_mutual_similarities():\n    X = np.array([[-1, 1], [1, -1]])\n    S = -euclidean_distances(X, squared=True)\n\n    # setting preference > similarity\n    cluster_center_indices, labels = assert_warns_message(\n        UserWarning, \"mutually equal\", affinity_propagation, S, preference=0)\n\n    # expect every sample to become an exemplar\n    assert_array_equal([0, 1], cluster_center_indices)\n    assert_array_equal([0, 1], labels)\n\n    # setting preference < similarity\n    cluster_center_indices, labels = assert_warns_message(\n        UserWarning, \"mutually equal\", affinity_propagation, S, preference=-10)\n\n    # expect one cluster, with arbitrary (first) sample as exemplar\n    assert_array_equal([0], cluster_center_indices)\n    assert_array_equal([0, 0], labels)\n\n    # setting different preferences\n    cluster_center_indices, labels = assert_no_warnings(\n        affinity_propagation, S, preference=[-20, -10])\n\n    # expect one cluster, with highest-preference sample as exemplar\n    assert_array_equal([1], cluster_center_indices)\n    assert_array_equal([0, 0], labels)",
            "def test_affinity_propagation_predict_error():\n    # Test exception in AffinityPropagation.predict\n    # Not fitted.\n    af = AffinityPropagation(affinity=\"euclidean\")\n    with pytest.raises(ValueError):\n        af.predict(X)\n\n    # Predict not supported when affinity=\"precomputed\".\n    S = np.dot(X, X.T)\n    af = AffinityPropagation(affinity=\"precomputed\")\n    af.fit(S)\n    with pytest.raises(ValueError):\n        af.predict(X)",
            "def test_equal_similarities_and_preferences():\n    # Unequal distances\n    X = np.array([[0, 0], [1, 1], [-2, -2]])\n    S = -euclidean_distances(X, squared=True)\n\n    assert not _equal_similarities_and_preferences(S, np.array(0))\n    assert not _equal_similarities_and_preferences(S, np.array([0, 0]))\n    assert not _equal_similarities_and_preferences(S, np.array([0, 1]))\n\n    # Equal distances\n    X = np.array([[0, 0], [1, 1]])\n    S = -euclidean_distances(X, squared=True)\n\n    # Different preferences\n    assert not _equal_similarities_and_preferences(S, np.array([0, 1]))\n\n    # Same preferences\n    assert _equal_similarities_and_preferences(S, np.array([0, 0]))\n    assert _equal_similarities_and_preferences(S, np.array(0))"
        ]
    },
    {
        "repo": "scikit-learn/scikit-learn",
        "instance_id": "scikit-learn__scikit-learn-11040",
        "base_commit": "96a02f3934952d486589dddd3f00b40d5a5ab5f2",
        "issue_title": "Missing parameter validation in Neighbors estimator for float n_neighbors",
        "test_file": "/home/zqc/RepoCodeEdit/data/swe_bench_temp_wy/scikit-learn/sklearn/neighbors/tests/test_neighbors.py",
        "searched_functions": [
            "def test_radius_neighbors_regressor(n_samples=40,\n                                    n_features=3,\n                                    n_test_pts=10,\n                                    radius=0.5,\n                                    random_state=0):\n    # Test radius-based neighbors regression\n    rng = np.random.RandomState(random_state)\n    X = 2 * rng.rand(n_samples, n_features) - 1\n    y = np.sqrt((X ** 2).sum(1))\n    y /= y.max()\n\n    y_target = y[:n_test_pts]\n\n    weight_func = _weight_func\n\n    for algorithm in ALGORITHMS:\n        for weights in ['uniform', 'distance', weight_func]:\n            neigh = neighbors.RadiusNeighborsRegressor(radius=radius,\n                                                       weights=weights,\n                                                       algorithm=algorithm)\n            neigh.fit(X, y)\n            epsilon = 1E-5 * (2 * rng.rand(1, n_features) - 1)\n            y_pred = neigh.predict(X[:n_test_pts] + epsilon)\n            assert_true(np.all(abs(y_pred - y_target) < radius / 2))\n\n    # test that nan is returned when no nearby observations\n    for weights in ['uniform', 'distance']:\n        neigh = neighbors.RadiusNeighborsRegressor(radius=radius,\n                                                   weights=weights,\n                                                   algorithm='auto')\n        neigh.fit(X, y)\n        X_test_nan = np.ones((1, n_features))*-1\n        empty_warning_msg = (\"One or more samples have no neighbors \"\n                             \"within specified radius; predicting NaN.\")\n        pred = assert_warns_message(UserWarning,\n                                    empty_warning_msg,\n                                    neigh.predict,\n                                    X_test_nan)\n        assert_true(np.all(np.isnan(pred)))",
            "def test_kneighbors_classifier_float_labels(n_samples=40, n_features=5,\n                                            n_test_pts=10, n_neighbors=5,\n                                            random_state=0):\n    # Test k-neighbors classification\n    rng = np.random.RandomState(random_state)\n    X = 2 * rng.rand(n_samples, n_features) - 1\n    y = ((X ** 2).sum(axis=1) < .5).astype(np.int)\n\n    knn = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors)\n    knn.fit(X, y.astype(np.float))\n    epsilon = 1e-5 * (2 * rng.rand(1, n_features) - 1)\n    y_pred = knn.predict(X[:n_test_pts] + epsilon)\n    assert_array_equal(y_pred, y[:n_test_pts])",
            "def test_neighbors_badargs():\n    # Test bad argument values: these should all raise ValueErrors\n    assert_raises(ValueError,\n                  neighbors.NearestNeighbors,\n                  algorithm='blah')\n\n    X = rng.random_sample((10, 2))\n    Xsparse = csr_matrix(X)\n    y = np.ones(10)\n\n    for cls in (neighbors.KNeighborsClassifier,\n                neighbors.RadiusNeighborsClassifier,\n                neighbors.KNeighborsRegressor,\n                neighbors.RadiusNeighborsRegressor):\n        assert_raises(ValueError,\n                      cls,\n                      weights='blah')\n        assert_raises(ValueError,\n                      cls, p=-1)\n        assert_raises(ValueError,\n                      cls, algorithm='blah')\n        nbrs = cls(algorithm='ball_tree', metric='haversine')\n        assert_raises(ValueError,\n                      nbrs.predict,\n                      X)\n        assert_raises(ValueError,\n                      ignore_warnings(nbrs.fit),\n                      Xsparse, y)\n        nbrs = cls()\n        assert_raises(ValueError,\n                      nbrs.fit,\n                      np.ones((0, 2)), np.ones(0))\n        assert_raises(ValueError,\n                      nbrs.fit,\n                      X[:, :, None], y)\n        nbrs.fit(X, y)\n        assert_raises(ValueError,\n                      nbrs.predict,\n                      [[]])\n        if (isinstance(cls, neighbors.KNeighborsClassifier) or\n                isinstance(cls, neighbors.KNeighborsRegressor)):\n            nbrs = cls(n_neighbors=-1)\n            assert_raises(ValueError, nbrs.fit, X, y)\n\n    nbrs = neighbors.NearestNeighbors().fit(X)\n\n    assert_raises(ValueError, nbrs.kneighbors_graph, X, mode='blah')\n    assert_raises(ValueError, nbrs.radius_neighbors_graph, X, mode='blah')",
            "def test_neighbors_digits():\n    # Sanity check on the digits dataset\n    # the 'brute' algorithm has been observed to fail if the input\n    # dtype is uint8 due to overflow in distance calculations.\n\n    X = digits.data.astype('uint8')\n    Y = digits.target\n    (n_samples, n_features) = X.shape\n    train_test_boundary = int(n_samples * 0.8)\n    train = np.arange(0, train_test_boundary)\n    test = np.arange(train_test_boundary, n_samples)\n    (X_train, Y_train, X_test, Y_test) = X[train], Y[train], X[test], Y[test]\n\n    clf = neighbors.KNeighborsClassifier(n_neighbors=1, algorithm='brute')\n    score_uint8 = clf.fit(X_train, Y_train).score(X_test, Y_test)\n    score_float = clf.fit(X_train.astype(float), Y_train).score(\n        X_test.astype(float), Y_test)\n    assert_equal(score_uint8, score_float)",
            "def test_metric_params_interface():\n    assert_warns(SyntaxWarning, neighbors.KNeighborsClassifier,\n                 metric_params={'p': 3})",
            "def test_radius_neighbors_classifier_when_no_neighbors():\n    # Test radius-based classifier when no neighbors found.\n    # In this case it should rise an informative exception\n\n    X = np.array([[1.0, 1.0], [2.0, 2.0]])\n    y = np.array([1, 2])\n    radius = 0.1\n\n    z1 = np.array([[1.01, 1.01], [2.01, 2.01]])  # no outliers\n    z2 = np.array([[1.01, 1.01], [1.4, 1.4]])    # one outlier\n\n    weight_func = _weight_func\n\n    for outlier_label in [0, -1, None]:\n        for algorithm in ALGORITHMS:\n            for weights in ['uniform', 'distance', weight_func]:\n                rnc = neighbors.RadiusNeighborsClassifier\n                clf = rnc(radius=radius, weights=weights, algorithm=algorithm,\n                          outlier_label=outlier_label)\n                clf.fit(X, y)\n                assert_array_equal(np.array([1, 2]),\n                                   clf.predict(z1))\n                if outlier_label is None:\n                    assert_raises(ValueError, clf.predict, z2)\n                elif False:\n                    assert_array_equal(np.array([1, outlier_label]),\n                                       clf.predict(z2))",
            "def test_kneighbors_regressor(n_samples=40,\n                              n_features=5,\n                              n_test_pts=10,\n                              n_neighbors=3,\n                              random_state=0):\n    # Test k-neighbors regression\n    rng = np.random.RandomState(random_state)\n    X = 2 * rng.rand(n_samples, n_features) - 1\n    y = np.sqrt((X ** 2).sum(1))\n    y /= y.max()\n\n    y_target = y[:n_test_pts]\n\n    weight_func = _weight_func\n\n    for algorithm in ALGORITHMS:\n        for weights in ['uniform', 'distance', weight_func]:\n            knn = neighbors.KNeighborsRegressor(n_neighbors=n_neighbors,\n                                                weights=weights,\n                                                algorithm=algorithm)\n            knn.fit(X, y)\n            epsilon = 1E-5 * (2 * rng.rand(1, n_features) - 1)\n            y_pred = knn.predict(X[:n_test_pts] + epsilon)\n            assert_true(np.all(abs(y_pred - y_target) < 0.3))",
            "def test_kneighbors_regressor_sparse(n_samples=40,\n                                     n_features=5,\n                                     n_test_pts=10,\n                                     n_neighbors=5,\n                                     random_state=0):\n    # Test radius-based regression on sparse matrices\n    # Like the above, but with various types of sparse matrices\n    rng = np.random.RandomState(random_state)\n    X = 2 * rng.rand(n_samples, n_features) - 1\n    y = ((X ** 2).sum(axis=1) < .25).astype(np.int)\n\n    for sparsemat in SPARSE_TYPES:\n        knn = neighbors.KNeighborsRegressor(n_neighbors=n_neighbors,\n                                            algorithm='auto')\n        knn.fit(sparsemat(X), y)\n\n        knn_pre = neighbors.KNeighborsRegressor(n_neighbors=n_neighbors,\n                                                metric='precomputed')\n        knn_pre.fit(pairwise_distances(X, metric='euclidean'), y)\n\n        for sparsev in SPARSE_OR_DENSE:\n            X2 = sparsev(X)\n            assert_true(np.mean(knn.predict(X2).round() == y) > 0.95)\n\n            X2_pre = sparsev(pairwise_distances(X, metric='euclidean'))\n            if issparse(sparsev(X2_pre)):\n                assert_raises(ValueError, knn_pre.predict, X2_pre)\n            else:\n                assert_true(\n                    np.mean(knn_pre.predict(X2_pre).round() == y) > 0.95)",
            "def test_RadiusNeighborsRegressor_multioutput(n_samples=40,\n                                              n_features=5,\n                                              n_test_pts=10,\n                                              n_neighbors=3,\n                                              random_state=0):\n    # Test k-neighbors in multi-output regression with various weight\n    rng = np.random.RandomState(random_state)\n    X = 2 * rng.rand(n_samples, n_features) - 1\n    y = np.sqrt((X ** 2).sum(1))\n    y /= y.max()\n    y = np.vstack([y, y]).T\n\n    y_target = y[:n_test_pts]\n    weights = ['uniform', 'distance', _weight_func]\n\n    for algorithm, weights in product(ALGORITHMS, weights):\n        rnn = neighbors.RadiusNeighborsRegressor(n_neighbors=n_neighbors,\n                                                 weights=weights,\n                                                 algorithm=algorithm)\n        rnn.fit(X, y)\n        epsilon = 1E-5 * (2 * rng.rand(1, n_features) - 1)\n        y_pred = rnn.predict(X[:n_test_pts] + epsilon)\n\n        assert_equal(y_pred.shape, y_target.shape)\n        assert_true(np.all(np.abs(y_pred - y_target) < 0.3))",
            "def test_k_and_radius_neighbors_train_is_not_query():\n    # Test kneighbors et.al when query is not training data\n\n    for algorithm in ALGORITHMS:\n\n        nn = neighbors.NearestNeighbors(n_neighbors=1, algorithm=algorithm)\n\n        X = [[0], [1]]\n        nn.fit(X)\n        test_data = [[2], [1]]\n\n        # Test neighbors.\n        dist, ind = nn.kneighbors(test_data)\n        assert_array_equal(dist, [[1], [0]])\n        assert_array_equal(ind, [[1], [1]])\n        dist, ind = nn.radius_neighbors([[2], [1]], radius=1.5)\n        check_object_arrays(dist, [[1], [1, 0]])\n        check_object_arrays(ind, [[1], [0, 1]])\n\n        # Test the graph variants.\n        assert_array_equal(\n            nn.kneighbors_graph(test_data).A, [[0., 1.], [0., 1.]])\n        assert_array_equal(\n            nn.kneighbors_graph([[2], [1]], mode='distance').A,\n            np.array([[0., 1.], [0., 0.]]))\n        rng = nn.radius_neighbors_graph([[2], [1]], radius=1.5)\n        assert_array_equal(rng.A, [[0, 1], [1, 1]])"
        ]
    },
    {
        "repo": "scikit-learn/scikit-learn",
        "instance_id": "scikit-learn__scikit-learn-10297",
        "base_commit": "b90661d6a46aa3619d3eec94d5281f5888add501",
        "issue_title": "linear_model.RidgeClassifierCV's Parameter store_cv_values issue",
        "test_file": "/home/zqc/RepoCodeEdit/data/swe_bench_temp_wy/scikit-learn/sklearn/linear_model/tests/test_ridge.py",
        "searched_functions": [
            "def test_ridgecv_store_cv_values():\n    # Test _RidgeCV's store_cv_values attribute.\n    rng = rng = np.random.RandomState(42)\n\n    n_samples = 8\n    n_features = 5\n    x = rng.randn(n_samples, n_features)\n    alphas = [1e-1, 1e0, 1e1]\n    n_alphas = len(alphas)\n\n    r = RidgeCV(alphas=alphas, store_cv_values=True)\n\n    # with len(y.shape) == 1\n    y = rng.randn(n_samples)\n    r.fit(x, y)\n    assert_equal(r.cv_values_.shape, (n_samples, n_alphas))\n\n    # with len(y.shape) == 2\n    n_responses = 3\n    y = rng.randn(n_samples, n_responses)\n    r.fit(x, y)\n    assert_equal(r.cv_values_.shape, (n_samples, n_responses, n_alphas))",
            "def test_ridge_cv_sparse_svd():\n    X = sp.csr_matrix(X_diabetes)\n    ridge = RidgeCV(gcv_mode=\"svd\")\n    assert_raises(TypeError, ridge.fit, X)",
            "def _test_ridge_cv_normalize(filter_):\n    ridge_cv = RidgeCV(normalize=True, cv=3)\n    ridge_cv.fit(filter_(10. * X_diabetes), y_diabetes)\n\n    gs = GridSearchCV(Ridge(normalize=True), cv=3,\n                      param_grid={'alpha': ridge_cv.alphas})\n    gs.fit(filter_(10. * X_diabetes), y_diabetes)\n    assert_equal(gs.best_estimator_.alpha, ridge_cv.alpha_)",
            "def _test_ridge_cv(filter_):\n    ridge_cv = RidgeCV()\n    ridge_cv.fit(filter_(X_diabetes), y_diabetes)\n    ridge_cv.predict(filter_(X_diabetes))\n\n    assert_equal(len(ridge_cv.coef_.shape), 1)\n    assert_equal(type(ridge_cv.intercept_), np.float64)\n\n    cv = KFold(5)\n    ridge_cv.set_params(cv=cv)\n    ridge_cv.fit(filter_(X_diabetes), y_diabetes)\n    ridge_cv.predict(filter_(X_diabetes))\n\n    assert_equal(len(ridge_cv.coef_.shape), 1)\n    assert_equal(type(ridge_cv.intercept_), np.float64)",
            "def test_class_weights_cv():\n    # Test class weights for cross validated ridge classifier.\n    X = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0],\n                  [1.0, 1.0], [1.0, 0.0]])\n    y = [1, 1, 1, -1, -1]\n\n    reg = RidgeClassifierCV(class_weight=None, alphas=[.01, .1, 1])\n    reg.fit(X, y)\n\n    # we give a small weights to class 1\n    reg = RidgeClassifierCV(class_weight={1: 0.001}, alphas=[.01, .1, 1, 10])\n    reg.fit(X, y)\n\n    assert_array_equal(reg.predict([[-.2, 2]]), np.array([-1]))",
            "def test_errors_and_values_svd_helper():\n    ridgecv = _RidgeGCV()\n    rng = check_random_state(42)\n    alpha = 1.\n    for n, p in zip((5, 10), (12, 6)):\n        y = rng.randn(n)\n        v = rng.randn(p)\n        U = rng.randn(n, p)\n        UT_y = U.T.dot(y)\n        G_diag, c = ridgecv._errors_and_values_svd_helper(alpha, y, v, U, UT_y)\n\n        # test that helper function behaves as expected\n        out, c_ = ridgecv._errors_svd(alpha, y, v, U, UT_y)\n        np.testing.assert_array_equal(out, (c / G_diag) ** 2)\n        np.testing.assert_array_equal(c, c)\n\n        out, c_ = ridgecv._values_svd(alpha, y, v, U, UT_y)\n        np.testing.assert_array_equal(out, y - (c / G_diag))\n        np.testing.assert_array_equal(c_, c)",
            "def fit_ridge_not_ok():\n            ridge.fit(X, y, sample_weights_not_OK)",
            "def test_errors_and_values_helper():\n    ridgecv = _RidgeGCV()\n    rng = check_random_state(42)\n    alpha = 1.\n    n = 5\n    y = rng.randn(n)\n    v = rng.randn(n)\n    Q = rng.randn(len(v), len(v))\n    QT_y = Q.T.dot(y)\n    G_diag, c = ridgecv._errors_and_values_helper(alpha, y, v, Q, QT_y)\n\n    # test that helper function behaves as expected\n    out, c_ = ridgecv._errors(alpha, y, v, Q, QT_y)\n    np.testing.assert_array_equal(out, (c / G_diag) ** 2)\n    np.testing.assert_array_equal(c, c)\n\n    out, c_ = ridgecv._values(alpha, y, v, Q, QT_y)\n    np.testing.assert_array_equal(out, y - (c / G_diag))\n    np.testing.assert_array_equal(c_, c)",
            "def fit_ridge_not_ok_2():\n            ridge.fit(X, y, sample_weights_not_OK_2)",
            "def test_ridgecv_sample_weight():\n    rng = np.random.RandomState(0)\n    alphas = (0.1, 1.0, 10.0)\n\n    # There are different algorithms for n_samples > n_features\n    # and the opposite, so test them both.\n    for n_samples, n_features in ((6, 5), (5, 10)):\n        y = rng.randn(n_samples)\n        X = rng.randn(n_samples, n_features)\n        sample_weight = 1.0 + rng.rand(n_samples)\n\n        cv = KFold(5)\n        ridgecv = RidgeCV(alphas=alphas, cv=cv)\n        ridgecv.fit(X, y, sample_weight=sample_weight)\n\n        # Check using GridSearchCV directly\n        parameters = {'alpha': alphas}\n        gs = GridSearchCV(Ridge(), parameters, cv=cv)\n        gs.fit(X, y, sample_weight=sample_weight)\n\n        assert_equal(ridgecv.alpha_, gs.best_estimator_.alpha)\n        assert_array_almost_equal(ridgecv.coef_, gs.best_estimator_.coef_)"
        ]
    },
    {
        "repo": "scikit-learn/scikit-learn",
        "instance_id": "scikit-learn__scikit-learn-25638",
        "base_commit": "6adb209acd63825affc884abcd85381f148fb1b0",
        "issue_title": "Support nullable pandas dtypes in `unique_labels`",
        "test_file": "/home/zqc/RepoCodeEdit/data/swe_bench_temp_wy/scikit-learn/sklearn/preprocessing/tests/test_encoders.py",
        "searched_functions": [
            "def test_ohe_missing_value_support_pandas():\n    # check support for pandas with mixed dtypes and missing values\n    pd = pytest.importorskip(\"pandas\")\n    df = pd.DataFrame(\n        {\n            \"col1\": [\"dog\", \"cat\", None, \"cat\"],\n            \"col2\": np.array([3, 0, 4, np.nan], dtype=float),\n        },\n        columns=[\"col1\", \"col2\"],\n    )\n    expected_df_trans = np.array(\n        [\n            [0, 1, 0, 0, 1, 0, 0],\n            [1, 0, 0, 1, 0, 0, 0],\n            [0, 0, 1, 0, 0, 1, 0],\n            [1, 0, 0, 0, 0, 0, 1],\n        ]\n    )\n\n    Xtr = check_categorical_onehot(df)\n    assert_allclose(Xtr, expected_df_trans)",
            "def test_ohe_missing_value_support_pandas_categorical(pd_nan_type, handle_unknown):\n    # checks pandas dataframe with categorical features\n    pd = pytest.importorskip(\"pandas\")\n\n    pd_missing_value = pd.NA if pd_nan_type == \"pd.NA\" else np.nan\n\n    df = pd.DataFrame(\n        {\n            \"col1\": pd.Series([\"c\", \"a\", pd_missing_value, \"b\", \"a\"], dtype=\"category\"),\n        }\n    )\n    expected_df_trans = np.array(\n        [\n            [0, 0, 1, 0],\n            [1, 0, 0, 0],\n            [0, 0, 0, 1],\n            [0, 1, 0, 0],\n            [1, 0, 0, 0],\n        ]\n    )\n\n    ohe = OneHotEncoder(sparse_output=False, handle_unknown=handle_unknown)\n    df_trans = ohe.fit_transform(df)\n    assert_allclose(expected_df_trans, df_trans)\n\n    assert len(ohe.categories_) == 1\n    assert_array_equal(ohe.categories_[0][:-1], [\"a\", \"b\", \"c\"])\n    assert np.isnan(ohe.categories_[0][-1])",
            "def test_ohe_infrequent_multiple_categories_dtypes():\n    \"\"\"Test infrequent categories with a pandas dataframe with multiple dtypes.\"\"\"\n\n    pd = pytest.importorskip(\"pandas\")\n    X = pd.DataFrame(\n        {\n            \"str\": [\"a\", \"f\", \"c\", \"f\", \"f\", \"a\", \"c\", \"b\", \"b\"],\n            \"int\": [5, 3, 0, 10, 10, 12, 0, 3, 5],\n        },\n        columns=[\"str\", \"int\"],\n    )\n\n    ohe = OneHotEncoder(\n        categories=\"auto\", max_categories=3, handle_unknown=\"infrequent_if_exist\"\n    )\n    # X[:, 0] 'a', 'b', 'c' have the same frequency. 'a' and 'b' will be\n    # considered infrequent because they are greater\n\n    # X[:, 1] 0, 3, 5, 10 has frequency 2 and 12 has frequency 1.\n    # 0, 3, 12 will be considered infrequent\n\n    X_trans = ohe.fit_transform(X).toarray()\n    assert_array_equal(ohe.infrequent_categories_[0], [\"a\", \"b\"])\n    assert_array_equal(ohe.infrequent_categories_[1], [0, 3, 12])\n\n    expected = [\n        [0, 0, 1, 1, 0, 0],\n        [0, 1, 0, 0, 0, 1],\n        [1, 0, 0, 0, 0, 1],\n        [0, 1, 0, 0, 1, 0],\n        [0, 1, 0, 0, 1, 0],\n        [0, 0, 1, 0, 0, 1],\n        [1, 0, 0, 0, 0, 1],\n        [0, 0, 1, 0, 0, 1],\n        [0, 0, 1, 1, 0, 0],\n    ]\n\n    assert_allclose(expected, X_trans)\n\n    X_test = pd.DataFrame({\"str\": [\"b\", \"f\"], \"int\": [14, 12]}, columns=[\"str\", \"int\"])\n\n    expected = [[0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 0, 1]]\n    X_test_trans = ohe.transform(X_test)\n    assert_allclose(expected, X_test_trans.toarray())\n\n    X_inv = ohe.inverse_transform(X_test_trans)\n    expected_inv = np.array(\n        [[\"infrequent_sklearn\", \"infrequent_sklearn\"], [\"f\", \"infrequent_sklearn\"]],\n        dtype=object,\n    )\n    assert_array_equal(expected_inv, X_inv)\n\n    # only infrequent or known categories\n    X_test = pd.DataFrame({\"str\": [\"c\", \"b\"], \"int\": [12, 5]}, columns=[\"str\", \"int\"])\n    X_test_trans = ohe.transform(X_test).toarray()\n    expected = [[1, 0, 0, 0, 0, 1], [0, 0, 1, 1, 0, 0]]\n    assert_allclose(expected, X_test_trans)\n\n    X_inv = ohe.inverse_transform(X_test_trans)\n    expected_inv = np.array(\n        [[\"c\", \"infrequent_sklearn\"], [\"infrequent_sklearn\", 5]], dtype=object\n    )\n    assert_array_equal(expected_inv, X_inv)",
            "def test_encoder_dtypes_pandas():\n    # check dtype (similar to test_categorical_encoder_dtypes for dataframes)\n    pd = pytest.importorskip(\"pandas\")\n\n    enc = OneHotEncoder(categories=\"auto\")\n    exp = np.array(\n        [[1.0, 0.0, 1.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 1.0, 0.0, 1.0]],\n        dtype=\"float64\",\n    )\n\n    X = pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4], \"C\": [5, 6]}, dtype=\"int64\")\n    enc.fit(X)\n    assert all([enc.categories_[i].dtype == \"int64\" for i in range(2)])\n    assert_array_equal(enc.transform(X).toarray(), exp)\n\n    X = pd.DataFrame({\"A\": [1, 2], \"B\": [\"a\", \"b\"], \"C\": [3.0, 4.0]})\n    X_type = [X[\"A\"].dtype, X[\"B\"].dtype, X[\"C\"].dtype]\n    enc.fit(X)\n    assert all([enc.categories_[i].dtype == X_type[i] for i in range(3)])\n    assert_array_equal(enc.transform(X).toarray(), exp)",
            "def test_one_hot_encoder_dtype_pandas(output_dtype):\n    pd = pytest.importorskip(\"pandas\")\n\n    X_df = pd.DataFrame({\"A\": [\"a\", \"b\"], \"B\": [1, 2]})\n    X_expected = np.array([[1, 0, 1, 0], [0, 1, 0, 1]], dtype=output_dtype)\n\n    oh = OneHotEncoder(dtype=output_dtype)\n    assert_array_equal(oh.fit_transform(X_df).toarray(), X_expected)\n    assert_array_equal(oh.fit(X_df).transform(X_df).toarray(), X_expected)\n\n    oh = OneHotEncoder(dtype=output_dtype, sparse_output=False)\n    assert_array_equal(oh.fit_transform(X_df), X_expected)\n    assert_array_equal(oh.fit(X_df).transform(X_df), X_expected)",
            "def test_ordinal_encoder_handle_unknowns_nan_non_float_dtype():\n    # Make sure an error is raised when unknown_value=np.nan and the dtype\n    # isn't a float dtype\n    enc = OrdinalEncoder(\n        handle_unknown=\"use_encoded_value\", unknown_value=np.nan, dtype=int\n    )\n\n    X_fit = np.array([[1], [2], [3]])\n    with pytest.raises(ValueError, match=\"dtype parameter should be a float dtype\"):\n        enc.fit(X_fit)",
            "def test_ordinal_encoder_handle_unknown_string_dtypes(X_train, X_test):\n    \"\"\"Checks that `OrdinalEncoder` transforms string dtypes.\n    Non-regression test for:\n    https://github.com/scikit-learn/scikit-learn/issues/19872\n    \"\"\"\n    enc = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-9)\n    enc.fit(X_train)\n\n    X_trans = enc.transform(X_test)\n    assert_allclose(X_trans, [[-9, 0]])",
            "def test_ordinal_encoder_missing_value_support_pandas_categorical(\n    pd_nan_type, encoded_missing_value\n):\n    \"\"\"Check ordinal encoder is compatible with pandas.\"\"\"\n    # checks pandas dataframe with categorical features\n    pd = pytest.importorskip(\"pandas\")\n\n    pd_missing_value = pd.NA if pd_nan_type == \"pd.NA\" else np.nan\n\n    df = pd.DataFrame(\n        {\n            \"col1\": pd.Series([\"c\", \"a\", pd_missing_value, \"b\", \"a\"], dtype=\"category\"),\n        }\n    )\n\n    oe = OrdinalEncoder(encoded_missing_value=encoded_missing_value).fit(df)\n    assert len(oe.categories_) == 1\n    assert_array_equal(oe.categories_[0][:3], [\"a\", \"b\", \"c\"])\n    assert np.isnan(oe.categories_[0][-1])\n\n    df_trans = oe.transform(df)\n\n    assert_allclose(df_trans, [[2.0], [0.0], [encoded_missing_value], [1.0], [0.0]])\n\n    X_inverse = oe.inverse_transform(df_trans)\n    assert X_inverse.shape == (5, 1)\n    assert_array_equal(X_inverse[:2, 0], [\"c\", \"a\"])\n    assert_array_equal(X_inverse[3:, 0], [\"b\", \"a\"])\n    assert np.isnan(X_inverse[2, 0])",
            "def test_encoder_dtypes():\n    # check that dtypes are preserved when determining categories\n    enc = OneHotEncoder(categories=\"auto\")\n    exp = np.array([[1.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 1.0]], dtype=\"float64\")\n\n    for X in [\n        np.array([[1, 2], [3, 4]], dtype=\"int64\"),\n        np.array([[1, 2], [3, 4]], dtype=\"float64\"),\n        np.array([[\"a\", \"b\"], [\"c\", \"d\"]]),  # str dtype\n        np.array([[b\"a\", b\"b\"], [b\"c\", b\"d\"]]),  # bytes dtype\n        np.array([[1, \"a\"], [3, \"b\"]], dtype=\"object\"),\n    ]:\n        enc.fit(X)\n        assert all([enc.categories_[i].dtype == X.dtype for i in range(2)])\n        assert_array_equal(enc.transform(X).toarray(), exp)\n\n    X = [[1, 2], [3, 4]]\n    enc.fit(X)\n    assert all([np.issubdtype(enc.categories_[i].dtype, np.integer) for i in range(2)])\n    assert_array_equal(enc.transform(X).toarray(), exp)\n\n    X = [[1, \"a\"], [3, \"b\"]]\n    enc.fit(X)\n    assert all([enc.categories_[i].dtype == \"object\" for i in range(2)])\n    assert_array_equal(enc.transform(X).toarray(), exp)",
            "def test_ordinal_encoder_handle_unknowns_numeric(dtype):\n    enc = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-999)\n    X_fit = np.array([[1, 7], [2, 8], [3, 9]], dtype=dtype)\n    X_trans = np.array([[3, 12], [23, 8], [1, 7]], dtype=dtype)\n    enc.fit(X_fit)\n\n    X_trans_enc = enc.transform(X_trans)\n    exp = np.array([[2, -999], [-999, 1], [0, 0]], dtype=\"int64\")\n    assert_array_equal(X_trans_enc, exp)\n\n    X_trans_inv = enc.inverse_transform(X_trans_enc)\n    inv_exp = np.array([[3, None], [None, 8], [1, 7]], dtype=object)\n    assert_array_equal(X_trans_inv, inv_exp)"
        ]
    },
    {
        "repo": "scikit-learn/scikit-learn",
        "instance_id": "scikit-learn__scikit-learn-25500",
        "base_commit": "4db04923a754b6a2defa1b172f55d492b85d165e",
        "issue_title": "CalibratedClassifierCV doesn't work with `set_config(transform_output=\"pandas\")`",
        "test_file": "/home/zqc/RepoCodeEdit/data/swe_bench_temp_wy/scikit-learn/sklearn/tests/test_calibration.py",
        "searched_functions": [
            "def test_calibration_votingclassifier():\n    # Check that `CalibratedClassifier` works with `VotingClassifier`.\n    # The method `predict_proba` from `VotingClassifier` is dynamically\n    # defined via a property that only works when voting=\"soft\".\n    X, y = make_classification(n_samples=10, n_features=5, n_classes=2, random_state=7)\n    vote = VotingClassifier(\n        estimators=[(\"lr\" + str(i), LogisticRegression()) for i in range(3)],\n        voting=\"soft\",\n    )\n    vote.fit(X, y)\n\n    calib_clf = CalibratedClassifierCV(estimator=vote, cv=\"prefit\")\n    # smoke test: should not raise an error\n    calib_clf.fit(X, y)",
            "def test_calibration_dict_pipeline(dict_data, dict_data_pipeline):\n    \"\"\"Test that calibration works in prefit pipeline with transformer\n\n    `X` is not array-like, sparse matrix or dataframe at the start.\n    See https://github.com/scikit-learn/scikit-learn/issues/8710\n\n    Also test it can predict without running into validation errors.\n    See https://github.com/scikit-learn/scikit-learn/issues/19637\n    \"\"\"\n    X, y = dict_data\n    clf = dict_data_pipeline\n    calib_clf = CalibratedClassifierCV(clf, cv=\"prefit\")\n    calib_clf.fit(X, y)\n    # Check attributes are obtained from fitted estimator\n    assert_array_equal(calib_clf.classes_, clf.classes_)\n\n    # Neither the pipeline nor the calibration meta-estimator\n    # expose the n_features_in_ check on this kind of data.\n    assert not hasattr(clf, \"n_features_in_\")\n    assert not hasattr(calib_clf, \"n_features_in_\")\n\n    # Ensure that no error is thrown with predict and predict_proba\n    calib_clf.predict(X)\n    calib_clf.predict_proba(X)",
            "def test_calibration_prefit():\n    \"\"\"Test calibration for prefitted classifiers\"\"\"\n    n_samples = 50\n    X, y = make_classification(n_samples=3 * n_samples, n_features=6, random_state=42)\n    sample_weight = np.random.RandomState(seed=42).uniform(size=y.size)\n\n    X -= X.min()  # MultinomialNB only allows positive X\n\n    # split train and test\n    X_train, y_train, sw_train = X[:n_samples], y[:n_samples], sample_weight[:n_samples]\n    X_calib, y_calib, sw_calib = (\n        X[n_samples : 2 * n_samples],\n        y[n_samples : 2 * n_samples],\n        sample_weight[n_samples : 2 * n_samples],\n    )\n    X_test, y_test = X[2 * n_samples :], y[2 * n_samples :]\n\n    # Naive-Bayes\n    clf = MultinomialNB(force_alpha=True)\n    # Check error if clf not prefit\n    unfit_clf = CalibratedClassifierCV(clf, cv=\"prefit\")\n    with pytest.raises(NotFittedError):\n        unfit_clf.fit(X_calib, y_calib)\n\n    clf.fit(X_train, y_train, sw_train)\n    prob_pos_clf = clf.predict_proba(X_test)[:, 1]\n\n    # Naive Bayes with calibration\n    for this_X_calib, this_X_test in [\n        (X_calib, X_test),\n        (sparse.csr_matrix(X_calib), sparse.csr_matrix(X_test)),\n    ]:\n        for method in [\"isotonic\", \"sigmoid\"]:\n            cal_clf = CalibratedClassifierCV(clf, method=method, cv=\"prefit\")\n\n            for sw in [sw_calib, None]:\n                cal_clf.fit(this_X_calib, y_calib, sample_weight=sw)\n                y_prob = cal_clf.predict_proba(this_X_test)\n                y_pred = cal_clf.predict(this_X_test)\n                prob_pos_cal_clf = y_prob[:, 1]\n                assert_array_equal(y_pred, np.array([0, 1])[np.argmax(y_prob, axis=1)])\n\n                assert brier_score_loss(y_test, prob_pos_clf) > brier_score_loss(\n                    y_test, prob_pos_cal_clf\n                )",
            "def test_calibration(data, method, ensemble):\n    # Test calibration objects with isotonic and sigmoid\n    n_samples = N_SAMPLES // 2\n    X, y = data\n    sample_weight = np.random.RandomState(seed=42).uniform(size=y.size)\n\n    X -= X.min()  # MultinomialNB only allows positive X\n\n    # split train and test\n    X_train, y_train, sw_train = X[:n_samples], y[:n_samples], sample_weight[:n_samples]\n    X_test, y_test = X[n_samples:], y[n_samples:]\n\n    # Naive-Bayes\n    clf = MultinomialNB(force_alpha=True).fit(X_train, y_train, sample_weight=sw_train)\n    prob_pos_clf = clf.predict_proba(X_test)[:, 1]\n\n    cal_clf = CalibratedClassifierCV(clf, cv=y.size + 1, ensemble=ensemble)\n    with pytest.raises(ValueError):\n        cal_clf.fit(X, y)\n\n    # Naive Bayes with calibration\n    for this_X_train, this_X_test in [\n        (X_train, X_test),\n        (sparse.csr_matrix(X_train), sparse.csr_matrix(X_test)),\n    ]:\n        cal_clf = CalibratedClassifierCV(clf, method=method, cv=5, ensemble=ensemble)\n        # Note that this fit overwrites the fit on the entire training\n        # set\n        cal_clf.fit(this_X_train, y_train, sample_weight=sw_train)\n        prob_pos_cal_clf = cal_clf.predict_proba(this_X_test)[:, 1]\n\n        # Check that brier score has improved after calibration\n        assert brier_score_loss(y_test, prob_pos_clf) > brier_score_loss(\n            y_test, prob_pos_cal_clf\n        )\n\n        # Check invariance against relabeling [0, 1] -> [1, 2]\n        cal_clf.fit(this_X_train, y_train + 1, sample_weight=sw_train)\n        prob_pos_cal_clf_relabeled = cal_clf.predict_proba(this_X_test)[:, 1]\n        assert_array_almost_equal(prob_pos_cal_clf, prob_pos_cal_clf_relabeled)\n\n        # Check invariance against relabeling [0, 1] -> [-1, 1]\n        cal_clf.fit(this_X_train, 2 * y_train - 1, sample_weight=sw_train)\n        prob_pos_cal_clf_relabeled = cal_clf.predict_proba(this_X_test)[:, 1]\n        assert_array_almost_equal(prob_pos_cal_clf, prob_pos_cal_clf_relabeled)\n\n        # Check invariance against relabeling [0, 1] -> [1, 0]\n        cal_clf.fit(this_X_train, (y_train + 1) % 2, sample_weight=sw_train)\n        prob_pos_cal_clf_relabeled = cal_clf.predict_proba(this_X_test)[:, 1]\n        if method == \"sigmoid\":\n            assert_array_almost_equal(prob_pos_cal_clf, 1 - prob_pos_cal_clf_relabeled)\n        else:\n            # Isotonic calibration is not invariant against relabeling\n            # but should improve in both cases\n            assert brier_score_loss(y_test, prob_pos_clf) > brier_score_loss(\n                (y_test + 1) % 2, prob_pos_cal_clf_relabeled\n            )",
            "def test_calibration_multiclass(method, ensemble, seed):\n    def multiclass_brier(y_true, proba_pred, n_classes):\n        Y_onehot = np.eye(n_classes)[y_true]\n        return np.sum((Y_onehot - proba_pred) ** 2) / Y_onehot.shape[0]\n\n    # Test calibration for multiclass with classifier that implements\n    # only decision function.\n    clf = LinearSVC(random_state=7)\n    X, y = make_blobs(\n        n_samples=500, n_features=100, random_state=seed, centers=10, cluster_std=15.0\n    )\n\n    # Use an unbalanced dataset by collapsing 8 clusters into one class\n    # to make the naive calibration based on a softmax more unlikely\n    # to work.\n    y[y > 2] = 2\n    n_classes = np.unique(y).shape[0]\n    X_train, y_train = X[::2], y[::2]\n    X_test, y_test = X[1::2], y[1::2]\n\n    clf.fit(X_train, y_train)\n\n    cal_clf = CalibratedClassifierCV(clf, method=method, cv=5, ensemble=ensemble)\n    cal_clf.fit(X_train, y_train)\n    probas = cal_clf.predict_proba(X_test)\n    # Check probabilities sum to 1\n    assert_allclose(np.sum(probas, axis=1), np.ones(len(X_test)))\n\n    # Check that the dataset is not too trivial, otherwise it's hard\n    # to get interesting calibration data during the internal\n    # cross-validation loop.\n    assert 0.65 < clf.score(X_test, y_test) < 0.95\n\n    # Check that the accuracy of the calibrated model is never degraded\n    # too much compared to the original classifier.\n    assert cal_clf.score(X_test, y_test) > 0.95 * clf.score(X_test, y_test)\n\n    # Check that Brier loss of calibrated classifier is smaller than\n    # loss obtained by naively turning OvR decision function to\n    # probabilities via a softmax\n    uncalibrated_brier = multiclass_brier(\n        y_test, softmax(clf.decision_function(X_test)), n_classes=n_classes\n    )\n    calibrated_brier = multiclass_brier(y_test, probas, n_classes=n_classes)\n\n    assert calibrated_brier < 1.1 * uncalibrated_brier\n\n    # Test that calibration of a multiclass classifier decreases log-loss\n    # for RandomForestClassifier\n    clf = RandomForestClassifier(n_estimators=30, random_state=42)\n    clf.fit(X_train, y_train)\n    clf_probs = clf.predict_proba(X_test)\n    uncalibrated_brier = multiclass_brier(y_test, clf_probs, n_classes=n_classes)\n\n    cal_clf = CalibratedClassifierCV(clf, method=method, cv=5, ensemble=ensemble)\n    cal_clf.fit(X_train, y_train)\n    cal_clf_probs = cal_clf.predict_proba(X_test)\n    calibrated_brier = multiclass_brier(y_test, cal_clf_probs, n_classes=n_classes)\n    assert calibrated_brier < 1.1 * uncalibrated_brier",
            "def test_calibration_attributes(clf, cv):\n    # Check that `n_features_in_` and `classes_` attributes created properly\n    X, y = make_classification(n_samples=10, n_features=5, n_classes=2, random_state=7)\n    if cv == \"prefit\":\n        clf = clf.fit(X, y)\n    calib_clf = CalibratedClassifierCV(clf, cv=cv)\n    calib_clf.fit(X, y)\n\n    if cv == \"prefit\":\n        assert_array_equal(calib_clf.classes_, clf.classes_)\n        assert calib_clf.n_features_in_ == clf.n_features_in_\n    else:\n        classes = LabelEncoder().fit(y).classes_\n        assert_array_equal(calib_clf.classes_, classes)\n        assert calib_clf.n_features_in_ == X.shape[1]",
            "def test_calibration_inconsistent_prefit_n_features_in():\n    # Check that `n_features_in_` from prefit base estimator\n    # is consistent with training set\n    X, y = make_classification(n_samples=10, n_features=5, n_classes=2, random_state=7)\n    clf = LinearSVC(C=1).fit(X, y)\n    calib_clf = CalibratedClassifierCV(clf, cv=\"prefit\")\n\n    msg = \"X has 3 features, but LinearSVC is expecting 5 features as input.\"\n    with pytest.raises(ValueError, match=msg):\n        calib_clf.fit(X[:, :3], y)",
            "def test_calibration_display_validation(pyplot, iris_data, iris_data_binary):\n    X, y = iris_data\n    X_binary, y_binary = iris_data_binary\n\n    reg = LinearRegression().fit(X, y)\n    msg = \"'estimator' should be a fitted classifier\"\n    with pytest.raises(ValueError, match=msg):\n        CalibrationDisplay.from_estimator(reg, X, y)\n\n    clf = LinearSVC().fit(X, y)\n    msg = \"response method predict_proba is not defined in\"\n    with pytest.raises(ValueError, match=msg):\n        CalibrationDisplay.from_estimator(clf, X, y)\n\n    clf = LogisticRegression()\n    with pytest.raises(NotFittedError):\n        CalibrationDisplay.from_estimator(clf, X, y)",
            "def test_calibration_nan_imputer(ensemble):\n    \"\"\"Test that calibration can accept nan\"\"\"\n    X, y = make_classification(\n        n_samples=10, n_features=2, n_informative=2, n_redundant=0, random_state=42\n    )\n    X[0, 0] = np.nan\n    clf = Pipeline(\n        [(\"imputer\", SimpleImputer()), (\"rf\", RandomForestClassifier(n_estimators=1))]\n    )\n    clf_c = CalibratedClassifierCV(clf, cv=2, method=\"isotonic\", ensemble=ensemble)\n    clf_c.fit(X, y)\n    clf_c.predict(X)",
            "def test_calibration_less_classes(ensemble):\n    # Test to check calibration works fine when train set in a test-train\n    # split does not contain all classes\n    # Since this test uses LOO, at each iteration train set will not contain a\n    # class label\n    X = np.random.randn(10, 5)\n    y = np.arange(10)\n    clf = LinearSVC(C=1.0, random_state=7)\n    cal_clf = CalibratedClassifierCV(\n        clf, method=\"sigmoid\", cv=LeaveOneOut(), ensemble=ensemble\n    )\n    cal_clf.fit(X, y)\n\n    for i, calibrated_classifier in enumerate(cal_clf.calibrated_classifiers_):\n        proba = calibrated_classifier.predict_proba(X)\n        if ensemble:\n            # Check that the unobserved class has proba=0\n            assert_array_equal(proba[:, i], np.zeros(len(y)))\n            # Check for all other classes proba>0\n            assert np.all(proba[:, :i] > 0)\n            assert np.all(proba[:, i + 1 :] > 0)\n        else:\n            # Check `proba` are all 1/n_classes\n            assert np.allclose(proba, 1 / proba.shape[0])"
        ]
    },
    {
        "repo": "scikit-learn/scikit-learn",
        "instance_id": "scikit-learn__scikit-learn-13497",
        "base_commit": "26f690961a52946dd2f53bf0fdd4264b2ae5be90",
        "issue_title": "Comparing string to array in _estimate_mi",
        "test_file": "/home/zqc/RepoCodeEdit/data/swe_bench_temp_wy/scikit-learn/sklearn/tests/test_common.py",
        "searched_functions": [
            "def test_estimators(estimator, check):\n    # Common tests for estimator instances\n    with ignore_warnings(category=(DeprecationWarning, ConvergenceWarning,\n                                   UserWarning, FutureWarning)):\n        set_checking_parameters(estimator)\n        name = estimator.__class__.__name__\n        check(name, estimator)",
            "def test_parameters_default_constructible(name, Estimator):\n    # Test that estimators are default-constructible\n    check_parameters_default_constructible(name, Estimator)",
            "def _tested_estimators():\n    for name, Estimator in all_estimators():\n        if issubclass(Estimator, BiclusterMixin):\n            continue\n        if name.startswith(\"_\"):\n            continue\n        # FIXME _skip_test should be used here (if we could)\n\n        required_parameters = getattr(Estimator, \"_required_parameters\", [])\n        if len(required_parameters):\n            if required_parameters in ([\"estimator\"], [\"base_estimator\"]):\n                if issubclass(Estimator, RegressorMixin):\n                    estimator = Estimator(Ridge())\n                else:\n                    estimator = Estimator(LinearDiscriminantAnalysis())\n            else:\n                warnings.warn(\"Can't instantiate estimator {} which requires \"\n                              \"parameters {}\".format(name,\n                                                     required_parameters),\n                              SkipTestWarning)\n                continue\n        else:\n            estimator = Estimator()\n        yield name, estimator",
            "def test_no_attributes_set_in_init(name, estimator):\n    # input validation etc for all estimators\n    with ignore_warnings(category=(DeprecationWarning, ConvergenceWarning,\n                                   UserWarning, FutureWarning)):\n        tags = _safe_tags(estimator)\n        if tags['_skip_test']:\n            warnings.warn(\"Explicit SKIP via _skip_test tag for \"\n                          \"{}.\".format(name),\n                          SkipTestWarning)\n            return\n        # check this on class\n        check_no_attributes_set_in_init(name, estimator)",
            "def test_all_estimator_no_base_class():\n    # test that all_estimators doesn't find abstract classes.\n    for name, Estimator in all_estimators():\n        msg = (\"Base estimators such as {0} should not be included\"\n               \" in all_estimators\").format(name)\n        assert not name.lower().startswith('base'), msg",
            "def _tested_linear_classifiers():\n    classifiers = all_estimators(type_filter='classifier')\n\n    clean_warning_registry()\n    with warnings.catch_warnings(record=True):\n        for name, clazz in classifiers:\n            required_parameters = getattr(clazz, \"_required_parameters\", [])\n            if len(required_parameters):\n                # FIXME\n                continue\n\n            if ('class_weight' in clazz().get_params().keys() and\n                    issubclass(clazz, LinearClassifierMixin)):\n                yield name, clazz",
            "def _generate_checks_per_estimator(check_generator, estimators):\n    with ignore_warnings(category=(DeprecationWarning, FutureWarning)):\n        for name, estimator in estimators:\n            for check in check_generator(name, estimator):\n                yield estimator, check",
            "def test_import_all_consistency():\n    # Smoke test to check that any name in a __all__ list is actually defined\n    # in the namespace of the module or package.\n    pkgs = pkgutil.walk_packages(path=sklearn.__path__, prefix='sklearn.',\n                                 onerror=lambda _: None)\n    submods = [modname for _, modname, _ in pkgs]\n    for modname in submods + ['sklearn']:\n        if \".tests.\" in modname:\n            continue\n        if IS_PYPY and ('_svmlight_format' in modname or\n                        'feature_extraction._hashing' in modname):\n            continue\n        package = __import__(modname, fromlist=\"dummy\")\n        for name in getattr(package, '__all__', ()):\n            if getattr(package, name, None) is None:\n                raise AttributeError(\n                    \"Module '{0}' has no attribute '{1}'\".format(\n                        modname, name))",
            "def test_class_weight_balanced_linear_classifiers(name, Classifier):\n    check_class_weight_balanced_linear_classifier(name, Classifier)",
            "def test_root_import_all_completeness():\n    EXCEPTIONS = ('utils', 'tests', 'base', 'setup')\n    for _, modname, _ in pkgutil.walk_packages(path=sklearn.__path__,\n                                               onerror=lambda _: None):\n        if '.' in modname or modname.startswith('_') or modname in EXCEPTIONS:\n            continue\n        assert_in(modname, sklearn.__all__)"
        ]
    },
    {
        "repo": "scikit-learn/scikit-learn",
        "instance_id": "scikit-learn__scikit-learn-13439",
        "base_commit": "a62775e99f2a5ea3d51db7160fad783f6cd8a4c5",
        "issue_title": "Pipeline should implement __len__",
        "test_file": "/home/zqc/RepoCodeEdit/data/swe_bench_temp_wy/scikit-learn/sklearn/tests/test_pipeline.py",
        "searched_functions": [
            "def test_pipeline_slice():\n    pipe = Pipeline([('transf1', Transf()),\n                     ('transf2', Transf()),\n                     ('clf', FitParamT())])\n    pipe2 = pipe[:-1]\n    assert isinstance(pipe2, Pipeline)\n    assert pipe2.steps == pipe.steps[:-1]\n    assert 2 == len(pipe2.named_steps)\n    assert_raises(ValueError, lambda: pipe[::-1])",
            "def test_make_pipeline_memory():\n    cachedir = mkdtemp()\n    if LooseVersion(joblib_version) < LooseVersion('0.12'):\n        # Deal with change of API in joblib\n        memory = Memory(cachedir=cachedir, verbose=10)\n    else:\n        memory = Memory(location=cachedir, verbose=10)\n    pipeline = make_pipeline(DummyTransf(), SVC(), memory=memory)\n    assert pipeline.memory is memory\n    pipeline = make_pipeline(DummyTransf(), SVC())\n    assert pipeline.memory is None\n\n    shutil.rmtree(cachedir)",
            "def test_pipeline_index():\n    transf = Transf()\n    clf = FitParamT()\n    pipe = Pipeline([('transf', transf), ('clf', clf)])\n    assert pipe[0] == transf\n    assert pipe['transf'] == transf\n    assert pipe[-1] == clf\n    assert pipe['clf'] == clf\n    assert_raises(IndexError, lambda: pipe[3])\n    assert_raises(KeyError, lambda: pipe['foobar'])",
            "def test_pipeline_memory():\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    cachedir = mkdtemp()\n    try:\n        if LooseVersion(joblib_version) < LooseVersion('0.12'):\n            # Deal with change of API in joblib\n            memory = Memory(cachedir=cachedir, verbose=10)\n        else:\n            memory = Memory(location=cachedir, verbose=10)\n        # Test with Transformer + SVC\n        clf = SVC(gamma='scale', probability=True, random_state=0)\n        transf = DummyTransf()\n        pipe = Pipeline([('transf', clone(transf)), ('svc', clf)])\n        cached_pipe = Pipeline([('transf', transf), ('svc', clf)],\n                               memory=memory)\n\n        # Memoize the transformer at the first fit\n        cached_pipe.fit(X, y)\n        pipe.fit(X, y)\n        # Get the time stamp of the transformer in the cached pipeline\n        ts = cached_pipe.named_steps['transf'].timestamp_\n        # Check that cached_pipe and pipe yield identical results\n        assert_array_equal(pipe.predict(X), cached_pipe.predict(X))\n        assert_array_equal(pipe.predict_proba(X), cached_pipe.predict_proba(X))\n        assert_array_equal(pipe.predict_log_proba(X),\n                           cached_pipe.predict_log_proba(X))\n        assert_array_equal(pipe.score(X, y), cached_pipe.score(X, y))\n        assert_array_equal(pipe.named_steps['transf'].means_,\n                           cached_pipe.named_steps['transf'].means_)\n        assert not hasattr(transf, 'means_')\n        # Check that we are reading the cache while fitting\n        # a second time\n        cached_pipe.fit(X, y)\n        # Check that cached_pipe and pipe yield identical results\n        assert_array_equal(pipe.predict(X), cached_pipe.predict(X))\n        assert_array_equal(pipe.predict_proba(X), cached_pipe.predict_proba(X))\n        assert_array_equal(pipe.predict_log_proba(X),\n                           cached_pipe.predict_log_proba(X))\n        assert_array_equal(pipe.score(X, y), cached_pipe.score(X, y))\n        assert_array_equal(pipe.named_steps['transf'].means_,\n                           cached_pipe.named_steps['transf'].means_)\n        assert_equal(ts, cached_pipe.named_steps['transf'].timestamp_)\n        # Create a new pipeline with cloned estimators\n        # Check that even changing the name step does not affect the cache hit\n        clf_2 = SVC(gamma='scale', probability=True, random_state=0)\n        transf_2 = DummyTransf()\n        cached_pipe_2 = Pipeline([('transf_2', transf_2), ('svc', clf_2)],\n                                 memory=memory)\n        cached_pipe_2.fit(X, y)\n\n        # Check that cached_pipe and pipe yield identical results\n        assert_array_equal(pipe.predict(X), cached_pipe_2.predict(X))\n        assert_array_equal(pipe.predict_proba(X),\n                           cached_pipe_2.predict_proba(X))\n        assert_array_equal(pipe.predict_log_proba(X),\n                           cached_pipe_2.predict_log_proba(X))\n        assert_array_equal(pipe.score(X, y), cached_pipe_2.score(X, y))\n        assert_array_equal(pipe.named_steps['transf'].means_,\n                           cached_pipe_2.named_steps['transf_2'].means_)\n        assert_equal(ts, cached_pipe_2.named_steps['transf_2'].timestamp_)\n    finally:\n        shutil.rmtree(cachedir)",
            "def test_pipeline_fit_params():\n    # Test that the pipeline can take fit parameters\n    pipe = Pipeline([('transf', Transf()), ('clf', FitParamT())])\n    pipe.fit(X=None, y=None, clf__should_succeed=True)\n    # classifier should return True\n    assert pipe.predict(None)\n    # and transformer params should not be changed\n    assert pipe.named_steps['transf'].a is None\n    assert pipe.named_steps['transf'].b is None\n    # invalid parameters should raise an error message\n    assert_raise_message(\n        TypeError,\n        \"fit() got an unexpected keyword argument 'bad'\",\n        pipe.fit, None, None, clf__bad=True\n    )",
            "def test_pipeline_sample_weight_supported():\n    # Pipeline should pass sample_weight\n    X = np.array([[1, 2]])\n    pipe = Pipeline([('transf', Transf()), ('clf', FitParamT())])\n    pipe.fit(X, y=None)\n    assert_equal(pipe.score(X), 3)\n    assert_equal(pipe.score(X, y=None), 3)\n    assert_equal(pipe.score(X, y=None, sample_weight=None), 3)\n    assert_equal(pipe.score(X, sample_weight=np.array([2, 3])), 8)",
            "def test_pipeline_ducktyping():\n    pipeline = make_pipeline(Mult(5))\n    pipeline.predict\n    pipeline.transform\n    pipeline.inverse_transform\n\n    pipeline = make_pipeline(Transf())\n    assert not hasattr(pipeline, 'predict')\n    pipeline.transform\n    pipeline.inverse_transform\n\n    pipeline = make_pipeline('passthrough')\n    assert pipeline.steps[0] == ('passthrough', 'passthrough')\n    assert not hasattr(pipeline, 'predict')\n    pipeline.transform\n    pipeline.inverse_transform\n\n    pipeline = make_pipeline(Transf(), NoInvTransf())\n    assert not hasattr(pipeline, 'predict')\n    pipeline.transform\n    assert not hasattr(pipeline, 'inverse_transform')\n\n    pipeline = make_pipeline(NoInvTransf(), Transf())\n    assert not hasattr(pipeline, 'predict')\n    pipeline.transform\n    assert not hasattr(pipeline, 'inverse_transform')",
            "def test_pipeline_init():\n    # Test the various init parameters of the pipeline.\n    assert_raises(TypeError, Pipeline)\n    # Check that we can't instantiate pipelines with objects without fit\n    # method\n    assert_raises_regex(TypeError,\n                        'Last step of Pipeline should implement fit '\n                        'or be the string \\'passthrough\\''\n                        '.*NoFit.*',\n                        Pipeline, [('clf', NoFit())])\n    # Smoke test with only an estimator\n    clf = NoTrans()\n    pipe = Pipeline([('svc', clf)])\n    assert_equal(pipe.get_params(deep=True),\n                 dict(svc__a=None, svc__b=None, svc=clf,\n                      **pipe.get_params(deep=False)))\n\n    # Check that params are set\n    pipe.set_params(svc__a=0.1)\n    assert_equal(clf.a, 0.1)\n    assert_equal(clf.b, None)\n    # Smoke test the repr:\n    repr(pipe)\n\n    # Test with two objects\n    clf = SVC()\n    filter1 = SelectKBest(f_classif)\n    pipe = Pipeline([('anova', filter1), ('svc', clf)])\n\n    # Check that we can't instantiate with non-transformers on the way\n    # Note that NoTrans implements fit, but not transform\n    assert_raises_regex(TypeError,\n                        'All intermediate steps should be transformers'\n                        '.*\\\\bNoTrans\\\\b.*',\n                        Pipeline, [('t', NoTrans()), ('svc', clf)])\n\n    # Check that params are set\n    pipe.set_params(svc__C=0.1)\n    assert_equal(clf.C, 0.1)\n    # Smoke test the repr:\n    repr(pipe)\n\n    # Check that params are not set when naming them wrong\n    assert_raises(ValueError, pipe.set_params, anova__C=0.1)\n\n    # Test clone\n    pipe2 = assert_no_warnings(clone, pipe)\n    assert not pipe.named_steps['svc'] is pipe2.named_steps['svc']\n\n    # Check that apart from estimators, the parameters are the same\n    params = pipe.get_params(deep=True)\n    params2 = pipe2.get_params(deep=True)\n\n    for x in pipe.get_params(deep=False):\n        params.pop(x)\n\n    for x in pipe2.get_params(deep=False):\n        params2.pop(x)\n\n    # Remove estimators that where copied\n    params.pop('svc')\n    params.pop('anova')\n    params2.pop('svc')\n    params2.pop('anova')\n    assert_equal(params, params2)",
            "def test_pipeline_sample_weight_unsupported():\n    # When sample_weight is None it shouldn't be passed\n    X = np.array([[1, 2]])\n    pipe = Pipeline([('transf', Transf()), ('clf', Mult())])\n    pipe.fit(X, y=None)\n    assert_equal(pipe.score(X), 3)\n    assert_equal(pipe.score(X, sample_weight=None), 3)\n    assert_raise_message(\n        TypeError,\n        \"score() got an unexpected keyword argument 'sample_weight'\",\n        pipe.score, X, sample_weight=np.array([2, 3])\n    )",
            "def test_make_pipeline():\n    t1 = Transf()\n    t2 = Transf()\n    pipe = make_pipeline(t1, t2)\n    assert isinstance(pipe, Pipeline)\n    assert_equal(pipe.steps[0][0], \"transf-1\")\n    assert_equal(pipe.steps[1][0], \"transf-2\")\n\n    pipe = make_pipeline(t1, t2, FitParamT())\n    assert isinstance(pipe, Pipeline)\n    assert_equal(pipe.steps[0][0], \"transf-1\")\n    assert_equal(pipe.steps[1][0], \"transf-2\")\n    assert_equal(pipe.steps[2][0], \"fitparamt\")\n\n    assert_raise_message(\n        TypeError,\n        'Unknown keyword arguments: \"random_parameter\"',\n        make_pipeline, t1, t2, random_parameter='rnd'\n    )"
        ]
    },
    {
        "repo": "scikit-learn/scikit-learn",
        "instance_id": "scikit-learn__scikit-learn-14894",
        "base_commit": "fdbaa58acbead5a254f2e6d597dc1ab3b947f4c6",
        "issue_title": "ZeroDivisionError in _sparse_fit for SVM with empty support_vectors_",
        "test_file": "/home/zqc/RepoCodeEdit/data/swe_bench_temp_wy/scikit-learn/sklearn/svm/tests/test_svm.py",
        "searched_functions": [
            "def test_svr_errors():\n    X = [[0.0], [1.0]]\n    y = [0.0, 0.5]\n\n    # Bad kernel\n    clf = svm.SVR(kernel=lambda x, y: np.array([[1.0]]))\n    clf.fit(X, y)\n    with pytest.raises(ValueError):\n        clf.predict(X)",
            "def test_sparse_precomputed():\n    clf = svm.SVC(kernel='precomputed')\n    sparse_gram = sparse.csr_matrix([[1, 0], [0, 1]])\n    try:\n        clf.fit(sparse_gram, [0, 1])\n        assert not \"reached\"\n    except TypeError as e:\n        assert \"Sparse precomputed\" in str(e)",
            "def test_svr():\n    # Test Support Vector Regression\n\n    diabetes = datasets.load_diabetes()\n    for clf in (svm.NuSVR(kernel='linear', nu=.4, C=1.0),\n                svm.NuSVR(kernel='linear', nu=.4, C=10.),\n                svm.SVR(kernel='linear', C=10.),\n                svm.LinearSVR(C=10.),\n                svm.LinearSVR(C=10.)):\n        clf.fit(diabetes.data, diabetes.target)\n        assert clf.score(diabetes.data, diabetes.target) > 0.02\n\n    # non-regression test; previously, BaseLibSVM would check that\n    # len(np.unique(y)) < 2, which must only be done for SVC\n    svm.SVR().fit(diabetes.data, np.ones(len(diabetes.data)))\n    svm.LinearSVR().fit(diabetes.data, np.ones(len(diabetes.data)))",
            "def test_n_support_oneclass_svr():\n    # Make n_support is correct for oneclass and SVR (used to be\n    # non-initialized)\n    # this is a non regression test for issue #14774\n    X = np.array([[0], [0.44], [0.45], [0.46], [1]])\n    clf = svm.OneClassSVM()\n    assert not hasattr(clf, 'n_support_')\n    clf.fit(X)\n    assert clf.n_support_ == clf.support_vectors_.shape[0]\n    assert clf.n_support_.size == 1\n    assert clf.n_support_ == 3\n\n    y = np.arange(X.shape[0])\n    reg = svm.SVR().fit(X, y)\n    assert reg.n_support_ == reg.support_vectors_.shape[0]\n    assert reg.n_support_.size == 1\n    assert reg.n_support_ == 4",
            "def test_unfitted():\n    X = \"foo!\"  # input validation not required when SVM not fitted\n\n    clf = svm.SVC()\n    with pytest.raises(Exception, match=r\".*\\bSVC\\b.*\\bnot\\b.*\\bfitted\\b\"):\n        clf.predict(X)\n\n    clf = svm.NuSVR()\n    with pytest.raises(Exception, match=r\".*\\bNuSVR\\b.*\\bnot\\b.*\\bfitted\\b\"):\n        clf.predict(X)",
            "def test_linear_svx_uppercase_loss_penality_raises_error():\n    # Check if Upper case notation raises error at _fit_liblinear\n    # which is called by fit\n\n    X, y = [[0.0], [1.0]], [0, 1]\n\n    assert_raise_message(ValueError, \"loss='SQuared_hinge' is not supported\",\n                         svm.LinearSVC(loss=\"SQuared_hinge\").fit, X, y)\n\n    assert_raise_message(ValueError,\n                         (\"The combination of penalty='L2'\"\n                          \" and loss='squared_hinge' is not supported\"),\n                         svm.LinearSVC(penalty=\"L2\").fit, X, y)",
            "def test_svc_bad_kernel():\n    svc = svm.SVC(kernel=lambda x, y: x)\n    with pytest.raises(ValueError):\n        svc.fit(X, Y)",
            "def test_dense_liblinear_intercept_handling(classifier=svm.LinearSVC):\n    # Test that dense liblinear honours intercept_scaling param\n    X = [[2, 1],\n         [3, 1],\n         [1, 3],\n         [2, 3]]\n    y = [0, 0, 1, 1]\n    clf = classifier(fit_intercept=True, penalty='l1', loss='squared_hinge',\n                     dual=False, C=4, tol=1e-7, random_state=0)\n    assert clf.intercept_scaling == 1, clf.intercept_scaling\n    assert clf.fit_intercept\n\n    # when intercept_scaling is low the intercept value is highly \"penalized\"\n    # by regularization\n    clf.intercept_scaling = 1\n    clf.fit(X, y)\n    assert_almost_equal(clf.intercept_, 0, decimal=5)\n\n    # when intercept_scaling is sufficiently high, the intercept value\n    # is not affected by regularization\n    clf.intercept_scaling = 100\n    clf.fit(X, y)\n    intercept1 = clf.intercept_\n    assert intercept1 < -1\n\n    # when intercept_scaling is sufficiently high, the intercept value\n    # doesn't depend on intercept_scaling value\n    clf.intercept_scaling = 1000\n    clf.fit(X, y)\n    intercept2 = clf.intercept_\n    assert_array_almost_equal(intercept1, intercept2, decimal=2)",
            "def test_lsvc_intercept_scaling_zero():\n    # Test that intercept_scaling is ignored when fit_intercept is False\n\n    lsvc = svm.LinearSVC(fit_intercept=False)\n    lsvc.fit(X, Y)\n    assert lsvc.intercept_ == 0.",
            "def test_bad_input():\n    # Test that it gives proper exception on deficient input\n    # impossible value of C\n    with pytest.raises(ValueError):\n        svm.SVC(C=-1).fit(X, Y)\n\n    # impossible value of nu\n    clf = svm.NuSVC(nu=0.0)\n    with pytest.raises(ValueError):\n        clf.fit(X, Y)\n\n    Y2 = Y[:-1]  # wrong dimensions for labels\n    with pytest.raises(ValueError):\n        clf.fit(X, Y2)\n\n    # Test with arrays that are non-contiguous.\n    for clf in (svm.SVC(), svm.LinearSVC(random_state=0)):\n        Xf = np.asfortranarray(X)\n        assert not Xf.flags['C_CONTIGUOUS']\n        yf = np.ascontiguousarray(np.tile(Y, (2, 1)).T)\n        yf = yf[:, -1]\n        assert not yf.flags['F_CONTIGUOUS']\n        assert not yf.flags['C_CONTIGUOUS']\n        clf.fit(Xf, yf)\n        assert_array_equal(clf.predict(T), true_result)\n\n    # error for precomputed kernelsx\n    clf = svm.SVC(kernel='precomputed')\n    with pytest.raises(ValueError):\n        clf.fit(X, Y)\n\n    # sample_weight bad dimensions\n    clf = svm.SVC()\n    with pytest.raises(ValueError):\n        clf.fit(X, Y, sample_weight=range(len(X) - 1))\n\n    # predict with sparse input when trained with dense\n    clf = svm.SVC().fit(X, Y)\n    with pytest.raises(ValueError):\n        clf.predict(sparse.lil_matrix(X))\n\n    Xt = np.array(X).T\n    clf.fit(np.dot(X, Xt), Y)\n    with pytest.raises(ValueError):\n        clf.predict(X)\n\n    clf = svm.SVC()\n    clf.fit(X, Y)\n    with pytest.raises(ValueError):\n        clf.predict(Xt)"
        ]
    }
]